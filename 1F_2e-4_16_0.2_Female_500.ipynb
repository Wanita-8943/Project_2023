{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wanita-8943/Project_2023/blob/main/1F_2e-4_16_0.2_Female_500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#เรียกใช้ CSV"
      ],
      "metadata": {
        "id": "ow7eWoNw6U-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil"
      ],
      "metadata": {
        "id": "DlQ2oT7cIR6o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_2Fe8u81d5r",
        "outputId": "fdea3b28-a248-49f5-efdd-ac92c1751e7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "5qxePnnn7TGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "D-hCRloc3t39"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "YZWXwjXeGxZP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#กำหนดค่าพารามิเตอร์\n"
      ],
      "metadata": {
        "id": "RooqSdBc7QHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "width = 150\n",
        "height = 150\n",
        "epochs = 500\n",
        "NUM_TRAIN = 1425\n",
        "NUM_TEST = 475\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3)"
      ],
      "metadata": {
        "id": "thDb7U9B3xOo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clone efficientnet repo\n"
      ],
      "metadata": {
        "id": "pumGmy6f3eSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7iy2f8n16p0",
        "outputId": "7879bdc9-174d-44eb-bcb9-3bba13a402d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 837 (delta 255), reused 328 (delta 235), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 8.55 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "Vbdblwbv89fe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "vnLrobY_-GCx",
        "outputId": "53103402-fc3b-4080-a299-fc4415ba2b95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "tjZBRnfo3bN0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "hNZAzbDp3lgA",
        "outputId": "c40cd80a-26d8-42c1-8f3e-f15c628b1e09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(19, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "yWNKfQUt5rga"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "NadBB12251jh",
        "outputId": "97c7fd2f-39b4-4f85-8733-15ea214eab8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 19)                24339     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,073,923\n",
            "Trainable params: 4,031,907\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "GepWq3yy53t5",
        "outputId": "b3f42503-94ad-4a69-b5dc-82e9008725e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD76D6c_79py",
        "outputId": "61b2c6e2-f7d4-4af1-8a37-f41597ebf68a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 0\n",
            "Non-trainable params: 4,049,564\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#สร้างโฟลเดอร์ Train Valodation และ Test"
      ],
      "metadata": {
        "id": "J36J9EAE7qSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/MyDrive/cut_panoramic/Data/New_Data_Female125.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "skoKhKJDngAZ",
        "outputId": "2b322865-076e-4907-e7c5-fc78becef029"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Fig_Age  Fig_Person_Sex  Age(year) Class  Class_Re       Filename  \\\n",
              "0           1               1          7  Y07F         0         V1.jpg   \n",
              "1           2               1          7  Y07F         0    Flip_V1.jpg   \n",
              "2           3               2          7  Y07F         0         V2.jpg   \n",
              "3           4               2          7  Y07F         0    Flip_V2.jpg   \n",
              "4           5               3          7  Y07F         0         V3.jpg   \n",
              "...       ...             ...        ...   ...       ...            ...   \n",
              "2370      121              65         25  Y25F        18  Flip_J145.jpg   \n",
              "2371      122              66         25  Y25F        18  Flip_J149.jpg   \n",
              "2372      123              67         25  Y25F        18  Flip_J158.jpg   \n",
              "2373      124              68         25  Y25F        18  Flip_J177.jpg   \n",
              "2374      125              69         25  Y25F        18  Flip_J180.jpg   \n",
              "\n",
              "                                          Path_filename     Sex Floder  \n",
              "0     /content/drive/My Drive/TVT_Female125/train/Y0...  Female   Both  \n",
              "1     /content/drive/My Drive/TVT_Female125/train/Y0...  Female   Both  \n",
              "2     /content/drive/My Drive/TVT_Female125/train/Y0...  Female   Both  \n",
              "3     /content/drive/My Drive/TVT_Female125/train/Y0...  Female   Both  \n",
              "4     /content/drive/My Drive/TVT_Female125/train/Y0...  Female   Both  \n",
              "...                                                 ...     ...    ...  \n",
              "2370  /content/drive/My Drive/TVT_Female125/test/Y25...  Female     Lt  \n",
              "2371  /content/drive/My Drive/TVT_Female125/test/Y25...  Female     Lt  \n",
              "2372  /content/drive/My Drive/TVT_Female125/test/Y25...  Female     Lt  \n",
              "2373  /content/drive/My Drive/TVT_Female125/test/Y25...  Female     Lt  \n",
              "2374  /content/drive/My Drive/TVT_Female125/test/Y25...  Female     Lt  \n",
              "\n",
              "[2375 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-817280a3-9cf9-4b13-a328-24e49b4b46a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fig_Age</th>\n",
              "      <th>Fig_Person_Sex</th>\n",
              "      <th>Age(year)</th>\n",
              "      <th>Class</th>\n",
              "      <th>Class_Re</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Path_filename</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Floder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07F</td>\n",
              "      <td>0</td>\n",
              "      <td>V1.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/train/Y0...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07F</td>\n",
              "      <td>0</td>\n",
              "      <td>Flip_V1.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/train/Y0...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07F</td>\n",
              "      <td>0</td>\n",
              "      <td>V2.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/train/Y0...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07F</td>\n",
              "      <td>0</td>\n",
              "      <td>Flip_V2.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/train/Y0...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07F</td>\n",
              "      <td>0</td>\n",
              "      <td>V3.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/train/Y0...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2370</th>\n",
              "      <td>121</td>\n",
              "      <td>65</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25F</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J145.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/test/Y25...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Lt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2371</th>\n",
              "      <td>122</td>\n",
              "      <td>66</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25F</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J149.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/test/Y25...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Lt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2372</th>\n",
              "      <td>123</td>\n",
              "      <td>67</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25F</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J158.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/test/Y25...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Lt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2373</th>\n",
              "      <td>124</td>\n",
              "      <td>68</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25F</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J177.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/test/Y25...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Lt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2374</th>\n",
              "      <td>125</td>\n",
              "      <td>69</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25F</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J180.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Female125/test/Y25...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Lt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2375 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-817280a3-9cf9-4b13-a328-24e49b4b46a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-817280a3-9cf9-4b13-a328-24e49b4b46a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-817280a3-9cf9-4b13-a328-24e49b4b46a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = df[df['Fig_Age'].between(1,75)]\n",
        "val = df[df['Fig_Age'].between(76,100)]"
      ],
      "metadata": {
        "id": "Z1zBw01Gl8Ac"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/My Drive/TVT_Female125\"\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL0g-8iOnMC4",
        "outputId": "878e8e15-7fd3-4026-ab83-ea1bc7990cfe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/TVT_Female125/train\n",
            "/content/drive/My Drive/TVT_Female125/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Train"
      ],
      "metadata": {
        "id": "bWEnlTSwazL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, #โมเดลส่วนใหญ่ต้องใช้ RGB ในช่วง 0–1\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "xGPrsn9no_pa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'Path_filename',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'Path_filename',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nVlmAszntK_",
        "outputId": "0a5a899a-bf7b-4b72-fd1a-3c89b7743cbd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1425 validated image filenames.\n",
            "Found 475 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "          optimizer=Adam(learning_rate=2e-4),\n",
        "          metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "N6qUmmF856ZE",
        "outputId": "b4af86cc-2be5-4b91-aac9-8b8dd7481daa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-0f54266a785f>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "89/89 [==============================] - 409s 4s/step - loss: 105.0385 - mae: 8.6987 - val_loss: 101.6982 - val_mae: 8.5278\n",
            "Epoch 2/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 100.6831 - mae: 8.4722 - val_loss: 100.9475 - val_mae: 8.4924\n",
            "Epoch 3/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 99.6365 - mae: 8.4198 - val_loss: 99.6225 - val_mae: 8.4159\n",
            "Epoch 4/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 99.0062 - mae: 8.3830 - val_loss: 99.6283 - val_mae: 8.4141\n",
            "Epoch 5/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 98.8751 - mae: 8.3769 - val_loss: 99.1272 - val_mae: 8.3985\n",
            "Epoch 6/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 98.0494 - mae: 8.3307 - val_loss: 98.4745 - val_mae: 8.3515\n",
            "Epoch 7/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 97.6803 - mae: 8.3021 - val_loss: 97.0123 - val_mae: 8.2916\n",
            "Epoch 8/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 97.2908 - mae: 8.2862 - val_loss: 96.5989 - val_mae: 8.2362\n",
            "Epoch 9/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 96.0740 - mae: 8.2195 - val_loss: 96.1561 - val_mae: 8.2160\n",
            "Epoch 10/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 96.3024 - mae: 8.2388 - val_loss: 96.4851 - val_mae: 8.2221\n",
            "Epoch 11/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 95.5097 - mae: 8.1841 - val_loss: 94.9806 - val_mae: 8.1764\n",
            "Epoch 12/500\n",
            "89/89 [==============================] - 25s 281ms/step - loss: 94.9643 - mae: 8.1624 - val_loss: 94.2901 - val_mae: 8.1239\n",
            "Epoch 13/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 94.2864 - mae: 8.1262 - val_loss: 93.9571 - val_mae: 8.1100\n",
            "Epoch 14/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 93.4604 - mae: 8.0775 - val_loss: 93.5161 - val_mae: 8.0956\n",
            "Epoch 15/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 93.4624 - mae: 8.0798 - val_loss: 92.3558 - val_mae: 8.0234\n",
            "Epoch 16/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 92.9207 - mae: 8.0487 - val_loss: 91.7198 - val_mae: 7.9915\n",
            "Epoch 17/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 92.0154 - mae: 8.0018 - val_loss: 91.8809 - val_mae: 8.0144\n",
            "Epoch 18/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 91.6799 - mae: 7.9905 - val_loss: 91.3421 - val_mae: 7.9862\n",
            "Epoch 19/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 91.3288 - mae: 7.9744 - val_loss: 91.6659 - val_mae: 7.9963\n",
            "Epoch 20/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 90.6268 - mae: 7.9318 - val_loss: 90.0489 - val_mae: 7.8896\n",
            "Epoch 21/500\n",
            "89/89 [==============================] - 26s 292ms/step - loss: 89.5051 - mae: 7.8788 - val_loss: 88.9996 - val_mae: 7.8641\n",
            "Epoch 22/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 89.2388 - mae: 7.8686 - val_loss: 88.7184 - val_mae: 7.8401\n",
            "Epoch 23/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 89.3151 - mae: 7.8717 - val_loss: 88.7014 - val_mae: 7.8497\n",
            "Epoch 24/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 88.3352 - mae: 7.8141 - val_loss: 88.4878 - val_mae: 7.8374\n",
            "Epoch 25/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 88.3017 - mae: 7.8257 - val_loss: 87.5879 - val_mae: 7.7738\n",
            "Epoch 26/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 87.6156 - mae: 7.7845 - val_loss: 87.8393 - val_mae: 7.8058\n",
            "Epoch 27/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 86.6893 - mae: 7.7398 - val_loss: 85.8869 - val_mae: 7.6824\n",
            "Epoch 28/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 86.3843 - mae: 7.7161 - val_loss: 85.6020 - val_mae: 7.6728\n",
            "Epoch 29/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 86.1208 - mae: 7.7004 - val_loss: 85.5322 - val_mae: 7.6696\n",
            "Epoch 30/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 84.8844 - mae: 7.6412 - val_loss: 84.5473 - val_mae: 7.6133\n",
            "Epoch 31/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 84.6235 - mae: 7.6271 - val_loss: 83.5154 - val_mae: 7.5517\n",
            "Epoch 32/500\n",
            "89/89 [==============================] - 22s 239ms/step - loss: 84.3733 - mae: 7.6148 - val_loss: 84.4959 - val_mae: 7.6128\n",
            "Epoch 33/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 83.8092 - mae: 7.5805 - val_loss: 83.1770 - val_mae: 7.5398\n",
            "Epoch 34/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 83.2648 - mae: 7.5498 - val_loss: 82.5224 - val_mae: 7.5153\n",
            "Epoch 35/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 82.9312 - mae: 7.5328 - val_loss: 82.8004 - val_mae: 7.5503\n",
            "Epoch 36/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 82.2736 - mae: 7.5013 - val_loss: 82.3559 - val_mae: 7.4960\n",
            "Epoch 37/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 81.9257 - mae: 7.4811 - val_loss: 81.8780 - val_mae: 7.4675\n",
            "Epoch 38/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 81.4269 - mae: 7.4540 - val_loss: 81.3107 - val_mae: 7.4360\n",
            "Epoch 39/500\n",
            "89/89 [==============================] - 21s 229ms/step - loss: 80.8710 - mae: 7.4220 - val_loss: 81.1225 - val_mae: 7.4334\n",
            "Epoch 40/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 80.4956 - mae: 7.4011 - val_loss: 79.6372 - val_mae: 7.3527\n",
            "Epoch 41/500\n",
            "89/89 [==============================] - 22s 238ms/step - loss: 79.7026 - mae: 7.3531 - val_loss: 79.2405 - val_mae: 7.3314\n",
            "Epoch 42/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 79.4139 - mae: 7.3332 - val_loss: 78.7026 - val_mae: 7.3181\n",
            "Epoch 43/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 79.2490 - mae: 7.3309 - val_loss: 78.0103 - val_mae: 7.2582\n",
            "Epoch 44/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 78.5118 - mae: 7.2861 - val_loss: 78.8753 - val_mae: 7.3235\n",
            "Epoch 45/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 78.0799 - mae: 7.2725 - val_loss: 77.4101 - val_mae: 7.2404\n",
            "Epoch 46/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 77.3344 - mae: 7.2371 - val_loss: 77.4334 - val_mae: 7.2400\n",
            "Epoch 47/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 77.3289 - mae: 7.2357 - val_loss: 76.8387 - val_mae: 7.2180\n",
            "Epoch 48/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 76.9333 - mae: 7.2166 - val_loss: 77.3986 - val_mae: 7.2388\n",
            "Epoch 49/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 76.0728 - mae: 7.1672 - val_loss: 75.9921 - val_mae: 7.1725\n",
            "Epoch 50/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 75.9843 - mae: 7.1600 - val_loss: 75.5886 - val_mae: 7.1567\n",
            "Epoch 51/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 74.9157 - mae: 7.1109 - val_loss: 74.4163 - val_mae: 7.0841\n",
            "Epoch 52/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 74.7805 - mae: 7.0985 - val_loss: 75.7888 - val_mae: 7.1651\n",
            "Epoch 53/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 74.3238 - mae: 7.0748 - val_loss: 74.0649 - val_mae: 7.0693\n",
            "Epoch 54/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 73.9753 - mae: 7.0661 - val_loss: 73.6437 - val_mae: 7.0317\n",
            "Epoch 55/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 73.5489 - mae: 7.0472 - val_loss: 72.4331 - val_mae: 6.9837\n",
            "Epoch 56/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 73.2066 - mae: 7.0271 - val_loss: 73.8927 - val_mae: 7.0605\n",
            "Epoch 57/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 72.8429 - mae: 7.0044 - val_loss: 72.9106 - val_mae: 7.0078\n",
            "Epoch 58/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 72.4388 - mae: 6.9832 - val_loss: 71.3324 - val_mae: 6.9225\n",
            "Epoch 59/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 71.4277 - mae: 6.9262 - val_loss: 71.5836 - val_mae: 6.9467\n",
            "Epoch 60/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 71.5702 - mae: 6.9367 - val_loss: 70.4934 - val_mae: 6.8712\n",
            "Epoch 61/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 71.0763 - mae: 6.9175 - val_loss: 70.4674 - val_mae: 6.8621\n",
            "Epoch 62/500\n",
            "89/89 [==============================] - 26s 295ms/step - loss: 70.6669 - mae: 6.8909 - val_loss: 70.5473 - val_mae: 6.9075\n",
            "Epoch 63/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 70.2811 - mae: 6.8706 - val_loss: 69.9917 - val_mae: 6.8515\n",
            "Epoch 64/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 69.8966 - mae: 6.8503 - val_loss: 70.1274 - val_mae: 6.8824\n",
            "Epoch 65/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 69.1765 - mae: 6.8036 - val_loss: 69.9599 - val_mae: 6.8580\n",
            "Epoch 66/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 69.0373 - mae: 6.7992 - val_loss: 68.6216 - val_mae: 6.7664\n",
            "Epoch 67/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 68.4381 - mae: 6.7633 - val_loss: 69.1898 - val_mae: 6.8179\n",
            "Epoch 68/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 68.3053 - mae: 6.7613 - val_loss: 68.0675 - val_mae: 6.7417\n",
            "Epoch 69/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 67.5718 - mae: 6.7164 - val_loss: 67.8863 - val_mae: 6.7513\n",
            "Epoch 70/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 67.3864 - mae: 6.7036 - val_loss: 67.3805 - val_mae: 6.7150\n",
            "Epoch 71/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 66.8513 - mae: 6.6816 - val_loss: 66.2939 - val_mae: 6.6568\n",
            "Epoch 72/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 66.5248 - mae: 6.6561 - val_loss: 65.8684 - val_mae: 6.6230\n",
            "Epoch 73/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 66.2241 - mae: 6.6471 - val_loss: 65.7542 - val_mae: 6.6291\n",
            "Epoch 74/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 65.9124 - mae: 6.6240 - val_loss: 65.2884 - val_mae: 6.5967\n",
            "Epoch 75/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 65.6186 - mae: 6.6151 - val_loss: 65.9926 - val_mae: 6.6487\n",
            "Epoch 76/500\n",
            "89/89 [==============================] - 21s 236ms/step - loss: 64.7796 - mae: 6.5743 - val_loss: 64.5778 - val_mae: 6.5544\n",
            "Epoch 77/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 65.0465 - mae: 6.5961 - val_loss: 64.6765 - val_mae: 6.5739\n",
            "Epoch 78/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 64.4084 - mae: 6.5552 - val_loss: 63.2460 - val_mae: 6.4793\n",
            "Epoch 79/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 63.9561 - mae: 6.5317 - val_loss: 63.9569 - val_mae: 6.5214\n",
            "Epoch 80/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 63.3981 - mae: 6.5030 - val_loss: 63.7224 - val_mae: 6.5267\n",
            "Epoch 81/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 63.0562 - mae: 6.4889 - val_loss: 62.6131 - val_mae: 6.4568\n",
            "Epoch 82/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 62.9700 - mae: 6.4837 - val_loss: 62.4857 - val_mae: 6.4590\n",
            "Epoch 83/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 62.6156 - mae: 6.4676 - val_loss: 61.4741 - val_mae: 6.3901\n",
            "Epoch 84/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 61.8000 - mae: 6.4225 - val_loss: 62.1457 - val_mae: 6.4536\n",
            "Epoch 85/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 61.6917 - mae: 6.4243 - val_loss: 61.4885 - val_mae: 6.4111\n",
            "Epoch 86/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 61.3201 - mae: 6.3967 - val_loss: 61.1512 - val_mae: 6.4070\n",
            "Epoch 87/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 61.1268 - mae: 6.3894 - val_loss: 60.6905 - val_mae: 6.3640\n",
            "Epoch 88/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 60.3171 - mae: 6.3390 - val_loss: 60.4621 - val_mae: 6.3521\n",
            "Epoch 89/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 60.0642 - mae: 6.3340 - val_loss: 59.7943 - val_mae: 6.3161\n",
            "Epoch 90/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 59.8830 - mae: 6.3241 - val_loss: 59.2355 - val_mae: 6.2825\n",
            "Epoch 91/500\n",
            "89/89 [==============================] - 23s 249ms/step - loss: 59.3340 - mae: 6.2886 - val_loss: 59.7509 - val_mae: 6.3223\n",
            "Epoch 92/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 59.1624 - mae: 6.2850 - val_loss: 59.2069 - val_mae: 6.2868\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 59.0827 - mae: 6.2813 - val_loss: 58.8166 - val_mae: 6.2760\n",
            "Epoch 94/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 58.7127 - mae: 6.2661 - val_loss: 59.1479 - val_mae: 6.2854\n",
            "Epoch 95/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 58.1882 - mae: 6.2317 - val_loss: 58.5950 - val_mae: 6.2640\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 57.6163 - mae: 6.1982 - val_loss: 57.3036 - val_mae: 6.1688\n",
            "Epoch 97/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 57.6265 - mae: 6.1997 - val_loss: 56.9511 - val_mae: 6.1566\n",
            "Epoch 98/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 57.0912 - mae: 6.1673 - val_loss: 56.6126 - val_mae: 6.1367\n",
            "Epoch 99/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 56.5135 - mae: 6.1375 - val_loss: 56.7021 - val_mae: 6.1460\n",
            "Epoch 100/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 56.5400 - mae: 6.1497 - val_loss: 55.9259 - val_mae: 6.1095\n",
            "Epoch 101/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 55.9969 - mae: 6.1150 - val_loss: 56.0749 - val_mae: 6.1045\n",
            "Epoch 102/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 55.7628 - mae: 6.0912 - val_loss: 54.8774 - val_mae: 6.0452\n",
            "Epoch 103/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 55.2808 - mae: 6.0687 - val_loss: 55.7194 - val_mae: 6.1024\n",
            "Epoch 104/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 55.3382 - mae: 6.0768 - val_loss: 54.5602 - val_mae: 6.0277\n",
            "Epoch 105/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 54.8446 - mae: 6.0433 - val_loss: 55.3101 - val_mae: 6.0835\n",
            "Epoch 106/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 54.4215 - mae: 6.0191 - val_loss: 54.7071 - val_mae: 6.0553\n",
            "Epoch 107/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 54.2269 - mae: 6.0142 - val_loss: 54.3200 - val_mae: 6.0424\n",
            "Epoch 108/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 53.7154 - mae: 5.9826 - val_loss: 53.8517 - val_mae: 6.0040\n",
            "Epoch 109/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 53.7209 - mae: 5.9934 - val_loss: 53.5664 - val_mae: 6.0009\n",
            "Epoch 110/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 53.2976 - mae: 5.9663 - val_loss: 52.5401 - val_mae: 5.9141\n",
            "Epoch 111/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 53.0156 - mae: 5.9585 - val_loss: 52.8391 - val_mae: 5.9547\n",
            "Epoch 112/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 52.6518 - mae: 5.9421 - val_loss: 52.4555 - val_mae: 5.9210\n",
            "Epoch 113/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 52.4252 - mae: 5.9254 - val_loss: 52.1457 - val_mae: 5.9038\n",
            "Epoch 114/500\n",
            "89/89 [==============================] - 26s 292ms/step - loss: 51.9075 - mae: 5.8939 - val_loss: 51.9899 - val_mae: 5.9114\n",
            "Epoch 115/500\n",
            "89/89 [==============================] - 21s 232ms/step - loss: 51.4404 - mae: 5.8734 - val_loss: 51.2482 - val_mae: 5.8650\n",
            "Epoch 116/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 51.5552 - mae: 5.8831 - val_loss: 51.0889 - val_mae: 5.8555\n",
            "Epoch 117/500\n",
            "89/89 [==============================] - 27s 287ms/step - loss: 51.2943 - mae: 5.8721 - val_loss: 51.5453 - val_mae: 5.8817\n",
            "Epoch 118/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 51.0775 - mae: 5.8580 - val_loss: 49.4518 - val_mae: 5.7541\n",
            "Epoch 119/500\n",
            "89/89 [==============================] - 22s 229ms/step - loss: 50.7002 - mae: 5.8363 - val_loss: 50.3139 - val_mae: 5.8106\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 50.3171 - mae: 5.8173 - val_loss: 50.6295 - val_mae: 5.8386\n",
            "Epoch 121/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 49.9843 - mae: 5.8022 - val_loss: 50.3904 - val_mae: 5.8318\n",
            "Epoch 122/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 49.8304 - mae: 5.7898 - val_loss: 49.9705 - val_mae: 5.8066\n",
            "Epoch 123/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 49.6455 - mae: 5.7858 - val_loss: 48.2915 - val_mae: 5.6952\n",
            "Epoch 124/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 49.4802 - mae: 5.7745 - val_loss: 49.0521 - val_mae: 5.7472\n",
            "Epoch 125/500\n",
            "89/89 [==============================] - 22s 245ms/step - loss: 48.9359 - mae: 5.7464 - val_loss: 48.6924 - val_mae: 5.7280\n",
            "Epoch 126/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 48.5564 - mae: 5.7218 - val_loss: 48.2510 - val_mae: 5.6979\n",
            "Epoch 127/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 48.2131 - mae: 5.7017 - val_loss: 48.6948 - val_mae: 5.7480\n",
            "Epoch 128/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 48.1858 - mae: 5.7039 - val_loss: 47.7242 - val_mae: 5.6783\n",
            "Epoch 129/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 47.6089 - mae: 5.6687 - val_loss: 48.2080 - val_mae: 5.7166\n",
            "Epoch 130/500\n",
            "89/89 [==============================] - 27s 285ms/step - loss: 47.4289 - mae: 5.6615 - val_loss: 47.6443 - val_mae: 5.6850\n",
            "Epoch 131/500\n",
            "89/89 [==============================] - 22s 245ms/step - loss: 47.2546 - mae: 5.6502 - val_loss: 47.2033 - val_mae: 5.6510\n",
            "Epoch 132/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 46.9289 - mae: 5.6263 - val_loss: 47.3066 - val_mae: 5.6641\n",
            "Epoch 133/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 46.9729 - mae: 5.6397 - val_loss: 46.2429 - val_mae: 5.5839\n",
            "Epoch 134/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 46.7419 - mae: 5.6273 - val_loss: 46.6685 - val_mae: 5.6079\n",
            "Epoch 135/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 46.4879 - mae: 5.6144 - val_loss: 46.4481 - val_mae: 5.6044\n",
            "Epoch 136/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 45.9971 - mae: 5.5742 - val_loss: 46.3436 - val_mae: 5.5901\n",
            "Epoch 137/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 45.9614 - mae: 5.5786 - val_loss: 45.4143 - val_mae: 5.5385\n",
            "Epoch 138/500\n",
            "89/89 [==============================] - 23s 248ms/step - loss: 45.6911 - mae: 5.5676 - val_loss: 44.9651 - val_mae: 5.5099\n",
            "Epoch 139/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 45.3299 - mae: 5.5459 - val_loss: 45.1692 - val_mae: 5.5288\n",
            "Epoch 140/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 45.1236 - mae: 5.5404 - val_loss: 44.7327 - val_mae: 5.5010\n",
            "Epoch 141/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 44.6064 - mae: 5.5064 - val_loss: 44.6929 - val_mae: 5.5115\n",
            "Epoch 142/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 44.7806 - mae: 5.5299 - val_loss: 44.7356 - val_mae: 5.5347\n",
            "Epoch 143/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 44.2407 - mae: 5.4914 - val_loss: 44.0572 - val_mae: 5.4799\n",
            "Epoch 144/500\n",
            "89/89 [==============================] - 21s 229ms/step - loss: 44.2132 - mae: 5.4923 - val_loss: 44.1897 - val_mae: 5.4982\n",
            "Epoch 145/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 44.0175 - mae: 5.4867 - val_loss: 43.7159 - val_mae: 5.4663\n",
            "Epoch 146/500\n",
            "89/89 [==============================] - 24s 258ms/step - loss: 43.9234 - mae: 5.4854 - val_loss: 43.2482 - val_mae: 5.4410\n",
            "Epoch 147/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 43.6825 - mae: 5.4757 - val_loss: 43.8844 - val_mae: 5.4914\n",
            "Epoch 148/500\n",
            "89/89 [==============================] - 24s 257ms/step - loss: 43.0587 - mae: 5.4314 - val_loss: 42.8910 - val_mae: 5.4281\n",
            "Epoch 149/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 43.1283 - mae: 5.4416 - val_loss: 42.4983 - val_mae: 5.4070\n",
            "Epoch 150/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 42.7384 - mae: 5.4134 - val_loss: 42.6555 - val_mae: 5.4130\n",
            "Epoch 151/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 42.7868 - mae: 5.4294 - val_loss: 42.5604 - val_mae: 5.4047\n",
            "Epoch 152/500\n",
            "89/89 [==============================] - 21s 230ms/step - loss: 42.6048 - mae: 5.4143 - val_loss: 42.0383 - val_mae: 5.3759\n",
            "Epoch 153/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 42.1824 - mae: 5.3919 - val_loss: 42.4588 - val_mae: 5.4251\n",
            "Epoch 154/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 42.0129 - mae: 5.3806 - val_loss: 41.2251 - val_mae: 5.3282\n",
            "Epoch 155/500\n",
            "89/89 [==============================] - 22s 243ms/step - loss: 41.5749 - mae: 5.3549 - val_loss: 41.7852 - val_mae: 5.3770\n",
            "Epoch 156/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 41.6223 - mae: 5.3676 - val_loss: 40.9895 - val_mae: 5.3260\n",
            "Epoch 157/500\n",
            "89/89 [==============================] - 23s 251ms/step - loss: 41.2923 - mae: 5.3458 - val_loss: 41.4952 - val_mae: 5.3571\n",
            "Epoch 158/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 41.3162 - mae: 5.3454 - val_loss: 40.9709 - val_mae: 5.3263\n",
            "Epoch 159/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 40.7049 - mae: 5.3025 - val_loss: 41.2377 - val_mae: 5.3479\n",
            "Epoch 160/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 40.8910 - mae: 5.3295 - val_loss: 40.5939 - val_mae: 5.2911\n",
            "Epoch 161/500\n",
            "89/89 [==============================] - 21s 231ms/step - loss: 40.7455 - mae: 5.3167 - val_loss: 40.7384 - val_mae: 5.3301\n",
            "Epoch 162/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 40.4106 - mae: 5.2941 - val_loss: 40.4946 - val_mae: 5.3098\n",
            "Epoch 163/500\n",
            "89/89 [==============================] - 21s 232ms/step - loss: 40.2726 - mae: 5.2871 - val_loss: 40.4927 - val_mae: 5.3115\n",
            "Epoch 164/500\n",
            "89/89 [==============================] - 21s 231ms/step - loss: 40.1847 - mae: 5.2853 - val_loss: 40.5123 - val_mae: 5.3141\n",
            "Epoch 165/500\n",
            "89/89 [==============================] - 23s 243ms/step - loss: 39.5963 - mae: 5.2390 - val_loss: 39.9713 - val_mae: 5.2623\n",
            "Epoch 166/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 39.5979 - mae: 5.2458 - val_loss: 39.8389 - val_mae: 5.2681\n",
            "Epoch 167/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 39.5662 - mae: 5.2537 - val_loss: 38.8909 - val_mae: 5.1997\n",
            "Epoch 168/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 38.9783 - mae: 5.2043 - val_loss: 38.7184 - val_mae: 5.1847\n",
            "Epoch 169/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 38.9366 - mae: 5.2024 - val_loss: 39.4117 - val_mae: 5.2432\n",
            "Epoch 170/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 38.8028 - mae: 5.1967 - val_loss: 39.0006 - val_mae: 5.2179\n",
            "Epoch 171/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 38.6571 - mae: 5.1923 - val_loss: 38.3817 - val_mae: 5.1763\n",
            "Epoch 172/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 38.6306 - mae: 5.1962 - val_loss: 38.5177 - val_mae: 5.1984\n",
            "Epoch 173/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 38.2723 - mae: 5.1756 - val_loss: 38.5615 - val_mae: 5.2200\n",
            "Epoch 174/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 38.1347 - mae: 5.1705 - val_loss: 37.9279 - val_mae: 5.1547\n",
            "Epoch 175/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 38.1019 - mae: 5.1714 - val_loss: 37.4714 - val_mae: 5.1344\n",
            "Epoch 176/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 37.9645 - mae: 5.1605 - val_loss: 38.1200 - val_mae: 5.1797\n",
            "Epoch 177/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 37.7710 - mae: 5.1560 - val_loss: 37.4415 - val_mae: 5.1297\n",
            "Epoch 178/500\n",
            "89/89 [==============================] - 21s 229ms/step - loss: 37.5783 - mae: 5.1436 - val_loss: 37.1589 - val_mae: 5.1106\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 37.4503 - mae: 5.1400 - val_loss: 37.6723 - val_mae: 5.1582\n",
            "Epoch 180/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 37.4131 - mae: 5.1425 - val_loss: 37.1775 - val_mae: 5.1192\n",
            "Epoch 181/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 37.1450 - mae: 5.1218 - val_loss: 36.8545 - val_mae: 5.1132\n",
            "Epoch 182/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 36.9099 - mae: 5.1106 - val_loss: 36.9936 - val_mae: 5.1034\n",
            "Epoch 183/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 36.8669 - mae: 5.1173 - val_loss: 36.8997 - val_mae: 5.1164\n",
            "Epoch 184/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 36.7092 - mae: 5.1026 - val_loss: 36.8953 - val_mae: 5.1266\n",
            "Epoch 185/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 36.6375 - mae: 5.0981 - val_loss: 36.1767 - val_mae: 5.0730\n",
            "Epoch 186/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 36.2633 - mae: 5.0771 - val_loss: 36.2499 - val_mae: 5.0871\n",
            "Epoch 187/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 36.4205 - mae: 5.0923 - val_loss: 36.5709 - val_mae: 5.1035\n",
            "Epoch 188/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 36.1582 - mae: 5.0743 - val_loss: 36.5396 - val_mae: 5.1178\n",
            "Epoch 189/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 35.8439 - mae: 5.0503 - val_loss: 36.3021 - val_mae: 5.1049\n",
            "Epoch 190/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 35.8642 - mae: 5.0628 - val_loss: 35.7081 - val_mae: 5.0377\n",
            "Epoch 191/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 35.7655 - mae: 5.0585 - val_loss: 35.4211 - val_mae: 5.0302\n",
            "Epoch 192/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 35.6568 - mae: 5.0492 - val_loss: 35.5220 - val_mae: 5.0413\n",
            "Epoch 193/500\n",
            "89/89 [==============================] - 22s 243ms/step - loss: 35.5731 - mae: 5.0478 - val_loss: 35.2418 - val_mae: 5.0108\n",
            "Epoch 194/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 35.2704 - mae: 5.0222 - val_loss: 35.2106 - val_mae: 5.0132\n",
            "Epoch 195/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 35.1362 - mae: 5.0166 - val_loss: 35.4366 - val_mae: 5.0407\n",
            "Epoch 196/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 34.8431 - mae: 5.0005 - val_loss: 35.1939 - val_mae: 5.0280\n",
            "Epoch 197/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 35.1465 - mae: 5.0230 - val_loss: 34.8389 - val_mae: 5.0047\n",
            "Epoch 198/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 34.7514 - mae: 4.9929 - val_loss: 34.9700 - val_mae: 5.0109\n",
            "Epoch 199/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 34.7682 - mae: 4.9996 - val_loss: 34.6060 - val_mae: 4.9793\n",
            "Epoch 200/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 34.3940 - mae: 4.9730 - val_loss: 34.3523 - val_mae: 4.9641\n",
            "Epoch 201/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 34.6682 - mae: 4.9955 - val_loss: 34.7662 - val_mae: 5.0051\n",
            "Epoch 202/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 34.2852 - mae: 4.9613 - val_loss: 34.3234 - val_mae: 4.9621\n",
            "Epoch 203/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 34.2779 - mae: 4.9668 - val_loss: 34.0174 - val_mae: 4.9424\n",
            "Epoch 204/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 34.1630 - mae: 4.9578 - val_loss: 33.9282 - val_mae: 4.9377\n",
            "Epoch 205/500\n",
            "89/89 [==============================] - 22s 231ms/step - loss: 34.0413 - mae: 4.9490 - val_loss: 33.6249 - val_mae: 4.9222\n",
            "Epoch 206/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 34.0535 - mae: 4.9564 - val_loss: 33.9659 - val_mae: 4.9484\n",
            "Epoch 207/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 33.7829 - mae: 4.9387 - val_loss: 34.2593 - val_mae: 4.9824\n",
            "Epoch 208/500\n",
            "89/89 [==============================] - 22s 236ms/step - loss: 33.8550 - mae: 4.9522 - val_loss: 33.6780 - val_mae: 4.9375\n",
            "Epoch 209/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 33.6267 - mae: 4.9337 - val_loss: 33.4391 - val_mae: 4.9118\n",
            "Epoch 210/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 33.5729 - mae: 4.9338 - val_loss: 33.2764 - val_mae: 4.9120\n",
            "Epoch 211/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 33.2351 - mae: 4.9052 - val_loss: 33.2325 - val_mae: 4.9092\n",
            "Epoch 212/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 33.5424 - mae: 4.9400 - val_loss: 32.9569 - val_mae: 4.8958\n",
            "Epoch 213/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 33.2729 - mae: 4.9190 - val_loss: 33.1680 - val_mae: 4.9060\n",
            "Epoch 214/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 33.1617 - mae: 4.9103 - val_loss: 33.4207 - val_mae: 4.9302\n",
            "Epoch 215/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 33.0994 - mae: 4.9121 - val_loss: 33.0332 - val_mae: 4.9103\n",
            "Epoch 216/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 33.1259 - mae: 4.9155 - val_loss: 32.6016 - val_mae: 4.8763\n",
            "Epoch 217/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 32.8420 - mae: 4.8946 - val_loss: 32.7024 - val_mae: 4.8848\n",
            "Epoch 218/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 32.9645 - mae: 4.9094 - val_loss: 32.5896 - val_mae: 4.8797\n",
            "Epoch 219/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 32.8251 - mae: 4.9017 - val_loss: 33.0408 - val_mae: 4.9225\n",
            "Epoch 220/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 32.7271 - mae: 4.8928 - val_loss: 32.3113 - val_mae: 4.8633\n",
            "Epoch 221/500\n",
            "89/89 [==============================] - 21s 223ms/step - loss: 32.5306 - mae: 4.8801 - val_loss: 32.6993 - val_mae: 4.8895\n",
            "Epoch 222/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 32.5121 - mae: 4.8811 - val_loss: 32.3467 - val_mae: 4.8623\n",
            "Epoch 223/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 32.4796 - mae: 4.8820 - val_loss: 32.6262 - val_mae: 4.8983\n",
            "Epoch 224/500\n",
            "89/89 [==============================] - 27s 303ms/step - loss: 32.3353 - mae: 4.8716 - val_loss: 32.4062 - val_mae: 4.8840\n",
            "Epoch 225/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 32.2773 - mae: 4.8706 - val_loss: 32.0830 - val_mae: 4.8652\n",
            "Epoch 226/500\n",
            "89/89 [==============================] - 22s 241ms/step - loss: 32.1921 - mae: 4.8683 - val_loss: 31.6947 - val_mae: 4.8227\n",
            "Epoch 227/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 32.1396 - mae: 4.8610 - val_loss: 31.9160 - val_mae: 4.8496\n",
            "Epoch 228/500\n",
            "89/89 [==============================] - 26s 293ms/step - loss: 31.9849 - mae: 4.8502 - val_loss: 32.0900 - val_mae: 4.8546\n",
            "Epoch 229/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 32.0467 - mae: 4.8579 - val_loss: 31.7040 - val_mae: 4.8302\n",
            "Epoch 230/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 31.8523 - mae: 4.8416 - val_loss: 31.9295 - val_mae: 4.8514\n",
            "Epoch 231/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 31.8625 - mae: 4.8448 - val_loss: 31.8475 - val_mae: 4.8522\n",
            "Epoch 232/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 31.8750 - mae: 4.8473 - val_loss: 31.4677 - val_mae: 4.8122\n",
            "Epoch 233/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 31.6732 - mae: 4.8282 - val_loss: 31.7984 - val_mae: 4.8385\n",
            "Epoch 234/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 31.7595 - mae: 4.8383 - val_loss: 31.9225 - val_mae: 4.8636\n",
            "Epoch 235/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 31.6986 - mae: 4.8349 - val_loss: 31.6638 - val_mae: 4.8401\n",
            "Epoch 236/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 31.5879 - mae: 4.8309 - val_loss: 31.5100 - val_mae: 4.8132\n",
            "Epoch 237/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 31.5241 - mae: 4.8241 - val_loss: 31.7260 - val_mae: 4.8410\n",
            "Epoch 238/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 31.5918 - mae: 4.8326 - val_loss: 31.4174 - val_mae: 4.8247\n",
            "Epoch 239/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 31.3968 - mae: 4.8145 - val_loss: 31.8399 - val_mae: 4.8680\n",
            "Epoch 240/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 31.3838 - mae: 4.8133 - val_loss: 31.2961 - val_mae: 4.8039\n",
            "Epoch 241/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 31.3692 - mae: 4.8129 - val_loss: 30.8111 - val_mae: 4.7656\n",
            "Epoch 242/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 31.2989 - mae: 4.8129 - val_loss: 31.1169 - val_mae: 4.7988\n",
            "Epoch 243/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 31.2675 - mae: 4.8113 - val_loss: 31.4118 - val_mae: 4.8286\n",
            "Epoch 244/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 31.2522 - mae: 4.8148 - val_loss: 31.3237 - val_mae: 4.8133\n",
            "Epoch 245/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 31.1615 - mae: 4.7981 - val_loss: 31.0053 - val_mae: 4.7798\n",
            "Epoch 246/500\n",
            "89/89 [==============================] - 21s 232ms/step - loss: 31.0690 - mae: 4.7944 - val_loss: 31.0509 - val_mae: 4.7815\n",
            "Epoch 247/500\n",
            "89/89 [==============================] - 27s 285ms/step - loss: 31.0888 - mae: 4.7936 - val_loss: 30.8279 - val_mae: 4.7756\n",
            "Epoch 248/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 28.7745 - mae: 4.5098 - val_loss: 24.8514 - val_mae: 4.1090\n",
            "Epoch 249/500\n",
            "89/89 [==============================] - 27s 285ms/step - loss: 25.8610 - mae: 4.1812 - val_loss: 24.3503 - val_mae: 3.9580\n",
            "Epoch 250/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 24.6608 - mae: 4.0604 - val_loss: 23.7316 - val_mae: 3.9840\n",
            "Epoch 251/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 24.8598 - mae: 4.0723 - val_loss: 24.1605 - val_mae: 3.9281\n",
            "Epoch 252/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 24.1778 - mae: 3.9853 - val_loss: 23.5386 - val_mae: 3.9304\n",
            "Epoch 253/500\n",
            "89/89 [==============================] - 27s 304ms/step - loss: 23.4894 - mae: 3.9144 - val_loss: 22.9805 - val_mae: 3.9051\n",
            "Epoch 254/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 23.1726 - mae: 3.9015 - val_loss: 22.3588 - val_mae: 3.8357\n",
            "Epoch 255/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 23.0409 - mae: 3.9080 - val_loss: 22.4071 - val_mae: 3.8630\n",
            "Epoch 256/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 23.3100 - mae: 3.9649 - val_loss: 22.4132 - val_mae: 3.7852\n",
            "Epoch 257/500\n",
            "89/89 [==============================] - 27s 303ms/step - loss: 22.8723 - mae: 3.8807 - val_loss: 22.2663 - val_mae: 3.7187\n",
            "Epoch 258/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 22.5577 - mae: 3.8506 - val_loss: 21.5901 - val_mae: 3.7403\n",
            "Epoch 259/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 22.5057 - mae: 3.8499 - val_loss: 21.3818 - val_mae: 3.7037\n",
            "Epoch 260/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 22.1335 - mae: 3.8236 - val_loss: 21.5085 - val_mae: 3.7354\n",
            "Epoch 261/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 22.1182 - mae: 3.8097 - val_loss: 21.5138 - val_mae: 3.7279\n",
            "Epoch 262/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 21.8461 - mae: 3.8002 - val_loss: 21.3995 - val_mae: 3.7031\n",
            "Epoch 263/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 21.8151 - mae: 3.7973 - val_loss: 20.9835 - val_mae: 3.6741\n",
            "Epoch 264/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 21.5522 - mae: 3.7604 - val_loss: 20.8440 - val_mae: 3.6957\n",
            "Epoch 265/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 21.1984 - mae: 3.7395 - val_loss: 21.0098 - val_mae: 3.6636\n",
            "Epoch 266/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 21.3067 - mae: 3.7506 - val_loss: 20.8296 - val_mae: 3.6786\n",
            "Epoch 267/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 21.3163 - mae: 3.7281 - val_loss: 20.3152 - val_mae: 3.6470\n",
            "Epoch 268/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 21.1421 - mae: 3.7435 - val_loss: 20.1892 - val_mae: 3.6445\n",
            "Epoch 269/500\n",
            "89/89 [==============================] - 27s 305ms/step - loss: 20.6962 - mae: 3.6852 - val_loss: 20.4081 - val_mae: 3.6143\n",
            "Epoch 270/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 20.5564 - mae: 3.6603 - val_loss: 19.9488 - val_mae: 3.5576\n",
            "Epoch 271/500\n",
            "89/89 [==============================] - 22s 245ms/step - loss: 20.4800 - mae: 3.6700 - val_loss: 19.7259 - val_mae: 3.5622\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 20.3393 - mae: 3.6782 - val_loss: 20.2363 - val_mae: 3.5618\n",
            "Epoch 273/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 20.1437 - mae: 3.6318 - val_loss: 19.2564 - val_mae: 3.4997\n",
            "Epoch 274/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 20.0428 - mae: 3.6366 - val_loss: 19.8040 - val_mae: 3.5383\n",
            "Epoch 275/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 20.1951 - mae: 3.6582 - val_loss: 19.8465 - val_mae: 3.5314\n",
            "Epoch 276/500\n",
            "89/89 [==============================] - 21s 232ms/step - loss: 20.1976 - mae: 3.6812 - val_loss: 19.2477 - val_mae: 3.4909\n",
            "Epoch 277/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 19.9508 - mae: 3.6243 - val_loss: 19.9790 - val_mae: 3.4938\n",
            "Epoch 278/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 19.2796 - mae: 3.5503 - val_loss: 19.3496 - val_mae: 3.4862\n",
            "Epoch 279/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 19.9934 - mae: 3.6352 - val_loss: 18.8759 - val_mae: 3.4606\n",
            "Epoch 280/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 19.5978 - mae: 3.5923 - val_loss: 18.9353 - val_mae: 3.4739\n",
            "Epoch 281/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 19.1607 - mae: 3.5578 - val_loss: 19.8484 - val_mae: 3.4971\n",
            "Epoch 282/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 19.3338 - mae: 3.5449 - val_loss: 18.9806 - val_mae: 3.4361\n",
            "Epoch 283/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 18.8721 - mae: 3.5110 - val_loss: 20.0647 - val_mae: 3.5071\n",
            "Epoch 284/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 19.1784 - mae: 3.5549 - val_loss: 18.9705 - val_mae: 3.4247\n",
            "Epoch 285/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 19.0171 - mae: 3.5598 - val_loss: 20.8994 - val_mae: 3.5559\n",
            "Epoch 286/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 18.6461 - mae: 3.5029 - val_loss: 17.9343 - val_mae: 3.3444\n",
            "Epoch 287/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 18.3806 - mae: 3.4691 - val_loss: 17.9097 - val_mae: 3.3627\n",
            "Epoch 288/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 18.5648 - mae: 3.5060 - val_loss: 17.8734 - val_mae: 3.3681\n",
            "Epoch 289/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 18.5563 - mae: 3.5193 - val_loss: 17.9851 - val_mae: 3.3589\n",
            "Epoch 290/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 18.5825 - mae: 3.5036 - val_loss: 17.7546 - val_mae: 3.3800\n",
            "Epoch 291/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 18.0263 - mae: 3.4413 - val_loss: 17.3070 - val_mae: 3.3418\n",
            "Epoch 292/500\n",
            "89/89 [==============================] - 21s 233ms/step - loss: 18.4343 - mae: 3.4983 - val_loss: 18.2543 - val_mae: 3.3789\n",
            "Epoch 293/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 18.4947 - mae: 3.5150 - val_loss: 17.3092 - val_mae: 3.3079\n",
            "Epoch 294/500\n",
            "89/89 [==============================] - 23s 246ms/step - loss: 17.9776 - mae: 3.4413 - val_loss: 17.4429 - val_mae: 3.3539\n",
            "Epoch 295/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 18.2024 - mae: 3.4941 - val_loss: 17.7149 - val_mae: 3.3458\n",
            "Epoch 296/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 17.8965 - mae: 3.4657 - val_loss: 17.2426 - val_mae: 3.3414\n",
            "Epoch 297/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 17.8598 - mae: 3.4432 - val_loss: 17.9122 - val_mae: 3.3455\n",
            "Epoch 298/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 17.4095 - mae: 3.4056 - val_loss: 17.7502 - val_mae: 3.3232\n",
            "Epoch 299/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 17.4211 - mae: 3.4139 - val_loss: 17.2405 - val_mae: 3.2972\n",
            "Epoch 300/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 16.9187 - mae: 3.3551 - val_loss: 17.6090 - val_mae: 3.3234\n",
            "Epoch 301/500\n",
            "89/89 [==============================] - 21s 233ms/step - loss: 17.2086 - mae: 3.3908 - val_loss: 16.9912 - val_mae: 3.2972\n",
            "Epoch 302/500\n",
            "89/89 [==============================] - 27s 288ms/step - loss: 17.1390 - mae: 3.3907 - val_loss: 16.8485 - val_mae: 3.2807\n",
            "Epoch 303/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 17.2834 - mae: 3.3956 - val_loss: 16.6130 - val_mae: 3.2664\n",
            "Epoch 304/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 17.1073 - mae: 3.3630 - val_loss: 16.6881 - val_mae: 3.2941\n",
            "Epoch 305/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 16.7050 - mae: 3.3379 - val_loss: 16.5292 - val_mae: 3.2638\n",
            "Epoch 306/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 16.8490 - mae: 3.3501 - val_loss: 17.9198 - val_mae: 3.3365\n",
            "Epoch 307/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 16.6250 - mae: 3.3309 - val_loss: 17.8642 - val_mae: 3.3315\n",
            "Epoch 308/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 17.1360 - mae: 3.3704 - val_loss: 16.2297 - val_mae: 3.2460\n",
            "Epoch 309/500\n",
            "89/89 [==============================] - 23s 249ms/step - loss: 16.6684 - mae: 3.3651 - val_loss: 17.1748 - val_mae: 3.2796\n",
            "Epoch 310/500\n",
            "89/89 [==============================] - 21s 232ms/step - loss: 16.5403 - mae: 3.3260 - val_loss: 15.9004 - val_mae: 3.1992\n",
            "Epoch 311/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 16.8974 - mae: 3.3777 - val_loss: 15.5758 - val_mae: 3.1731\n",
            "Epoch 312/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 16.8042 - mae: 3.3656 - val_loss: 15.8968 - val_mae: 3.2035\n",
            "Epoch 313/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 16.3236 - mae: 3.3093 - val_loss: 15.8028 - val_mae: 3.1996\n",
            "Epoch 314/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 16.5648 - mae: 3.3267 - val_loss: 16.1875 - val_mae: 3.2150\n",
            "Epoch 315/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 16.3343 - mae: 3.3049 - val_loss: 15.9854 - val_mae: 3.1875\n",
            "Epoch 316/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 16.3701 - mae: 3.3272 - val_loss: 15.9293 - val_mae: 3.1892\n",
            "Epoch 317/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 16.1414 - mae: 3.2696 - val_loss: 16.1436 - val_mae: 3.1817\n",
            "Epoch 318/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 16.1185 - mae: 3.2857 - val_loss: 16.7093 - val_mae: 3.2456\n",
            "Epoch 319/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 16.2662 - mae: 3.2759 - val_loss: 15.7841 - val_mae: 3.1700\n",
            "Epoch 320/500\n",
            "89/89 [==============================] - 21s 233ms/step - loss: 16.0636 - mae: 3.2706 - val_loss: 16.4801 - val_mae: 3.2034\n",
            "Epoch 321/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 15.6311 - mae: 3.2129 - val_loss: 16.1886 - val_mae: 3.1829\n",
            "Epoch 322/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 15.6117 - mae: 3.2275 - val_loss: 15.0168 - val_mae: 3.1169\n",
            "Epoch 323/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 15.8996 - mae: 3.2640 - val_loss: 15.2352 - val_mae: 3.1485\n",
            "Epoch 324/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 15.5072 - mae: 3.2140 - val_loss: 16.2192 - val_mae: 3.1904\n",
            "Epoch 325/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 15.9595 - mae: 3.2516 - val_loss: 15.6525 - val_mae: 3.1363\n",
            "Epoch 326/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 15.9833 - mae: 3.2512 - val_loss: 15.4336 - val_mae: 3.1334\n",
            "Epoch 327/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 15.6181 - mae: 3.2344 - val_loss: 16.2278 - val_mae: 3.1689\n",
            "Epoch 328/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 15.3159 - mae: 3.2072 - val_loss: 16.6796 - val_mae: 3.2034\n",
            "Epoch 329/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 15.4790 - mae: 3.2276 - val_loss: 16.3945 - val_mae: 3.1803\n",
            "Epoch 330/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 15.4246 - mae: 3.2370 - val_loss: 22.0687 - val_mae: 3.6653\n",
            "Epoch 331/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 15.6379 - mae: 3.2251 - val_loss: 14.5715 - val_mae: 3.0478\n",
            "Epoch 332/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 15.2448 - mae: 3.2285 - val_loss: 15.4173 - val_mae: 3.1045\n",
            "Epoch 333/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 14.8102 - mae: 3.1638 - val_loss: 16.1023 - val_mae: 3.1640\n",
            "Epoch 334/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 14.7147 - mae: 3.1447 - val_loss: 16.3459 - val_mae: 3.1826\n",
            "Epoch 335/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 14.8787 - mae: 3.1589 - val_loss: 14.0031 - val_mae: 3.0005\n",
            "Epoch 336/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 15.2779 - mae: 3.2408 - val_loss: 14.7227 - val_mae: 3.0449\n",
            "Epoch 337/500\n",
            "89/89 [==============================] - 26s 276ms/step - loss: 15.0004 - mae: 3.1774 - val_loss: 16.9297 - val_mae: 3.2270\n",
            "Epoch 338/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 14.9414 - mae: 3.1757 - val_loss: 14.7434 - val_mae: 3.0324\n",
            "Epoch 339/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 14.4519 - mae: 3.1203 - val_loss: 14.1101 - val_mae: 3.0029\n",
            "Epoch 340/500\n",
            "89/89 [==============================] - 26s 276ms/step - loss: 14.8242 - mae: 3.1845 - val_loss: 15.0955 - val_mae: 3.0825\n",
            "Epoch 341/500\n",
            "89/89 [==============================] - 20s 221ms/step - loss: 14.9045 - mae: 3.1490 - val_loss: 21.1140 - val_mae: 3.5837\n",
            "Epoch 342/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 14.5750 - mae: 3.1204 - val_loss: 15.3928 - val_mae: 3.1000\n",
            "Epoch 343/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 15.0374 - mae: 3.2230 - val_loss: 14.9260 - val_mae: 3.0530\n",
            "Epoch 344/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 15.1355 - mae: 3.2113 - val_loss: 14.2819 - val_mae: 3.0067\n",
            "Epoch 345/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 14.0513 - mae: 3.0902 - val_loss: 17.5824 - val_mae: 3.2825\n",
            "Epoch 346/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 14.8135 - mae: 3.1679 - val_loss: 15.2780 - val_mae: 3.0924\n",
            "Epoch 347/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 14.6696 - mae: 3.1566 - val_loss: 16.1455 - val_mae: 3.1605\n",
            "Epoch 348/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 14.1712 - mae: 3.1147 - val_loss: 16.6445 - val_mae: 3.2001\n",
            "Epoch 349/500\n",
            "89/89 [==============================] - 21s 233ms/step - loss: 14.0489 - mae: 3.0687 - val_loss: 13.8369 - val_mae: 2.9741\n",
            "Epoch 350/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 14.3134 - mae: 3.1394 - val_loss: 14.0145 - val_mae: 2.9915\n",
            "Epoch 351/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 14.5048 - mae: 3.1326 - val_loss: 15.4135 - val_mae: 3.0871\n",
            "Epoch 352/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 14.0921 - mae: 3.0911 - val_loss: 14.7259 - val_mae: 3.0515\n",
            "Epoch 353/500\n",
            "89/89 [==============================] - 23s 245ms/step - loss: 14.5667 - mae: 3.1463 - val_loss: 15.7417 - val_mae: 3.1191\n",
            "Epoch 354/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 14.0229 - mae: 3.0911 - val_loss: 14.6805 - val_mae: 3.0253\n",
            "Epoch 355/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 14.8259 - mae: 3.1626 - val_loss: 16.5672 - val_mae: 3.1896\n",
            "Epoch 356/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 14.1668 - mae: 3.1077 - val_loss: 15.5458 - val_mae: 3.0982\n",
            "Epoch 357/500\n",
            "89/89 [==============================] - 22s 242ms/step - loss: 14.1756 - mae: 3.0806 - val_loss: 13.2768 - val_mae: 2.9541\n",
            "Epoch 358/500\n",
            "89/89 [==============================] - 23s 246ms/step - loss: 14.2722 - mae: 3.0921 - val_loss: 15.1676 - val_mae: 3.0729\n",
            "Epoch 359/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 14.1776 - mae: 3.0763 - val_loss: 14.9095 - val_mae: 3.0435\n",
            "Epoch 360/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 14.3370 - mae: 3.1321 - val_loss: 13.5454 - val_mae: 2.9231\n",
            "Epoch 361/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 13.9261 - mae: 3.0703 - val_loss: 14.1830 - val_mae: 2.9780\n",
            "Epoch 362/500\n",
            "89/89 [==============================] - 20s 224ms/step - loss: 13.8264 - mae: 3.0755 - val_loss: 15.8021 - val_mae: 3.1172\n",
            "Epoch 363/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 13.4982 - mae: 3.0084 - val_loss: 13.7608 - val_mae: 2.9451\n",
            "Epoch 364/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 13.9037 - mae: 3.0680 - val_loss: 13.5220 - val_mae: 2.9361\n",
            "Epoch 365/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 13.8503 - mae: 3.0874 - val_loss: 14.9290 - val_mae: 3.0330\n",
            "Epoch 366/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 14.0748 - mae: 3.1015 - val_loss: 15.5026 - val_mae: 3.0991\n",
            "Epoch 367/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 13.5083 - mae: 3.0201 - val_loss: 19.2145 - val_mae: 3.4106\n",
            "Epoch 368/500\n",
            "89/89 [==============================] - 23s 249ms/step - loss: 13.5075 - mae: 3.0138 - val_loss: 16.8416 - val_mae: 3.2094\n",
            "Epoch 369/500\n",
            "89/89 [==============================] - 27s 304ms/step - loss: 13.7725 - mae: 3.0615 - val_loss: 14.0944 - val_mae: 2.9603\n",
            "Epoch 370/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 13.8514 - mae: 3.0992 - val_loss: 14.3278 - val_mae: 2.9897\n",
            "Epoch 371/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 13.6103 - mae: 3.0229 - val_loss: 18.2979 - val_mae: 3.3402\n",
            "Epoch 372/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 13.6233 - mae: 3.0208 - val_loss: 14.5057 - val_mae: 3.0085\n",
            "Epoch 373/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 14.1475 - mae: 3.1093 - val_loss: 12.8597 - val_mae: 2.8874\n",
            "Epoch 374/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 13.0917 - mae: 2.9665 - val_loss: 14.0914 - val_mae: 2.9610\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 13.9027 - mae: 3.0631 - val_loss: 13.3176 - val_mae: 2.8960\n",
            "Epoch 376/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 13.0273 - mae: 2.9858 - val_loss: 15.0574 - val_mae: 3.0369\n",
            "Epoch 377/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 13.4232 - mae: 3.0199 - val_loss: 19.3946 - val_mae: 3.4132\n",
            "Epoch 378/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 13.9235 - mae: 3.0546 - val_loss: 15.2764 - val_mae: 3.0668\n",
            "Epoch 379/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 13.6159 - mae: 3.0305 - val_loss: 15.0567 - val_mae: 3.0547\n",
            "Epoch 380/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 13.5836 - mae: 3.0263 - val_loss: 17.3838 - val_mae: 3.2640\n",
            "Epoch 381/500\n",
            "89/89 [==============================] - 27s 279ms/step - loss: 13.2348 - mae: 3.0094 - val_loss: 13.7738 - val_mae: 2.9402\n",
            "Epoch 382/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 13.1880 - mae: 2.9973 - val_loss: 17.4027 - val_mae: 3.2581\n",
            "Epoch 383/500\n",
            "89/89 [==============================] - 28s 297ms/step - loss: 13.1796 - mae: 2.9635 - val_loss: 15.0934 - val_mae: 3.0549\n",
            "Epoch 384/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 13.5285 - mae: 3.0211 - val_loss: 13.1504 - val_mae: 2.8826\n",
            "Epoch 385/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 13.3215 - mae: 3.0007 - val_loss: 14.0230 - val_mae: 2.9575\n",
            "Epoch 386/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 13.1165 - mae: 2.9598 - val_loss: 13.9307 - val_mae: 2.9475\n",
            "Epoch 387/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 13.6575 - mae: 3.0606 - val_loss: 14.9027 - val_mae: 3.0331\n",
            "Epoch 388/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 13.6008 - mae: 3.0459 - val_loss: 13.7020 - val_mae: 2.9283\n",
            "Epoch 389/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 13.2888 - mae: 3.0073 - val_loss: 12.2907 - val_mae: 2.8424\n",
            "Epoch 390/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 13.0204 - mae: 2.9783 - val_loss: 13.1788 - val_mae: 2.8878\n",
            "Epoch 391/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 13.1114 - mae: 2.9830 - val_loss: 14.0635 - val_mae: 2.9591\n",
            "Epoch 392/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 13.3106 - mae: 2.9745 - val_loss: 14.3751 - val_mae: 2.9737\n",
            "Epoch 393/500\n",
            "89/89 [==============================] - 24s 264ms/step - loss: 13.4625 - mae: 3.0318 - val_loss: 18.7689 - val_mae: 3.3644\n",
            "Epoch 394/500\n",
            "89/89 [==============================] - 21s 222ms/step - loss: 13.3969 - mae: 2.9910 - val_loss: 14.1495 - val_mae: 2.9806\n",
            "Epoch 395/500\n",
            "89/89 [==============================] - 22s 244ms/step - loss: 13.6284 - mae: 3.0359 - val_loss: 14.7521 - val_mae: 3.0285\n",
            "Epoch 396/500\n",
            "89/89 [==============================] - 22s 241ms/step - loss: 13.3176 - mae: 3.0060 - val_loss: 15.3479 - val_mae: 3.0610\n",
            "Epoch 397/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 12.8772 - mae: 2.9588 - val_loss: 18.2076 - val_mae: 3.3084\n",
            "Epoch 398/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 12.8057 - mae: 2.9465 - val_loss: 17.6727 - val_mae: 3.2618\n",
            "Epoch 399/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 12.7322 - mae: 2.9519 - val_loss: 18.9836 - val_mae: 3.3730\n",
            "Epoch 400/500\n",
            "89/89 [==============================] - 21s 230ms/step - loss: 13.2670 - mae: 3.0157 - val_loss: 24.1962 - val_mae: 3.7967\n",
            "Epoch 401/500\n",
            "89/89 [==============================] - 22s 241ms/step - loss: 13.2679 - mae: 2.9832 - val_loss: 19.8374 - val_mae: 3.4531\n",
            "Epoch 402/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 12.9419 - mae: 2.9577 - val_loss: 17.5182 - val_mae: 3.2287\n",
            "Epoch 403/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 12.9328 - mae: 2.9662 - val_loss: 15.6592 - val_mae: 3.0702\n",
            "Epoch 404/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 12.8735 - mae: 2.9482 - val_loss: 14.4098 - val_mae: 2.9725\n",
            "Epoch 405/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 12.6215 - mae: 2.9254 - val_loss: 16.5758 - val_mae: 3.1644\n",
            "Epoch 406/500\n",
            "89/89 [==============================] - 22s 241ms/step - loss: 13.3569 - mae: 2.9755 - val_loss: 16.1983 - val_mae: 3.1549\n",
            "Epoch 407/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 12.8531 - mae: 2.9618 - val_loss: 13.4452 - val_mae: 2.9160\n",
            "Epoch 408/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 13.3654 - mae: 2.9934 - val_loss: 14.9604 - val_mae: 3.0434\n",
            "Epoch 409/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 12.5425 - mae: 2.9052 - val_loss: 14.4085 - val_mae: 2.9928\n",
            "Epoch 410/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 13.3323 - mae: 2.9932 - val_loss: 15.0626 - val_mae: 3.0388\n",
            "Epoch 411/500\n",
            "89/89 [==============================] - 21s 223ms/step - loss: 12.6148 - mae: 2.9372 - val_loss: 16.4562 - val_mae: 3.1605\n",
            "Epoch 412/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 12.9736 - mae: 2.9582 - val_loss: 13.1879 - val_mae: 2.8847\n",
            "Epoch 413/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 13.0492 - mae: 2.9512 - val_loss: 12.8371 - val_mae: 2.8362\n",
            "Epoch 414/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 13.8196 - mae: 3.0598 - val_loss: 11.7024 - val_mae: 2.7799\n",
            "Epoch 415/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 13.0443 - mae: 2.9695 - val_loss: 14.5063 - val_mae: 2.9892\n",
            "Epoch 416/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 13.2978 - mae: 2.9829 - val_loss: 15.3577 - val_mae: 3.0526\n",
            "Epoch 417/500\n",
            "89/89 [==============================] - 22s 245ms/step - loss: 12.9175 - mae: 2.9454 - val_loss: 25.3871 - val_mae: 3.8862\n",
            "Epoch 418/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 13.1092 - mae: 2.9766 - val_loss: 16.6412 - val_mae: 3.1742\n",
            "Epoch 419/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 13.6067 - mae: 3.0210 - val_loss: 11.7069 - val_mae: 2.7672\n",
            "Epoch 420/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 12.8412 - mae: 2.9554 - val_loss: 13.2194 - val_mae: 2.8866\n",
            "Epoch 421/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 12.8857 - mae: 2.9576 - val_loss: 12.2882 - val_mae: 2.8050\n",
            "Epoch 422/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 12.7284 - mae: 2.9303 - val_loss: 13.4491 - val_mae: 2.8879\n",
            "Epoch 423/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 12.1273 - mae: 2.8941 - val_loss: 15.3016 - val_mae: 3.0574\n",
            "Epoch 424/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 13.5552 - mae: 3.0203 - val_loss: 13.8061 - val_mae: 2.9195\n",
            "Epoch 425/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 13.1017 - mae: 2.9783 - val_loss: 12.9768 - val_mae: 2.8456\n",
            "Epoch 426/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 12.9223 - mae: 2.9346 - val_loss: 12.0958 - val_mae: 2.7728\n",
            "Epoch 427/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 12.7241 - mae: 2.9301 - val_loss: 14.9112 - val_mae: 3.0055\n",
            "Epoch 428/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 12.9725 - mae: 2.9587 - val_loss: 17.8628 - val_mae: 3.2648\n",
            "Epoch 429/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 12.7659 - mae: 2.9320 - val_loss: 17.7681 - val_mae: 3.2713\n",
            "Epoch 430/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 12.7422 - mae: 2.9170 - val_loss: 12.6335 - val_mae: 2.8239\n",
            "Epoch 431/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 12.0635 - mae: 2.8341 - val_loss: 23.6528 - val_mae: 3.7439\n",
            "Epoch 432/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 12.7817 - mae: 2.9204 - val_loss: 14.8829 - val_mae: 2.9976\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - 22s 237ms/step - loss: 13.3369 - mae: 3.0002 - val_loss: 13.8987 - val_mae: 2.9226\n",
            "Epoch 434/500\n",
            "89/89 [==============================] - 22s 230ms/step - loss: 12.7118 - mae: 2.9094 - val_loss: 14.7289 - val_mae: 2.9889\n",
            "Epoch 435/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 12.4989 - mae: 2.9054 - val_loss: 13.3918 - val_mae: 2.8927\n",
            "Epoch 436/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 12.5184 - mae: 2.8869 - val_loss: 15.8888 - val_mae: 3.1073\n",
            "Epoch 437/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 12.8932 - mae: 2.9273 - val_loss: 11.4680 - val_mae: 2.7501\n",
            "Epoch 438/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 12.7711 - mae: 2.9411 - val_loss: 14.0784 - val_mae: 2.9539\n",
            "Epoch 439/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 12.7143 - mae: 2.9164 - val_loss: 18.6301 - val_mae: 3.3388\n",
            "Epoch 440/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 12.8868 - mae: 2.9501 - val_loss: 13.9970 - val_mae: 2.9338\n",
            "Epoch 441/500\n",
            "89/89 [==============================] - 29s 323ms/step - loss: 12.6268 - mae: 2.9109 - val_loss: 12.6904 - val_mae: 2.8084\n",
            "Epoch 442/500\n",
            "89/89 [==============================] - 22s 239ms/step - loss: 12.8008 - mae: 2.9161 - val_loss: 17.2112 - val_mae: 3.2318\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - 28s 296ms/step - loss: 12.4863 - mae: 2.8916 - val_loss: 19.6081 - val_mae: 3.4014\n",
            "Epoch 444/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 12.9122 - mae: 2.8957 - val_loss: 14.2294 - val_mae: 2.9373\n",
            "Epoch 445/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 12.3571 - mae: 2.8945 - val_loss: 14.8411 - val_mae: 3.0018\n",
            "Epoch 446/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 12.3294 - mae: 2.8879 - val_loss: 16.8411 - val_mae: 3.1856\n",
            "Epoch 447/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 12.9665 - mae: 2.9271 - val_loss: 19.7146 - val_mae: 3.4037\n",
            "Epoch 448/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 12.6176 - mae: 2.9031 - val_loss: 13.3276 - val_mae: 2.8503\n",
            "Epoch 449/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 12.8562 - mae: 2.9360 - val_loss: 17.4782 - val_mae: 3.2074\n",
            "Epoch 450/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 11.9735 - mae: 2.8271 - val_loss: 12.7211 - val_mae: 2.7959\n",
            "Epoch 451/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 12.1911 - mae: 2.8351 - val_loss: 17.0845 - val_mae: 3.1786\n",
            "Epoch 452/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 12.9905 - mae: 2.9656 - val_loss: 14.3234 - val_mae: 2.9494\n",
            "Epoch 453/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 12.3079 - mae: 2.8622 - val_loss: 13.7923 - val_mae: 2.8993\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 12.6570 - mae: 2.9020 - val_loss: 12.4040 - val_mae: 2.7694\n",
            "Epoch 455/500\n",
            "89/89 [==============================] - 22s 227ms/step - loss: 13.0831 - mae: 2.9078 - val_loss: 12.9706 - val_mae: 2.8300\n",
            "Epoch 456/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 12.4956 - mae: 2.8979 - val_loss: 12.1003 - val_mae: 2.7916\n",
            "Epoch 457/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 11.8433 - mae: 2.8208 - val_loss: 13.5508 - val_mae: 2.8763\n",
            "Epoch 458/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 12.4310 - mae: 2.8692 - val_loss: 11.9309 - val_mae: 2.7672\n",
            "Epoch 459/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 12.8869 - mae: 2.9535 - val_loss: 12.8503 - val_mae: 2.8041\n",
            "Epoch 460/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 12.8534 - mae: 2.9079 - val_loss: 12.1957 - val_mae: 2.7694\n",
            "Epoch 461/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 12.2815 - mae: 2.8636 - val_loss: 15.6802 - val_mae: 3.0523\n",
            "Epoch 462/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 12.7675 - mae: 2.9034 - val_loss: 15.6348 - val_mae: 3.0575\n",
            "Epoch 463/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 12.5377 - mae: 2.8702 - val_loss: 12.8061 - val_mae: 2.7918\n",
            "Epoch 464/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 12.1298 - mae: 2.8485 - val_loss: 12.4224 - val_mae: 2.7490\n",
            "Epoch 465/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 12.9603 - mae: 2.9545 - val_loss: 13.4572 - val_mae: 2.8331\n",
            "Epoch 466/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 12.1409 - mae: 2.8462 - val_loss: 14.5124 - val_mae: 2.9396\n",
            "Epoch 467/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 12.7060 - mae: 2.9177 - val_loss: 12.0660 - val_mae: 2.7222\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - 28s 314ms/step - loss: 11.9552 - mae: 2.8110 - val_loss: 12.5354 - val_mae: 2.7489\n",
            "Epoch 469/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 12.1557 - mae: 2.8601 - val_loss: 12.9329 - val_mae: 2.7923\n",
            "Epoch 470/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 12.3892 - mae: 2.8668 - val_loss: 11.3084 - val_mae: 2.7177\n",
            "Epoch 471/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 12.4359 - mae: 2.8982 - val_loss: 12.5312 - val_mae: 2.7839\n",
            "Epoch 472/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 12.3070 - mae: 2.8603 - val_loss: 15.5152 - val_mae: 3.0292\n",
            "Epoch 473/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 12.4533 - mae: 2.8721 - val_loss: 13.6893 - val_mae: 2.8652\n",
            "Epoch 474/500\n",
            "89/89 [==============================] - 29s 319ms/step - loss: 12.4047 - mae: 2.8549 - val_loss: 12.8497 - val_mae: 2.7898\n",
            "Epoch 475/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 12.6185 - mae: 2.8732 - val_loss: 13.7836 - val_mae: 2.8754\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 12.7401 - mae: 2.9061 - val_loss: 11.3910 - val_mae: 2.7357\n",
            "Epoch 477/500\n",
            "89/89 [==============================] - 21s 230ms/step - loss: 13.0038 - mae: 2.9272 - val_loss: 11.4389 - val_mae: 2.7168\n",
            "Epoch 478/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 12.9774 - mae: 2.9037 - val_loss: 11.2051 - val_mae: 2.7247\n",
            "Epoch 479/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 12.6445 - mae: 2.8835 - val_loss: 11.0162 - val_mae: 2.6863\n",
            "Epoch 480/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 12.2088 - mae: 2.8713 - val_loss: 12.2914 - val_mae: 2.9062\n",
            "Epoch 481/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 12.2939 - mae: 2.8482 - val_loss: 11.6992 - val_mae: 2.7174\n",
            "Epoch 482/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 12.0700 - mae: 2.8343 - val_loss: 11.5046 - val_mae: 2.7175\n",
            "Epoch 483/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 12.8353 - mae: 2.8918 - val_loss: 11.8411 - val_mae: 2.7171\n",
            "Epoch 484/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 12.7539 - mae: 2.8969 - val_loss: 12.9031 - val_mae: 2.8106\n",
            "Epoch 485/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 12.7969 - mae: 2.8957 - val_loss: 11.9331 - val_mae: 2.7335\n",
            "Epoch 486/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 13.0371 - mae: 2.9351 - val_loss: 12.1302 - val_mae: 2.7388\n",
            "Epoch 487/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 12.1281 - mae: 2.8412 - val_loss: 12.7953 - val_mae: 2.7940\n",
            "Epoch 488/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 11.4293 - mae: 2.7638 - val_loss: 12.5337 - val_mae: 2.7795\n",
            "Epoch 489/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 12.1473 - mae: 2.8320 - val_loss: 11.5492 - val_mae: 2.6991\n",
            "Epoch 490/500\n",
            "89/89 [==============================] - 28s 296ms/step - loss: 12.3362 - mae: 2.8568 - val_loss: 11.9612 - val_mae: 2.7304\n",
            "Epoch 491/500\n",
            "89/89 [==============================] - 28s 300ms/step - loss: 12.3821 - mae: 2.8775 - val_loss: 15.2909 - val_mae: 3.0188\n",
            "Epoch 492/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 12.2641 - mae: 2.8403 - val_loss: 11.3961 - val_mae: 2.7156\n",
            "Epoch 493/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 12.6999 - mae: 2.8984 - val_loss: 10.7628 - val_mae: 2.6937\n",
            "Epoch 494/500\n",
            "89/89 [==============================] - 30s 331ms/step - loss: 12.1740 - mae: 2.8520 - val_loss: 15.3670 - val_mae: 3.0110\n",
            "Epoch 495/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 12.7912 - mae: 2.9101 - val_loss: 11.4250 - val_mae: 2.7006\n",
            "Epoch 496/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 12.5633 - mae: 2.8772 - val_loss: 13.9355 - val_mae: 2.8898\n",
            "Epoch 497/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 12.1136 - mae: 2.8407 - val_loss: 14.5521 - val_mae: 2.9502\n",
            "Epoch 498/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 12.3864 - mae: 2.8445 - val_loss: 11.3174 - val_mae: 2.6934\n",
            "Epoch 499/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 12.3425 - mae: 2.8642 - val_loss: 12.6010 - val_mae: 2.7807\n",
            "Epoch 500/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 12.5735 - mae: 2.9075 - val_loss: 15.7628 - val_mae: 3.0473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "\n",
        "plt.plot(epochs_x, mae, 'mo', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and validation MeanAbsoluteError')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs_x, loss, 'mo', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y3K89-CM-dfg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "1393dfe7-1375-41cd-aa6d-24dcdf5914fd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_x"
      ],
      "metadata": {
        "id": "N_sP_ZmSY-Jv",
        "outputId": "318031cf-59ef-4eb0-a844-ddb1f82f116f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download the model\n"
      ],
      "metadata": {
        "id": "R19IJQSYoW7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/My Drive/cut_panoramic/Model', exist_ok=True)\n",
        "model.save('/content/drive/My Drive/cut_panoramic/Model/1F_2e-4_16_0.2_Female18_500.h5')"
      ],
      "metadata": {
        "id": "Zed4TdFcG2iJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n5YxZ-5QjQ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download('/content/drive/My Drive/cut_panoramic/Model/1.1_รอบแรก_Flimpano_Male125_250.h5')"
      ],
      "metadata": {
        "id": "P5eMxm1NV-oY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wY_pDlxkRwxS"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}