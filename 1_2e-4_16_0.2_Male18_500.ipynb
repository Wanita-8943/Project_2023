{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wanita-8943/Project_2023/blob/main/1_2e-4_16_0.2_Male18_500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#เรียกใช้ CSV"
      ],
      "metadata": {
        "id": "ow7eWoNw6U-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil"
      ],
      "metadata": {
        "id": "DlQ2oT7cIR6o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_2Fe8u81d5r",
        "outputId": "2cf2e547-43c1-4652-91b7-6ff923f3371c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "5qxePnnn7TGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "D-hCRloc3t39"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "YZWXwjXeGxZP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#กำหนดค่าพารามิเตอร์\n"
      ],
      "metadata": {
        "id": "RooqSdBc7QHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "width = 150\n",
        "height = 150\n",
        "epochs = 500\n",
        "NUM_TRAIN = 1425\n",
        "NUM_TEST = 475\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3)"
      ],
      "metadata": {
        "id": "thDb7U9B3xOo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clone efficientnet repo\n"
      ],
      "metadata": {
        "id": "pumGmy6f3eSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7iy2f8n16p0",
        "outputId": "28d6e958-c31d-494d-800a-1637e28e4d30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 834, done.\u001b[K\n",
            "remote: Counting objects: 100% (356/356), done.\u001b[K\n",
            "remote: Compressing objects: 100% (169/169), done.\u001b[K\n",
            "remote: Total 834 (delta 251), reused 248 (delta 187), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (834/834), 13.77 MiB | 15.79 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "Vbdblwbv89fe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "vnLrobY_-GCx",
        "outputId": "e3f4729f-d4e2-418a-c606-9e6400826589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "tjZBRnfo3bN0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "hNZAzbDp3lgA",
        "outputId": "f2d1d9df-aa59-43da-a6fe-e4554c00073a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(19, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "yWNKfQUt5rga"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "NadBB12251jh",
        "outputId": "f9f2acbc-30d0-4b95-8199-7fedb24c7496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 19)                24339     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,073,923\n",
            "Trainable params: 4,031,907\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "GepWq3yy53t5",
        "outputId": "3a747ff3-b3a5-4433-bd76-b991ea97c555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD76D6c_79py",
        "outputId": "1e091e7b-16e4-4d53-af76-1e02b88f8883"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 0\n",
            "Non-trainable params: 4,049,564\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#สร้างโฟลเดอร์ Train Valodation และ Test"
      ],
      "metadata": {
        "id": "J36J9EAE7qSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/MyDrive/cut_panoramic/Data/New_Data_Male125.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "skoKhKJDngAZ",
        "outputId": "b70be92e-6671-400b-cb2f-6a0689ab28f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Fig_Age  Fig_Person_Sex  Age(year) Class  Class_Re       Filename  \\\n",
              "0           1               1          7  Y07M         0       VV03.jpg   \n",
              "1           2               1          7  Y07M         0  Flip_VV03.jpg   \n",
              "2           3               2          7  Y07M         0       VV04.jpg   \n",
              "3           4               2          7  Y07M         0  Flip_VV04.jpg   \n",
              "4           5               3          7  Y07M         0       VV05.jpg   \n",
              "...       ...             ...        ...   ...       ...            ...   \n",
              "2370      121              77         25  Y25M        18  Flip_J463.jpg   \n",
              "2371      122              78         25  Y25M        18       J464.jpg   \n",
              "2372      123              78         25  Y25M        18  Flip_J464.jpg   \n",
              "2373      124              79         25  Y25M        18       J465.jpg   \n",
              "2374      125              79         25  Y25M        18  Flip_J465.jpg   \n",
              "\n",
              "                                          Path_filename     Sex Floder  \n",
              "0     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "1     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "2     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "3     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "4     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "...                                                 ...     ...    ...  \n",
              "2370  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2371  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2372  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2373  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2374  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "\n",
              "[2375 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a626afc8-f5a5-4173-9584-6e29be4cb9d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fig_Age</th>\n",
              "      <th>Fig_Person_Sex</th>\n",
              "      <th>Age(year)</th>\n",
              "      <th>Class</th>\n",
              "      <th>Class_Re</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Path_filename</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Floder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>VV03.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>Flip_VV03.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>VV04.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>Flip_VV04.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>VV05.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2370</th>\n",
              "      <td>121</td>\n",
              "      <td>77</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J463.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2371</th>\n",
              "      <td>122</td>\n",
              "      <td>78</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>J464.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2372</th>\n",
              "      <td>123</td>\n",
              "      <td>78</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J464.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2373</th>\n",
              "      <td>124</td>\n",
              "      <td>79</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>J465.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2374</th>\n",
              "      <td>125</td>\n",
              "      <td>79</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J465.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2375 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a626afc8-f5a5-4173-9584-6e29be4cb9d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a626afc8-f5a5-4173-9584-6e29be4cb9d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a626afc8-f5a5-4173-9584-6e29be4cb9d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = df[df['Fig_Age'].between(1,75)]\n",
        "val = df[df['Fig_Age'].between(76,100)]"
      ],
      "metadata": {
        "id": "Z1zBw01Gl8Ac"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/My Drive/TVT_Male125\"\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL0g-8iOnMC4",
        "outputId": "5e17708d-8020-4e17-9a63-53d96304443b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/TVT_Male125/train\n",
            "/content/drive/My Drive/TVT_Male125/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Train"
      ],
      "metadata": {
        "id": "bWEnlTSwazL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, #โมเดลส่วนใหญ่ต้องใช้ RGB ในช่วง 0–1\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "xGPrsn9no_pa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'Path_filename',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'Path_filename',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nVlmAszntK_",
        "outputId": "39c25342-9b3c-459f-a7a8-68ac23d3c8eb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1425 validated image filenames.\n",
            "Found 475 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "          optimizer=Adam(learning_rate=2e-4),\n",
        "          metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "N6qUmmF856ZE",
        "outputId": "e50bd9f4-9ad6-466e-f395-91694a88301a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-0f54266a785f>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "89/89 [==============================] - 104s 973ms/step - loss: 102.9400 - mae: 8.5877 - val_loss: 100.9389 - val_mae: 8.4904\n",
            "Epoch 2/500\n",
            "89/89 [==============================] - 24s 261ms/step - loss: 101.3350 - mae: 8.5050 - val_loss: 99.7156 - val_mae: 8.4156\n",
            "Epoch 3/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 99.6039 - mae: 8.4214 - val_loss: 100.2802 - val_mae: 8.4447\n",
            "Epoch 4/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 99.7114 - mae: 8.4182 - val_loss: 99.3540 - val_mae: 8.3933\n",
            "Epoch 5/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 99.4538 - mae: 8.4043 - val_loss: 98.0562 - val_mae: 8.3354\n",
            "Epoch 6/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 98.1314 - mae: 8.3349 - val_loss: 98.0233 - val_mae: 8.3306\n",
            "Epoch 7/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 98.2801 - mae: 8.3409 - val_loss: 97.1343 - val_mae: 8.2678\n",
            "Epoch 8/500\n",
            "89/89 [==============================] - 20s 214ms/step - loss: 97.8305 - mae: 8.3210 - val_loss: 97.1268 - val_mae: 8.2715\n",
            "Epoch 9/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 96.7242 - mae: 8.2538 - val_loss: 96.8254 - val_mae: 8.2698\n",
            "Epoch 10/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 96.2474 - mae: 8.2247 - val_loss: 96.8421 - val_mae: 8.2819\n",
            "Epoch 11/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 95.5889 - mae: 8.1916 - val_loss: 95.2690 - val_mae: 8.1771\n",
            "Epoch 12/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 95.5063 - mae: 8.1988 - val_loss: 94.5902 - val_mae: 8.1332\n",
            "Epoch 13/500\n",
            "89/89 [==============================] - 26s 274ms/step - loss: 94.6366 - mae: 8.1451 - val_loss: 94.4617 - val_mae: 8.1149\n",
            "Epoch 14/500\n",
            "89/89 [==============================] - 25s 269ms/step - loss: 93.8985 - mae: 8.1010 - val_loss: 92.4169 - val_mae: 8.0340\n",
            "Epoch 15/500\n",
            "89/89 [==============================] - 25s 269ms/step - loss: 93.7403 - mae: 8.0954 - val_loss: 92.9586 - val_mae: 8.0631\n",
            "Epoch 16/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 93.0954 - mae: 8.0641 - val_loss: 91.7396 - val_mae: 8.0030\n",
            "Epoch 17/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 92.7804 - mae: 8.0429 - val_loss: 92.3702 - val_mae: 8.0334\n",
            "Epoch 18/500\n",
            "89/89 [==============================] - 29s 316ms/step - loss: 92.3169 - mae: 8.0176 - val_loss: 91.5596 - val_mae: 7.9672\n",
            "Epoch 19/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 91.7673 - mae: 7.9913 - val_loss: 90.8525 - val_mae: 7.9457\n",
            "Epoch 20/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 90.8925 - mae: 7.9508 - val_loss: 89.7230 - val_mae: 7.9120\n",
            "Epoch 21/500\n",
            "89/89 [==============================] - 20s 221ms/step - loss: 90.3911 - mae: 7.9266 - val_loss: 90.3253 - val_mae: 7.9236\n",
            "Epoch 22/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 89.2944 - mae: 7.8680 - val_loss: 89.7941 - val_mae: 7.8884\n",
            "Epoch 23/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 89.5354 - mae: 7.8817 - val_loss: 87.5906 - val_mae: 7.7787\n",
            "Epoch 24/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 88.9757 - mae: 7.8560 - val_loss: 88.9120 - val_mae: 7.8316\n",
            "Epoch 25/500\n",
            "89/89 [==============================] - 23s 248ms/step - loss: 88.1187 - mae: 7.8074 - val_loss: 87.6805 - val_mae: 7.7792\n",
            "Epoch 26/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 87.7181 - mae: 7.7836 - val_loss: 86.1535 - val_mae: 7.6884\n",
            "Epoch 27/500\n",
            "89/89 [==============================] - 26s 292ms/step - loss: 86.6501 - mae: 7.7265 - val_loss: 87.1360 - val_mae: 7.7527\n",
            "Epoch 28/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 86.4585 - mae: 7.7255 - val_loss: 86.1977 - val_mae: 7.7156\n",
            "Epoch 29/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 86.1036 - mae: 7.7001 - val_loss: 85.0720 - val_mae: 7.6460\n",
            "Epoch 30/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 85.8799 - mae: 7.6838 - val_loss: 84.8223 - val_mae: 7.6385\n",
            "Epoch 31/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 85.5300 - mae: 7.6706 - val_loss: 85.2631 - val_mae: 7.6669\n",
            "Epoch 32/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 85.0433 - mae: 7.6530 - val_loss: 85.6288 - val_mae: 7.6913\n",
            "Epoch 33/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 84.2860 - mae: 7.6098 - val_loss: 84.4281 - val_mae: 7.6167\n",
            "Epoch 34/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 84.0699 - mae: 7.6028 - val_loss: 83.3343 - val_mae: 7.5525\n",
            "Epoch 35/500\n",
            "89/89 [==============================] - 24s 262ms/step - loss: 83.2801 - mae: 7.5463 - val_loss: 83.3107 - val_mae: 7.5609\n",
            "Epoch 36/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 82.8375 - mae: 7.5318 - val_loss: 83.1228 - val_mae: 7.5398\n",
            "Epoch 37/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 82.1986 - mae: 7.4954 - val_loss: 82.6139 - val_mae: 7.5183\n",
            "Epoch 38/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 82.0628 - mae: 7.4854 - val_loss: 80.7700 - val_mae: 7.4235\n",
            "Epoch 39/500\n",
            "89/89 [==============================] - 20s 217ms/step - loss: 80.7563 - mae: 7.4091 - val_loss: 80.7714 - val_mae: 7.3946\n",
            "Epoch 40/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 80.8238 - mae: 7.4249 - val_loss: 80.0308 - val_mae: 7.3625\n",
            "Epoch 41/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 80.0852 - mae: 7.3788 - val_loss: 80.0515 - val_mae: 7.3681\n",
            "Epoch 42/500\n",
            "89/89 [==============================] - 21s 220ms/step - loss: 79.8570 - mae: 7.3621 - val_loss: 79.3151 - val_mae: 7.3354\n",
            "Epoch 43/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 79.4151 - mae: 7.3399 - val_loss: 79.6985 - val_mae: 7.3692\n",
            "Epoch 44/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 79.0528 - mae: 7.3189 - val_loss: 78.7236 - val_mae: 7.2965\n",
            "Epoch 45/500\n",
            "89/89 [==============================] - 20s 220ms/step - loss: 78.5949 - mae: 7.2968 - val_loss: 77.4489 - val_mae: 7.2370\n",
            "Epoch 46/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 78.1217 - mae: 7.2734 - val_loss: 76.9157 - val_mae: 7.1998\n",
            "Epoch 47/500\n",
            "89/89 [==============================] - 26s 276ms/step - loss: 77.7102 - mae: 7.2578 - val_loss: 77.1926 - val_mae: 7.2168\n",
            "Epoch 48/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 77.2040 - mae: 7.2337 - val_loss: 76.9809 - val_mae: 7.2243\n",
            "Epoch 49/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 76.7432 - mae: 7.1964 - val_loss: 75.5791 - val_mae: 7.1413\n",
            "Epoch 50/500\n",
            "89/89 [==============================] - 20s 216ms/step - loss: 76.1285 - mae: 7.1727 - val_loss: 75.7859 - val_mae: 7.1612\n",
            "Epoch 51/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 75.8655 - mae: 7.1604 - val_loss: 75.8012 - val_mae: 7.1584\n",
            "Epoch 52/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 75.1692 - mae: 7.1287 - val_loss: 74.9790 - val_mae: 7.1207\n",
            "Epoch 53/500\n",
            "89/89 [==============================] - 20s 216ms/step - loss: 74.3764 - mae: 7.0868 - val_loss: 74.3769 - val_mae: 7.0858\n",
            "Epoch 54/500\n",
            "89/89 [==============================] - 28s 298ms/step - loss: 74.0541 - mae: 7.0608 - val_loss: 73.3090 - val_mae: 7.0190\n",
            "Epoch 55/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 73.8759 - mae: 7.0570 - val_loss: 73.4433 - val_mae: 7.0223\n",
            "Epoch 56/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 73.4911 - mae: 7.0348 - val_loss: 72.3137 - val_mae: 6.9716\n",
            "Epoch 57/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 72.7307 - mae: 6.9998 - val_loss: 72.1966 - val_mae: 6.9711\n",
            "Epoch 58/500\n",
            "89/89 [==============================] - 26s 276ms/step - loss: 72.7146 - mae: 7.0008 - val_loss: 72.8877 - val_mae: 6.9960\n",
            "Epoch 59/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 71.9285 - mae: 6.9541 - val_loss: 72.0264 - val_mae: 6.9690\n",
            "Epoch 60/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 71.7913 - mae: 6.9507 - val_loss: 71.3841 - val_mae: 6.9124\n",
            "Epoch 61/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 71.4198 - mae: 6.9295 - val_loss: 71.0773 - val_mae: 6.9074\n",
            "Epoch 62/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 71.2477 - mae: 6.9229 - val_loss: 70.8416 - val_mae: 6.9030\n",
            "Epoch 63/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 70.5098 - mae: 6.8847 - val_loss: 70.8542 - val_mae: 6.8944\n",
            "Epoch 64/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 70.2154 - mae: 6.8637 - val_loss: 69.9410 - val_mae: 6.8477\n",
            "Epoch 65/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 69.9908 - mae: 6.8565 - val_loss: 69.7446 - val_mae: 6.8391\n",
            "Epoch 66/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 69.4195 - mae: 6.8221 - val_loss: 68.5487 - val_mae: 6.7634\n",
            "Epoch 67/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 68.8783 - mae: 6.7925 - val_loss: 68.4538 - val_mae: 6.7671\n",
            "Epoch 68/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 68.4638 - mae: 6.7696 - val_loss: 68.1620 - val_mae: 6.7447\n",
            "Epoch 69/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 67.7344 - mae: 6.7299 - val_loss: 68.3973 - val_mae: 6.7806\n",
            "Epoch 70/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 67.5126 - mae: 6.7191 - val_loss: 67.6315 - val_mae: 6.7194\n",
            "Epoch 71/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 67.3164 - mae: 6.7019 - val_loss: 67.0602 - val_mae: 6.6860\n",
            "Epoch 72/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 66.4972 - mae: 6.6567 - val_loss: 66.2273 - val_mae: 6.6510\n",
            "Epoch 73/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 66.5219 - mae: 6.6580 - val_loss: 65.9459 - val_mae: 6.6264\n",
            "Epoch 74/500\n",
            "89/89 [==============================] - 26s 275ms/step - loss: 65.9538 - mae: 6.6289 - val_loss: 65.2390 - val_mae: 6.5758\n",
            "Epoch 75/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 65.7640 - mae: 6.6254 - val_loss: 65.2839 - val_mae: 6.5930\n",
            "Epoch 76/500\n",
            "89/89 [==============================] - 28s 298ms/step - loss: 65.0086 - mae: 6.5763 - val_loss: 65.3503 - val_mae: 6.6194\n",
            "Epoch 77/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 64.6797 - mae: 6.5654 - val_loss: 64.4970 - val_mae: 6.5642\n",
            "Epoch 78/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 64.6712 - mae: 6.5728 - val_loss: 63.7472 - val_mae: 6.5070\n",
            "Epoch 79/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 63.9751 - mae: 6.5322 - val_loss: 64.3064 - val_mae: 6.5608\n",
            "Epoch 80/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 63.6726 - mae: 6.5195 - val_loss: 63.5736 - val_mae: 6.5173\n",
            "Epoch 81/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 63.3007 - mae: 6.5029 - val_loss: 63.2545 - val_mae: 6.4877\n",
            "Epoch 82/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 63.1106 - mae: 6.4898 - val_loss: 63.0596 - val_mae: 6.4888\n",
            "Epoch 83/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 62.6431 - mae: 6.4706 - val_loss: 61.4619 - val_mae: 6.3940\n",
            "Epoch 84/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 62.1652 - mae: 6.4396 - val_loss: 61.3085 - val_mae: 6.3890\n",
            "Epoch 85/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 61.8205 - mae: 6.4276 - val_loss: 61.5427 - val_mae: 6.4034\n",
            "Epoch 86/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 61.5054 - mae: 6.4051 - val_loss: 61.0046 - val_mae: 6.3755\n",
            "Epoch 87/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 61.1935 - mae: 6.3900 - val_loss: 60.9133 - val_mae: 6.3704\n",
            "Epoch 88/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 60.7097 - mae: 6.3612 - val_loss: 60.9487 - val_mae: 6.3861\n",
            "Epoch 89/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 60.6158 - mae: 6.3593 - val_loss: 60.2244 - val_mae: 6.3473\n",
            "Epoch 90/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 60.3197 - mae: 6.3527 - val_loss: 59.7086 - val_mae: 6.2965\n",
            "Epoch 91/500\n",
            "89/89 [==============================] - 21s 233ms/step - loss: 59.6792 - mae: 6.3124 - val_loss: 59.4198 - val_mae: 6.2930\n",
            "Epoch 92/500\n",
            "89/89 [==============================] - 26s 274ms/step - loss: 59.6094 - mae: 6.3137 - val_loss: 59.1986 - val_mae: 6.2914\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 59.2199 - mae: 6.2910 - val_loss: 59.0565 - val_mae: 6.2702\n",
            "Epoch 94/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 58.6460 - mae: 6.2564 - val_loss: 58.5599 - val_mae: 6.2568\n",
            "Epoch 95/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 58.2586 - mae: 6.2332 - val_loss: 58.3287 - val_mae: 6.2458\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 57.9402 - mae: 6.2213 - val_loss: 58.1860 - val_mae: 6.2236\n",
            "Epoch 97/500\n",
            "89/89 [==============================] - 29s 317ms/step - loss: 57.7461 - mae: 6.2053 - val_loss: 56.8914 - val_mae: 6.1464\n",
            "Epoch 98/500\n",
            "89/89 [==============================] - 26s 297ms/step - loss: 57.0887 - mae: 6.1636 - val_loss: 56.5435 - val_mae: 6.1299\n",
            "Epoch 99/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 56.8374 - mae: 6.1540 - val_loss: 56.6592 - val_mae: 6.1408\n",
            "Epoch 100/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 56.8022 - mae: 6.1603 - val_loss: 56.3721 - val_mae: 6.1243\n",
            "Epoch 101/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 56.5463 - mae: 6.1401 - val_loss: 55.9680 - val_mae: 6.1090\n",
            "Epoch 102/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 55.7000 - mae: 6.0935 - val_loss: 55.9788 - val_mae: 6.1118\n",
            "Epoch 103/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 55.3583 - mae: 6.0691 - val_loss: 55.2858 - val_mae: 6.0673\n",
            "Epoch 104/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 55.4482 - mae: 6.0742 - val_loss: 55.5272 - val_mae: 6.0860\n",
            "Epoch 105/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 54.8398 - mae: 6.0408 - val_loss: 55.6305 - val_mae: 6.0971\n",
            "Epoch 106/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 54.7588 - mae: 6.0411 - val_loss: 54.6678 - val_mae: 6.0413\n",
            "Epoch 107/500\n",
            "89/89 [==============================] - 24s 268ms/step - loss: 54.3650 - mae: 6.0176 - val_loss: 54.4844 - val_mae: 6.0323\n",
            "Epoch 108/500\n",
            "89/89 [==============================] - 24s 268ms/step - loss: 54.0308 - mae: 6.0042 - val_loss: 54.1796 - val_mae: 6.0193\n",
            "Epoch 109/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 53.6874 - mae: 5.9879 - val_loss: 53.6764 - val_mae: 5.9863\n",
            "Epoch 110/500\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 53.3228 - mae: 5.9719 - val_loss: 53.5963 - val_mae: 5.9961\n",
            "Epoch 111/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 52.8542 - mae: 5.9443 - val_loss: 53.0174 - val_mae: 5.9602\n",
            "Epoch 112/500\n",
            "89/89 [==============================] - 24s 269ms/step - loss: 52.8146 - mae: 5.9423 - val_loss: 52.6251 - val_mae: 5.9399\n",
            "Epoch 113/500\n",
            "89/89 [==============================] - 20s 216ms/step - loss: 52.7168 - mae: 5.9454 - val_loss: 52.6864 - val_mae: 5.9426\n",
            "Epoch 114/500\n",
            "89/89 [==============================] - 26s 275ms/step - loss: 52.1988 - mae: 5.9157 - val_loss: 52.8725 - val_mae: 5.9638\n",
            "Epoch 115/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 51.8058 - mae: 5.8939 - val_loss: 51.7078 - val_mae: 5.8851\n",
            "Epoch 116/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 51.4869 - mae: 5.8770 - val_loss: 51.6808 - val_mae: 5.8907\n",
            "Epoch 117/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 51.2646 - mae: 5.8686 - val_loss: 51.2553 - val_mae: 5.8683\n",
            "Epoch 118/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 51.0522 - mae: 5.8610 - val_loss: 50.0806 - val_mae: 5.7869\n",
            "Epoch 119/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 50.9527 - mae: 5.8585 - val_loss: 50.0117 - val_mae: 5.7919\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - 20s 216ms/step - loss: 50.3539 - mae: 5.8214 - val_loss: 50.3188 - val_mae: 5.8135\n",
            "Epoch 121/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 50.2921 - mae: 5.8209 - val_loss: 50.6863 - val_mae: 5.8478\n",
            "Epoch 122/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 50.1211 - mae: 5.8092 - val_loss: 49.3764 - val_mae: 5.7600\n",
            "Epoch 123/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 49.6192 - mae: 5.7804 - val_loss: 50.1452 - val_mae: 5.8197\n",
            "Epoch 124/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 49.6042 - mae: 5.7851 - val_loss: 48.8951 - val_mae: 5.7238\n",
            "Epoch 125/500\n",
            "89/89 [==============================] - 24s 267ms/step - loss: 48.9901 - mae: 5.7490 - val_loss: 49.3109 - val_mae: 5.7678\n",
            "Epoch 126/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 48.9044 - mae: 5.7519 - val_loss: 47.4565 - val_mae: 5.6473\n",
            "Epoch 127/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 48.0808 - mae: 5.6926 - val_loss: 47.6170 - val_mae: 5.6623\n",
            "Epoch 128/500\n",
            "89/89 [==============================] - 26s 293ms/step - loss: 48.3096 - mae: 5.7135 - val_loss: 47.9285 - val_mae: 5.6901\n",
            "Epoch 129/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 47.9874 - mae: 5.6904 - val_loss: 48.1851 - val_mae: 5.7127\n",
            "Epoch 130/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 47.7197 - mae: 5.6739 - val_loss: 46.8405 - val_mae: 5.6191\n",
            "Epoch 131/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 47.2618 - mae: 5.6498 - val_loss: 47.6028 - val_mae: 5.6789\n",
            "Epoch 132/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 47.3755 - mae: 5.6651 - val_loss: 46.7592 - val_mae: 5.6076\n",
            "Epoch 133/500\n",
            "89/89 [==============================] - 25s 267ms/step - loss: 46.7738 - mae: 5.6213 - val_loss: 47.3884 - val_mae: 5.6856\n",
            "Epoch 134/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 46.5671 - mae: 5.6095 - val_loss: 47.0554 - val_mae: 5.6546\n",
            "Epoch 135/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 46.5013 - mae: 5.6111 - val_loss: 46.4180 - val_mae: 5.5989\n",
            "Epoch 136/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 46.0793 - mae: 5.5860 - val_loss: 45.9230 - val_mae: 5.5713\n",
            "Epoch 137/500\n",
            "89/89 [==============================] - 20s 215ms/step - loss: 45.9672 - mae: 5.5811 - val_loss: 45.8229 - val_mae: 5.5723\n",
            "Epoch 138/500\n",
            "89/89 [==============================] - 25s 270ms/step - loss: 45.6287 - mae: 5.5612 - val_loss: 45.6102 - val_mae: 5.5658\n",
            "Epoch 139/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 45.6433 - mae: 5.5682 - val_loss: 45.2515 - val_mae: 5.5310\n",
            "Epoch 140/500\n",
            "89/89 [==============================] - 21s 230ms/step - loss: 45.0889 - mae: 5.5339 - val_loss: 44.9956 - val_mae: 5.5200\n",
            "Epoch 141/500\n",
            "89/89 [==============================] - 20s 216ms/step - loss: 44.9123 - mae: 5.5266 - val_loss: 44.9036 - val_mae: 5.5166\n",
            "Epoch 142/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 44.6746 - mae: 5.5117 - val_loss: 44.8868 - val_mae: 5.5323\n",
            "Epoch 143/500\n",
            "89/89 [==============================] - 25s 269ms/step - loss: 44.6935 - mae: 5.5211 - val_loss: 44.3071 - val_mae: 5.4923\n",
            "Epoch 144/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 44.2914 - mae: 5.5023 - val_loss: 44.1007 - val_mae: 5.4908\n",
            "Epoch 145/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 44.1861 - mae: 5.5022 - val_loss: 44.3868 - val_mae: 5.5213\n",
            "Epoch 146/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 43.8302 - mae: 5.4789 - val_loss: 44.2580 - val_mae: 5.5088\n",
            "Epoch 147/500\n",
            "89/89 [==============================] - 20s 219ms/step - loss: 43.5004 - mae: 5.4608 - val_loss: 42.9634 - val_mae: 5.4099\n",
            "Epoch 148/500\n",
            "89/89 [==============================] - 26s 274ms/step - loss: 43.3918 - mae: 5.4590 - val_loss: 43.4159 - val_mae: 5.4516\n",
            "Epoch 149/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 43.0814 - mae: 5.4375 - val_loss: 42.7425 - val_mae: 5.4199\n",
            "Epoch 150/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 42.7585 - mae: 5.4212 - val_loss: 42.8033 - val_mae: 5.4266\n",
            "Epoch 151/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 42.6896 - mae: 5.4177 - val_loss: 42.6250 - val_mae: 5.4140\n",
            "Epoch 152/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 42.6054 - mae: 5.4145 - val_loss: 42.5623 - val_mae: 5.4133\n",
            "Epoch 153/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 42.0820 - mae: 5.3800 - val_loss: 42.4933 - val_mae: 5.4107\n",
            "Epoch 154/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 41.9508 - mae: 5.3765 - val_loss: 41.8764 - val_mae: 5.3765\n",
            "Epoch 155/500\n",
            "89/89 [==============================] - 20s 217ms/step - loss: 42.0396 - mae: 5.3880 - val_loss: 41.6794 - val_mae: 5.3592\n",
            "Epoch 156/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 41.5893 - mae: 5.3580 - val_loss: 40.7305 - val_mae: 5.2950\n",
            "Epoch 157/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 41.4245 - mae: 5.3554 - val_loss: 41.6830 - val_mae: 5.3804\n",
            "Epoch 158/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 41.1422 - mae: 5.3319 - val_loss: 41.1844 - val_mae: 5.3468\n",
            "Epoch 159/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 40.9500 - mae: 5.3234 - val_loss: 41.3987 - val_mae: 5.3530\n",
            "Epoch 160/500\n",
            "89/89 [==============================] - 22s 242ms/step - loss: 40.7716 - mae: 5.3121 - val_loss: 40.4674 - val_mae: 5.2867\n",
            "Epoch 161/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 40.6814 - mae: 5.3122 - val_loss: 40.6200 - val_mae: 5.3016\n",
            "Epoch 162/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 40.5398 - mae: 5.3065 - val_loss: 40.3771 - val_mae: 5.2893\n",
            "Epoch 163/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 40.3389 - mae: 5.2844 - val_loss: 40.1891 - val_mae: 5.2698\n",
            "Epoch 164/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 39.9585 - mae: 5.2674 - val_loss: 39.4719 - val_mae: 5.2249\n",
            "Epoch 165/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 39.9861 - mae: 5.2734 - val_loss: 39.5640 - val_mae: 5.2468\n",
            "Epoch 166/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 39.7328 - mae: 5.2590 - val_loss: 39.3714 - val_mae: 5.2287\n",
            "Epoch 167/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 39.6382 - mae: 5.2497 - val_loss: 39.4017 - val_mae: 5.2236\n",
            "Epoch 168/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 39.1724 - mae: 5.2183 - val_loss: 39.1045 - val_mae: 5.2159\n",
            "Epoch 169/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 39.1697 - mae: 5.2211 - val_loss: 39.2436 - val_mae: 5.2204\n",
            "Epoch 170/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 38.9233 - mae: 5.2078 - val_loss: 38.6782 - val_mae: 5.1768\n",
            "Epoch 171/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 38.7870 - mae: 5.1985 - val_loss: 38.7304 - val_mae: 5.1954\n",
            "Epoch 172/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 38.5732 - mae: 5.1860 - val_loss: 39.0736 - val_mae: 5.2357\n",
            "Epoch 173/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 38.4681 - mae: 5.1864 - val_loss: 38.2126 - val_mae: 5.1655\n",
            "Epoch 174/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 38.2601 - mae: 5.1779 - val_loss: 38.5803 - val_mae: 5.2103\n",
            "Epoch 175/500\n",
            "89/89 [==============================] - 24s 268ms/step - loss: 38.3279 - mae: 5.1822 - val_loss: 37.6911 - val_mae: 5.1289\n",
            "Epoch 176/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 38.1869 - mae: 5.1817 - val_loss: 37.9731 - val_mae: 5.1654\n",
            "Epoch 177/500\n",
            "89/89 [==============================] - 24s 267ms/step - loss: 37.5812 - mae: 5.1371 - val_loss: 37.9040 - val_mae: 5.1677\n",
            "Epoch 178/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 37.5853 - mae: 5.1406 - val_loss: 37.8703 - val_mae: 5.1716\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 37.3462 - mae: 5.1318 - val_loss: 37.7595 - val_mae: 5.1776\n",
            "Epoch 180/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 37.2216 - mae: 5.1262 - val_loss: 37.4048 - val_mae: 5.1498\n",
            "Epoch 181/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 37.1535 - mae: 5.1249 - val_loss: 36.7900 - val_mae: 5.0889\n",
            "Epoch 182/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 37.0762 - mae: 5.1183 - val_loss: 36.4587 - val_mae: 5.0800\n",
            "Epoch 183/500\n",
            "89/89 [==============================] - 24s 267ms/step - loss: 36.8205 - mae: 5.1100 - val_loss: 36.7807 - val_mae: 5.1118\n",
            "Epoch 184/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 36.7740 - mae: 5.1083 - val_loss: 36.6130 - val_mae: 5.0916\n",
            "Epoch 185/500\n",
            "89/89 [==============================] - 25s 270ms/step - loss: 36.5744 - mae: 5.0964 - val_loss: 35.9652 - val_mae: 5.0454\n",
            "Epoch 186/500\n",
            "89/89 [==============================] - 25s 286ms/step - loss: 36.5289 - mae: 5.0954 - val_loss: 36.5469 - val_mae: 5.0839\n",
            "Epoch 187/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 36.5264 - mae: 5.1058 - val_loss: 36.2824 - val_mae: 5.0744\n",
            "Epoch 188/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 35.9768 - mae: 5.0641 - val_loss: 36.0059 - val_mae: 5.0671\n",
            "Epoch 189/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 36.0827 - mae: 5.0753 - val_loss: 35.6087 - val_mae: 5.0318\n",
            "Epoch 190/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 35.9240 - mae: 5.0644 - val_loss: 36.2674 - val_mae: 5.0958\n",
            "Epoch 191/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 35.7744 - mae: 5.0573 - val_loss: 35.8629 - val_mae: 5.0627\n",
            "Epoch 192/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 35.7512 - mae: 5.0597 - val_loss: 35.0967 - val_mae: 4.9977\n",
            "Epoch 193/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 35.3937 - mae: 5.0320 - val_loss: 35.2389 - val_mae: 5.0121\n",
            "Epoch 194/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 35.1435 - mae: 5.0094 - val_loss: 35.1727 - val_mae: 5.0182\n",
            "Epoch 195/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 35.2308 - mae: 5.0209 - val_loss: 34.8751 - val_mae: 4.9981\n",
            "Epoch 196/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 34.9920 - mae: 5.0057 - val_loss: 35.0040 - val_mae: 5.0221\n",
            "Epoch 197/500\n",
            "89/89 [==============================] - 25s 270ms/step - loss: 35.0126 - mae: 5.0115 - val_loss: 35.0099 - val_mae: 5.0127\n",
            "Epoch 198/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 34.9241 - mae: 5.0088 - val_loss: 35.3703 - val_mae: 5.0480\n",
            "Epoch 199/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 34.7520 - mae: 4.9899 - val_loss: 34.5127 - val_mae: 4.9713\n",
            "Epoch 200/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 34.4629 - mae: 4.9723 - val_loss: 34.7868 - val_mae: 4.9988\n",
            "Epoch 201/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 34.6883 - mae: 4.9961 - val_loss: 34.2261 - val_mae: 4.9616\n",
            "Epoch 202/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 34.4555 - mae: 4.9708 - val_loss: 34.4179 - val_mae: 4.9743\n",
            "Epoch 203/500\n",
            "89/89 [==============================] - 25s 270ms/step - loss: 34.3327 - mae: 4.9666 - val_loss: 34.5061 - val_mae: 4.9865\n",
            "Epoch 204/500\n",
            "89/89 [==============================] - 20s 215ms/step - loss: 34.3757 - mae: 4.9780 - val_loss: 34.1454 - val_mae: 4.9601\n",
            "Epoch 205/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 34.0100 - mae: 4.9438 - val_loss: 34.0620 - val_mae: 4.9532\n",
            "Epoch 206/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 34.0820 - mae: 4.9536 - val_loss: 33.4657 - val_mae: 4.9026\n",
            "Epoch 207/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 33.7551 - mae: 4.9326 - val_loss: 33.8328 - val_mae: 4.9480\n",
            "Epoch 208/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 33.8983 - mae: 4.9479 - val_loss: 33.7353 - val_mae: 4.9422\n",
            "Epoch 209/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 33.6349 - mae: 4.9318 - val_loss: 33.4973 - val_mae: 4.9189\n",
            "Epoch 210/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 33.5025 - mae: 4.9251 - val_loss: 33.3993 - val_mae: 4.9174\n",
            "Epoch 211/500\n",
            "89/89 [==============================] - 20s 224ms/step - loss: 33.4995 - mae: 4.9266 - val_loss: 33.7284 - val_mae: 4.9576\n",
            "Epoch 212/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 33.4688 - mae: 4.9335 - val_loss: 33.1764 - val_mae: 4.9105\n",
            "Epoch 213/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 33.2264 - mae: 4.9076 - val_loss: 33.2492 - val_mae: 4.9115\n",
            "Epoch 214/500\n",
            "89/89 [==============================] - 22s 234ms/step - loss: 33.1600 - mae: 4.9087 - val_loss: 33.4109 - val_mae: 4.9356\n",
            "Epoch 215/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 33.2328 - mae: 4.9199 - val_loss: 33.2110 - val_mae: 4.9233\n",
            "Epoch 216/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 32.9708 - mae: 4.8965 - val_loss: 32.9547 - val_mae: 4.8947\n",
            "Epoch 217/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 32.9486 - mae: 4.9020 - val_loss: 32.8493 - val_mae: 4.8927\n",
            "Epoch 218/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 32.8148 - mae: 4.8957 - val_loss: 32.9931 - val_mae: 4.9118\n",
            "Epoch 219/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 32.8837 - mae: 4.9039 - val_loss: 32.6195 - val_mae: 4.8750\n",
            "Epoch 220/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 32.7294 - mae: 4.8939 - val_loss: 32.4338 - val_mae: 4.8661\n",
            "Epoch 221/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 32.6465 - mae: 4.8885 - val_loss: 32.4416 - val_mae: 4.8677\n",
            "Epoch 222/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 32.5128 - mae: 4.8796 - val_loss: 32.6095 - val_mae: 4.8955\n",
            "Epoch 223/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 32.4253 - mae: 4.8787 - val_loss: 32.3495 - val_mae: 4.8756\n",
            "Epoch 224/500\n",
            "89/89 [==============================] - 20s 219ms/step - loss: 32.3993 - mae: 4.8744 - val_loss: 32.1470 - val_mae: 4.8632\n",
            "Epoch 225/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 32.4063 - mae: 4.8793 - val_loss: 32.2165 - val_mae: 4.8639\n",
            "Epoch 226/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 32.2355 - mae: 4.8647 - val_loss: 31.9236 - val_mae: 4.8431\n",
            "Epoch 227/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 32.3277 - mae: 4.8789 - val_loss: 32.1591 - val_mae: 4.8653\n",
            "Epoch 228/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 32.1141 - mae: 4.8634 - val_loss: 31.9869 - val_mae: 4.8511\n",
            "Epoch 229/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 32.1136 - mae: 4.8645 - val_loss: 31.9742 - val_mae: 4.8576\n",
            "Epoch 230/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 32.0298 - mae: 4.8592 - val_loss: 31.8709 - val_mae: 4.8331\n",
            "Epoch 231/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 32.0954 - mae: 4.8714 - val_loss: 32.1871 - val_mae: 4.8755\n",
            "Epoch 232/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 31.8186 - mae: 4.8434 - val_loss: 31.5293 - val_mae: 4.8194\n",
            "Epoch 233/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 31.9113 - mae: 4.8527 - val_loss: 31.7522 - val_mae: 4.8423\n",
            "Epoch 234/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 31.7962 - mae: 4.8430 - val_loss: 31.5863 - val_mae: 4.8259\n",
            "Epoch 235/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 31.6997 - mae: 4.8384 - val_loss: 31.4941 - val_mae: 4.8233\n",
            "Epoch 236/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 31.6590 - mae: 4.8376 - val_loss: 31.4678 - val_mae: 4.8169\n",
            "Epoch 237/500\n",
            "89/89 [==============================] - 20s 217ms/step - loss: 31.6374 - mae: 4.8323 - val_loss: 31.5786 - val_mae: 4.8231\n",
            "Epoch 238/500\n",
            "89/89 [==============================] - 27s 285ms/step - loss: 31.5167 - mae: 4.8266 - val_loss: 31.7185 - val_mae: 4.8404\n",
            "Epoch 239/500\n",
            "89/89 [==============================] - 26s 274ms/step - loss: 31.4917 - mae: 4.8287 - val_loss: 31.3630 - val_mae: 4.8156\n",
            "Epoch 240/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 31.4481 - mae: 4.8191 - val_loss: 31.3014 - val_mae: 4.8112\n",
            "Epoch 241/500\n",
            "89/89 [==============================] - 26s 278ms/step - loss: 31.3636 - mae: 4.8178 - val_loss: 30.8953 - val_mae: 4.7682\n",
            "Epoch 242/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 31.2973 - mae: 4.8142 - val_loss: 31.1932 - val_mae: 4.8000\n",
            "Epoch 243/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 31.2786 - mae: 4.8090 - val_loss: 31.5311 - val_mae: 4.8365\n",
            "Epoch 244/500\n",
            "89/89 [==============================] - 25s 270ms/step - loss: 29.6208 - mae: 4.6033 - val_loss: 26.1944 - val_mae: 4.2790\n",
            "Epoch 245/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 24.2723 - mae: 3.9439 - val_loss: 23.3944 - val_mae: 3.8784\n",
            "Epoch 246/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 23.6460 - mae: 3.8557 - val_loss: 23.7985 - val_mae: 3.9362\n",
            "Epoch 247/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 23.4882 - mae: 3.8527 - val_loss: 22.4263 - val_mae: 3.6657\n",
            "Epoch 248/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 22.9880 - mae: 3.7852 - val_loss: 21.9863 - val_mae: 3.6404\n",
            "Epoch 249/500\n",
            "89/89 [==============================] - 28s 298ms/step - loss: 22.5766 - mae: 3.7431 - val_loss: 21.7911 - val_mae: 3.6169\n",
            "Epoch 250/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 22.4382 - mae: 3.7264 - val_loss: 22.2817 - val_mae: 3.6269\n",
            "Epoch 251/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 22.4690 - mae: 3.7438 - val_loss: 21.0517 - val_mae: 3.5331\n",
            "Epoch 252/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 22.3330 - mae: 3.7415 - val_loss: 21.9988 - val_mae: 3.6139\n",
            "Epoch 253/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 22.0244 - mae: 3.7009 - val_loss: 20.9374 - val_mae: 3.5488\n",
            "Epoch 254/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 21.6177 - mae: 3.6554 - val_loss: 20.6881 - val_mae: 3.5379\n",
            "Epoch 255/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 21.4444 - mae: 3.6578 - val_loss: 21.3305 - val_mae: 3.5548\n",
            "Epoch 256/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 21.0492 - mae: 3.6061 - val_loss: 22.9232 - val_mae: 3.6838\n",
            "Epoch 257/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 21.1546 - mae: 3.6260 - val_loss: 20.7616 - val_mae: 3.5217\n",
            "Epoch 258/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 21.6121 - mae: 3.7360 - val_loss: 20.9024 - val_mae: 3.5228\n",
            "Epoch 259/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 20.7095 - mae: 3.5876 - val_loss: 19.8954 - val_mae: 3.4363\n",
            "Epoch 260/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 20.4806 - mae: 3.5760 - val_loss: 20.5727 - val_mae: 3.5045\n",
            "Epoch 261/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 20.6483 - mae: 3.6050 - val_loss: 19.8914 - val_mae: 3.4627\n",
            "Epoch 262/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 20.1266 - mae: 3.5319 - val_loss: 21.7162 - val_mae: 3.5984\n",
            "Epoch 263/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 20.2523 - mae: 3.5576 - val_loss: 19.5048 - val_mae: 3.4150\n",
            "Epoch 264/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 20.1243 - mae: 3.5503 - val_loss: 19.6016 - val_mae: 3.4182\n",
            "Epoch 265/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 19.8790 - mae: 3.5079 - val_loss: 19.1190 - val_mae: 3.3709\n",
            "Epoch 266/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 19.6753 - mae: 3.5189 - val_loss: 18.8251 - val_mae: 3.3465\n",
            "Epoch 267/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 19.5911 - mae: 3.5099 - val_loss: 18.7405 - val_mae: 3.3537\n",
            "Epoch 268/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 19.4123 - mae: 3.4852 - val_loss: 19.5208 - val_mae: 3.4082\n",
            "Epoch 269/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 19.3709 - mae: 3.4863 - val_loss: 19.0574 - val_mae: 3.3687\n",
            "Epoch 270/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 19.1080 - mae: 3.4596 - val_loss: 19.2402 - val_mae: 3.3920\n",
            "Epoch 271/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 19.3668 - mae: 3.4878 - val_loss: 17.5811 - val_mae: 3.2459\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 19.1923 - mae: 3.4764 - val_loss: 17.7913 - val_mae: 3.2783\n",
            "Epoch 273/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 18.9699 - mae: 3.4411 - val_loss: 18.7158 - val_mae: 3.3346\n",
            "Epoch 274/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 18.7516 - mae: 3.4371 - val_loss: 17.8055 - val_mae: 3.2614\n",
            "Epoch 275/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 18.7369 - mae: 3.4437 - val_loss: 17.9807 - val_mae: 3.2575\n",
            "Epoch 276/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 18.4157 - mae: 3.4033 - val_loss: 18.2081 - val_mae: 3.2949\n",
            "Epoch 277/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 18.4180 - mae: 3.3900 - val_loss: 17.8640 - val_mae: 3.2582\n",
            "Epoch 278/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 18.4097 - mae: 3.4013 - val_loss: 18.7587 - val_mae: 3.3746\n",
            "Epoch 279/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 18.2611 - mae: 3.3842 - val_loss: 17.1305 - val_mae: 3.1937\n",
            "Epoch 280/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 18.3637 - mae: 3.4008 - val_loss: 17.3042 - val_mae: 3.2208\n",
            "Epoch 281/500\n",
            "89/89 [==============================] - 25s 271ms/step - loss: 17.7041 - mae: 3.3338 - val_loss: 17.0178 - val_mae: 3.1857\n",
            "Epoch 282/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 17.7102 - mae: 3.3514 - val_loss: 17.1332 - val_mae: 3.2026\n",
            "Epoch 283/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 18.0423 - mae: 3.3765 - val_loss: 17.4683 - val_mae: 3.2379\n",
            "Epoch 284/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 17.5701 - mae: 3.3234 - val_loss: 16.5932 - val_mae: 3.1543\n",
            "Epoch 285/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 17.5967 - mae: 3.3308 - val_loss: 16.3230 - val_mae: 3.1326\n",
            "Epoch 286/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 17.4610 - mae: 3.3282 - val_loss: 16.4940 - val_mae: 3.1517\n",
            "Epoch 287/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 17.3545 - mae: 3.3245 - val_loss: 16.3638 - val_mae: 3.1372\n",
            "Epoch 288/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 17.1633 - mae: 3.3008 - val_loss: 16.9141 - val_mae: 3.2190\n",
            "Epoch 289/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 17.1372 - mae: 3.3108 - val_loss: 16.4968 - val_mae: 3.1675\n",
            "Epoch 290/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 16.9902 - mae: 3.2801 - val_loss: 17.0896 - val_mae: 3.2414\n",
            "Epoch 291/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 16.9543 - mae: 3.2988 - val_loss: 16.6457 - val_mae: 3.1880\n",
            "Epoch 292/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 16.6414 - mae: 3.2495 - val_loss: 15.4254 - val_mae: 3.0483\n",
            "Epoch 293/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 16.8170 - mae: 3.2873 - val_loss: 15.5709 - val_mae: 3.0734\n",
            "Epoch 294/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 16.6175 - mae: 3.2622 - val_loss: 16.0949 - val_mae: 3.1284\n",
            "Epoch 295/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 16.4748 - mae: 3.2395 - val_loss: 16.2973 - val_mae: 3.1516\n",
            "Epoch 296/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 16.3995 - mae: 3.2408 - val_loss: 17.6744 - val_mae: 3.2866\n",
            "Epoch 297/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 16.5093 - mae: 3.2536 - val_loss: 15.4333 - val_mae: 3.0731\n",
            "Epoch 298/500\n",
            "89/89 [==============================] - 25s 281ms/step - loss: 16.4753 - mae: 3.2264 - val_loss: 15.2012 - val_mae: 3.0375\n",
            "Epoch 299/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 16.3340 - mae: 3.2418 - val_loss: 15.0091 - val_mae: 3.0268\n",
            "Epoch 300/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 16.5627 - mae: 3.2572 - val_loss: 15.0887 - val_mae: 3.0421\n",
            "Epoch 301/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 15.7636 - mae: 3.1890 - val_loss: 16.1559 - val_mae: 3.1447\n",
            "Epoch 302/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 15.8380 - mae: 3.1846 - val_loss: 15.8824 - val_mae: 3.1195\n",
            "Epoch 303/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 15.8084 - mae: 3.1990 - val_loss: 15.5924 - val_mae: 3.0911\n",
            "Epoch 304/500\n",
            "89/89 [==============================] - 22s 239ms/step - loss: 15.9410 - mae: 3.2053 - val_loss: 15.8070 - val_mae: 3.1166\n",
            "Epoch 305/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 15.4905 - mae: 3.1677 - val_loss: 16.1112 - val_mae: 3.1486\n",
            "Epoch 306/500\n",
            "89/89 [==============================] - 20s 220ms/step - loss: 15.7952 - mae: 3.1810 - val_loss: 14.8981 - val_mae: 3.0319\n",
            "Epoch 307/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 15.3324 - mae: 3.1586 - val_loss: 15.4039 - val_mae: 3.0907\n",
            "Epoch 308/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 15.4936 - mae: 3.1618 - val_loss: 14.9264 - val_mae: 3.0330\n",
            "Epoch 309/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 15.0149 - mae: 3.1089 - val_loss: 18.6154 - val_mae: 3.3648\n",
            "Epoch 310/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 15.4865 - mae: 3.1682 - val_loss: 18.8555 - val_mae: 3.3895\n",
            "Epoch 311/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 15.0501 - mae: 3.1095 - val_loss: 14.6244 - val_mae: 2.9971\n",
            "Epoch 312/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 15.3437 - mae: 3.1595 - val_loss: 15.0177 - val_mae: 3.0415\n",
            "Epoch 313/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 15.0816 - mae: 3.1201 - val_loss: 17.2510 - val_mae: 3.2639\n",
            "Epoch 314/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 15.0877 - mae: 3.1460 - val_loss: 17.6995 - val_mae: 3.2909\n",
            "Epoch 315/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 15.0508 - mae: 3.1257 - val_loss: 15.5184 - val_mae: 3.0873\n",
            "Epoch 316/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 15.0994 - mae: 3.1192 - val_loss: 14.5619 - val_mae: 3.0050\n",
            "Epoch 317/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 14.9768 - mae: 3.1246 - val_loss: 17.3901 - val_mae: 3.2689\n",
            "Epoch 318/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 14.7765 - mae: 3.1065 - val_loss: 15.5745 - val_mae: 3.1014\n",
            "Epoch 319/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 14.8092 - mae: 3.0954 - val_loss: 18.7384 - val_mae: 3.3979\n",
            "Epoch 320/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 14.5181 - mae: 3.0734 - val_loss: 16.5139 - val_mae: 3.1909\n",
            "Epoch 321/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 14.8947 - mae: 3.1110 - val_loss: 13.9856 - val_mae: 2.9499\n",
            "Epoch 322/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 14.2718 - mae: 3.0475 - val_loss: 15.1301 - val_mae: 3.0552\n",
            "Epoch 323/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 14.3234 - mae: 3.0661 - val_loss: 21.5056 - val_mae: 3.6313\n",
            "Epoch 324/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 14.4928 - mae: 3.0865 - val_loss: 18.7083 - val_mae: 3.3886\n",
            "Epoch 325/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 14.5404 - mae: 3.0888 - val_loss: 13.1097 - val_mae: 2.8684\n",
            "Epoch 326/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 14.7859 - mae: 3.1253 - val_loss: 15.6184 - val_mae: 3.1050\n",
            "Epoch 327/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 14.1265 - mae: 3.0510 - val_loss: 16.5645 - val_mae: 3.1994\n",
            "Epoch 328/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 13.8764 - mae: 3.0114 - val_loss: 13.5786 - val_mae: 2.9062\n",
            "Epoch 329/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 14.3193 - mae: 3.0760 - val_loss: 14.3478 - val_mae: 2.9927\n",
            "Epoch 330/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 14.0057 - mae: 3.0427 - val_loss: 17.4411 - val_mae: 3.2791\n",
            "Epoch 331/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 13.9085 - mae: 3.0204 - val_loss: 17.1632 - val_mae: 3.2370\n",
            "Epoch 332/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 14.2105 - mae: 3.0550 - val_loss: 15.0031 - val_mae: 3.0522\n",
            "Epoch 333/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 13.8701 - mae: 3.0219 - val_loss: 13.4317 - val_mae: 2.8855\n",
            "Epoch 334/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 13.8324 - mae: 3.0292 - val_loss: 14.9681 - val_mae: 3.0588\n",
            "Epoch 335/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 13.7789 - mae: 3.0330 - val_loss: 14.6845 - val_mae: 3.0294\n",
            "Epoch 336/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 14.0177 - mae: 3.0509 - val_loss: 14.0858 - val_mae: 2.9551\n",
            "Epoch 337/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 13.6817 - mae: 3.0019 - val_loss: 13.8302 - val_mae: 2.9324\n",
            "Epoch 338/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 13.6917 - mae: 2.9898 - val_loss: 12.9164 - val_mae: 2.8514\n",
            "Epoch 339/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 13.8764 - mae: 3.0415 - val_loss: 12.7373 - val_mae: 2.8113\n",
            "Epoch 340/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 13.8343 - mae: 3.0440 - val_loss: 12.8754 - val_mae: 2.8369\n",
            "Epoch 341/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 13.6669 - mae: 2.9980 - val_loss: 13.7499 - val_mae: 2.9276\n",
            "Epoch 342/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 13.8109 - mae: 3.0504 - val_loss: 12.7868 - val_mae: 2.8324\n",
            "Epoch 343/500\n",
            "89/89 [==============================] - 23s 248ms/step - loss: 13.2314 - mae: 2.9539 - val_loss: 12.7574 - val_mae: 2.8136\n",
            "Epoch 344/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 13.5351 - mae: 3.0062 - val_loss: 11.9960 - val_mae: 2.7661\n",
            "Epoch 345/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 13.4347 - mae: 2.9840 - val_loss: 11.9239 - val_mae: 2.7472\n",
            "Epoch 346/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 13.4700 - mae: 2.9982 - val_loss: 11.9478 - val_mae: 2.7522\n",
            "Epoch 347/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 13.2750 - mae: 2.9601 - val_loss: 13.3494 - val_mae: 2.8812\n",
            "Epoch 348/500\n",
            "89/89 [==============================] - 22s 243ms/step - loss: 13.4856 - mae: 2.9927 - val_loss: 13.1321 - val_mae: 2.8643\n",
            "Epoch 349/500\n",
            "89/89 [==============================] - 20s 220ms/step - loss: 12.6754 - mae: 2.9061 - val_loss: 16.1363 - val_mae: 3.1505\n",
            "Epoch 350/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 13.0110 - mae: 2.9450 - val_loss: 12.2799 - val_mae: 2.7776\n",
            "Epoch 351/500\n",
            "89/89 [==============================] - 29s 321ms/step - loss: 13.0421 - mae: 2.9402 - val_loss: 14.4637 - val_mae: 2.9906\n",
            "Epoch 352/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 13.2417 - mae: 2.9608 - val_loss: 13.7468 - val_mae: 2.9318\n",
            "Epoch 353/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 12.7753 - mae: 2.9036 - val_loss: 13.5276 - val_mae: 2.9113\n",
            "Epoch 354/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 13.8551 - mae: 3.0769 - val_loss: 12.8523 - val_mae: 2.8283\n",
            "Epoch 355/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 13.0991 - mae: 2.9514 - val_loss: 12.0653 - val_mae: 2.7578\n",
            "Epoch 356/500\n",
            "89/89 [==============================] - 20s 221ms/step - loss: 12.7950 - mae: 2.9369 - val_loss: 12.5025 - val_mae: 2.7967\n",
            "Epoch 357/500\n",
            "89/89 [==============================] - 28s 296ms/step - loss: 12.9891 - mae: 2.9427 - val_loss: 15.2085 - val_mae: 3.0755\n",
            "Epoch 358/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 13.1195 - mae: 2.9636 - val_loss: 12.4974 - val_mae: 2.7919\n",
            "Epoch 359/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 12.6580 - mae: 2.9135 - val_loss: 12.6846 - val_mae: 2.8072\n",
            "Epoch 360/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 13.1428 - mae: 2.9400 - val_loss: 12.8329 - val_mae: 2.8199\n",
            "Epoch 361/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 13.0582 - mae: 2.9495 - val_loss: 12.8695 - val_mae: 2.8369\n",
            "Epoch 362/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 12.9553 - mae: 2.9193 - val_loss: 14.4005 - val_mae: 2.9880\n",
            "Epoch 363/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 12.6374 - mae: 2.8988 - val_loss: 12.4700 - val_mae: 2.7993\n",
            "Epoch 364/500\n",
            "89/89 [==============================] - 22s 230ms/step - loss: 12.6220 - mae: 2.9000 - val_loss: 13.0134 - val_mae: 2.8539\n",
            "Epoch 365/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 12.2956 - mae: 2.8661 - val_loss: 16.1723 - val_mae: 3.1683\n",
            "Epoch 366/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 12.6868 - mae: 2.8958 - val_loss: 12.8758 - val_mae: 2.8391\n",
            "Epoch 367/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 12.5016 - mae: 2.8483 - val_loss: 15.1701 - val_mae: 3.0689\n",
            "Epoch 368/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 13.0042 - mae: 2.9408 - val_loss: 12.5537 - val_mae: 2.8052\n",
            "Epoch 369/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 12.3879 - mae: 2.8635 - val_loss: 12.4733 - val_mae: 2.7841\n",
            "Epoch 370/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 12.4521 - mae: 2.8815 - val_loss: 14.6206 - val_mae: 3.0108\n",
            "Epoch 371/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 12.4132 - mae: 2.8776 - val_loss: 13.8905 - val_mae: 2.9329\n",
            "Epoch 372/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 12.6834 - mae: 2.9051 - val_loss: 18.1422 - val_mae: 3.3414\n",
            "Epoch 373/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 12.4461 - mae: 2.8678 - val_loss: 12.6632 - val_mae: 2.8207\n",
            "Epoch 374/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 12.5524 - mae: 2.9033 - val_loss: 21.2688 - val_mae: 3.6161\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 12.5223 - mae: 2.8780 - val_loss: 14.2083 - val_mae: 2.9927\n",
            "Epoch 376/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 11.9458 - mae: 2.8284 - val_loss: 15.0951 - val_mae: 3.0600\n",
            "Epoch 377/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 12.7287 - mae: 2.9013 - val_loss: 16.2832 - val_mae: 3.1656\n",
            "Epoch 378/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 12.6952 - mae: 2.9186 - val_loss: 15.0300 - val_mae: 3.0459\n",
            "Epoch 379/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 12.1018 - mae: 2.8368 - val_loss: 14.2100 - val_mae: 2.9735\n",
            "Epoch 380/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 12.3056 - mae: 2.8673 - val_loss: 20.6048 - val_mae: 3.5386\n",
            "Epoch 381/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 12.5038 - mae: 2.9004 - val_loss: 18.9493 - val_mae: 3.3871\n",
            "Epoch 382/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 11.8766 - mae: 2.8140 - val_loss: 18.9249 - val_mae: 3.3927\n",
            "Epoch 383/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 12.1724 - mae: 2.8456 - val_loss: 18.0215 - val_mae: 3.3173\n",
            "Epoch 384/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 12.2676 - mae: 2.8653 - val_loss: 15.6012 - val_mae: 3.1053\n",
            "Epoch 385/500\n",
            "89/89 [==============================] - 20s 218ms/step - loss: 12.1854 - mae: 2.8655 - val_loss: 18.2425 - val_mae: 3.3447\n",
            "Epoch 386/500\n",
            "89/89 [==============================] - 23s 245ms/step - loss: 11.7193 - mae: 2.7901 - val_loss: 15.5613 - val_mae: 3.0913\n",
            "Epoch 387/500\n",
            "89/89 [==============================] - 24s 263ms/step - loss: 12.1860 - mae: 2.8570 - val_loss: 14.9871 - val_mae: 3.0478\n",
            "Epoch 388/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 12.1957 - mae: 2.8692 - val_loss: 12.8992 - val_mae: 2.8415\n",
            "Epoch 389/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 12.6876 - mae: 2.8941 - val_loss: 14.6593 - val_mae: 3.0199\n",
            "Epoch 390/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 12.0723 - mae: 2.8454 - val_loss: 18.0259 - val_mae: 3.3105\n",
            "Epoch 391/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 11.9367 - mae: 2.8028 - val_loss: 20.6700 - val_mae: 3.5417\n",
            "Epoch 392/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 12.0366 - mae: 2.8308 - val_loss: 14.0391 - val_mae: 2.9446\n",
            "Epoch 393/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 12.1822 - mae: 2.8375 - val_loss: 18.2868 - val_mae: 3.3150\n",
            "Epoch 394/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 12.2012 - mae: 2.8769 - val_loss: 11.1731 - val_mae: 2.6829\n",
            "Epoch 395/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 12.0037 - mae: 2.8316 - val_loss: 15.2749 - val_mae: 3.0659\n",
            "Epoch 396/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 11.9936 - mae: 2.8291 - val_loss: 13.0500 - val_mae: 2.8580\n",
            "Epoch 397/500\n",
            "89/89 [==============================] - 22s 241ms/step - loss: 12.0569 - mae: 2.8412 - val_loss: 13.5875 - val_mae: 2.9125\n",
            "Epoch 398/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 11.8113 - mae: 2.8146 - val_loss: 13.9392 - val_mae: 2.9426\n",
            "Epoch 399/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 12.6259 - mae: 2.9163 - val_loss: 13.5124 - val_mae: 2.9006\n",
            "Epoch 400/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 12.3430 - mae: 2.8890 - val_loss: 11.3175 - val_mae: 2.6783\n",
            "Epoch 401/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 11.8171 - mae: 2.8237 - val_loss: 16.6143 - val_mae: 3.1919\n",
            "Epoch 402/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 12.0380 - mae: 2.8050 - val_loss: 12.2900 - val_mae: 2.7588\n",
            "Epoch 403/500\n",
            "89/89 [==============================] - 24s 258ms/step - loss: 11.8610 - mae: 2.8023 - val_loss: 10.5134 - val_mae: 2.5840\n",
            "Epoch 404/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 11.6585 - mae: 2.7905 - val_loss: 14.6559 - val_mae: 3.0094\n",
            "Epoch 405/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 11.4185 - mae: 2.7507 - val_loss: 10.9153 - val_mae: 2.6293\n",
            "Epoch 406/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 11.5311 - mae: 2.7684 - val_loss: 18.6353 - val_mae: 3.3529\n",
            "Epoch 407/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 12.1810 - mae: 2.8139 - val_loss: 13.3510 - val_mae: 2.8727\n",
            "Epoch 408/500\n",
            "89/89 [==============================] - 29s 322ms/step - loss: 11.8742 - mae: 2.8164 - val_loss: 14.8592 - val_mae: 3.0216\n",
            "Epoch 409/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 11.1364 - mae: 2.7072 - val_loss: 14.3854 - val_mae: 2.9509\n",
            "Epoch 410/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 11.1454 - mae: 2.7200 - val_loss: 12.0130 - val_mae: 2.7309\n",
            "Epoch 411/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 11.4359 - mae: 2.7802 - val_loss: 11.8680 - val_mae: 2.7091\n",
            "Epoch 412/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 11.7499 - mae: 2.8000 - val_loss: 14.4691 - val_mae: 2.9953\n",
            "Epoch 413/500\n",
            "89/89 [==============================] - 29s 319ms/step - loss: 11.6481 - mae: 2.7624 - val_loss: 15.2508 - val_mae: 3.0651\n",
            "Epoch 414/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 11.7364 - mae: 2.8090 - val_loss: 12.3782 - val_mae: 2.7507\n",
            "Epoch 415/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 12.1395 - mae: 2.8087 - val_loss: 14.1680 - val_mae: 2.9313\n",
            "Epoch 416/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 11.2383 - mae: 2.7115 - val_loss: 15.3328 - val_mae: 2.9909\n",
            "Epoch 417/500\n",
            "89/89 [==============================] - 23s 249ms/step - loss: 11.3360 - mae: 2.7013 - val_loss: 11.7811 - val_mae: 2.6876\n",
            "Epoch 418/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 11.4712 - mae: 2.7621 - val_loss: 11.5408 - val_mae: 2.6656\n",
            "Epoch 419/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 11.7262 - mae: 2.7743 - val_loss: 12.9369 - val_mae: 2.7882\n",
            "Epoch 420/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 11.4968 - mae: 2.7592 - val_loss: 11.3822 - val_mae: 2.6265\n",
            "Epoch 421/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 11.4571 - mae: 2.7758 - val_loss: 10.0338 - val_mae: 2.5179\n",
            "Epoch 422/500\n",
            "89/89 [==============================] - 30s 330ms/step - loss: 11.4699 - mae: 2.7616 - val_loss: 13.0262 - val_mae: 2.8177\n",
            "Epoch 423/500\n",
            "89/89 [==============================] - 23s 250ms/step - loss: 11.5239 - mae: 2.7707 - val_loss: 12.8770 - val_mae: 2.7981\n",
            "Epoch 424/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 11.0160 - mae: 2.6874 - val_loss: 12.3536 - val_mae: 2.7226\n",
            "Epoch 425/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 11.4783 - mae: 2.7298 - val_loss: 16.5658 - val_mae: 3.1136\n",
            "Epoch 426/500\n",
            "89/89 [==============================] - 23s 256ms/step - loss: 10.8013 - mae: 2.6657 - val_loss: 15.5619 - val_mae: 3.0315\n",
            "Epoch 427/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 11.1875 - mae: 2.7095 - val_loss: 12.6524 - val_mae: 2.7652\n",
            "Epoch 428/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 11.5682 - mae: 2.7419 - val_loss: 15.5306 - val_mae: 3.0254\n",
            "Epoch 429/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 11.3283 - mae: 2.7391 - val_loss: 13.0401 - val_mae: 2.7754\n",
            "Epoch 430/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 11.1041 - mae: 2.6887 - val_loss: 11.5409 - val_mae: 2.6284\n",
            "Epoch 431/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 11.0480 - mae: 2.6820 - val_loss: 14.1784 - val_mae: 2.8843\n",
            "Epoch 432/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 11.2324 - mae: 2.6943 - val_loss: 11.6881 - val_mae: 2.6505\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 11.3767 - mae: 2.7168 - val_loss: 10.5285 - val_mae: 2.5514\n",
            "Epoch 434/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 11.5009 - mae: 2.7573 - val_loss: 10.5212 - val_mae: 2.5275\n",
            "Epoch 435/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 11.5487 - mae: 2.7425 - val_loss: 13.9634 - val_mae: 2.8580\n",
            "Epoch 436/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 10.8550 - mae: 2.6208 - val_loss: 9.5036 - val_mae: 2.4234\n",
            "Epoch 437/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 11.1253 - mae: 2.6685 - val_loss: 12.8877 - val_mae: 2.7694\n",
            "Epoch 438/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 11.1164 - mae: 2.6773 - val_loss: 13.0586 - val_mae: 2.7752\n",
            "Epoch 439/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 10.8173 - mae: 2.6527 - val_loss: 10.6706 - val_mae: 2.5491\n",
            "Epoch 440/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 11.0153 - mae: 2.6505 - val_loss: 10.1637 - val_mae: 2.4901\n",
            "Epoch 441/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 10.3947 - mae: 2.6171 - val_loss: 14.2123 - val_mae: 2.8822\n",
            "Epoch 442/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 10.5899 - mae: 2.6079 - val_loss: 13.8085 - val_mae: 2.8388\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 11.0311 - mae: 2.6516 - val_loss: 12.5458 - val_mae: 2.7134\n",
            "Epoch 444/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 10.7885 - mae: 2.6528 - val_loss: 13.4825 - val_mae: 2.8327\n",
            "Epoch 445/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 10.2773 - mae: 2.5857 - val_loss: 13.7025 - val_mae: 2.8709\n",
            "Epoch 446/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 10.7402 - mae: 2.6043 - val_loss: 13.8764 - val_mae: 2.8682\n",
            "Epoch 447/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 11.3507 - mae: 2.7037 - val_loss: 11.2141 - val_mae: 2.6111\n",
            "Epoch 448/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 11.2698 - mae: 2.6882 - val_loss: 13.9276 - val_mae: 2.8747\n",
            "Epoch 449/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 10.4124 - mae: 2.5808 - val_loss: 13.2925 - val_mae: 2.8071\n",
            "Epoch 450/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 11.3176 - mae: 2.7115 - val_loss: 12.7513 - val_mae: 2.7686\n",
            "Epoch 451/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 10.9375 - mae: 2.6579 - val_loss: 10.8821 - val_mae: 2.5672\n",
            "Epoch 452/500\n",
            "89/89 [==============================] - 22s 244ms/step - loss: 11.0484 - mae: 2.6546 - val_loss: 12.7282 - val_mae: 2.7583\n",
            "Epoch 453/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 10.3282 - mae: 2.5886 - val_loss: 11.9328 - val_mae: 2.6916\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 10.7412 - mae: 2.6243 - val_loss: 13.9946 - val_mae: 2.8782\n",
            "Epoch 455/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 11.1682 - mae: 2.6456 - val_loss: 14.1263 - val_mae: 2.8890\n",
            "Epoch 456/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 10.2874 - mae: 2.5739 - val_loss: 15.7682 - val_mae: 3.0421\n",
            "Epoch 457/500\n",
            "89/89 [==============================] - 30s 333ms/step - loss: 10.4672 - mae: 2.5959 - val_loss: 11.8648 - val_mae: 2.6634\n",
            "Epoch 458/500\n",
            "89/89 [==============================] - 29s 318ms/step - loss: 10.3157 - mae: 2.5730 - val_loss: 12.1203 - val_mae: 2.6889\n",
            "Epoch 459/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 10.9736 - mae: 2.6649 - val_loss: 13.0087 - val_mae: 2.7580\n",
            "Epoch 460/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 10.6914 - mae: 2.6220 - val_loss: 15.4112 - val_mae: 3.0054\n",
            "Epoch 461/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 10.8142 - mae: 2.6127 - val_loss: 11.3833 - val_mae: 2.6249\n",
            "Epoch 462/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 10.0431 - mae: 2.5326 - val_loss: 13.0601 - val_mae: 2.7995\n",
            "Epoch 463/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 10.2350 - mae: 2.5293 - val_loss: 11.1086 - val_mae: 2.5794\n",
            "Epoch 464/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 10.3784 - mae: 2.5865 - val_loss: 17.4500 - val_mae: 3.1978\n",
            "Epoch 465/500\n",
            "89/89 [==============================] - 29s 327ms/step - loss: 10.5777 - mae: 2.5852 - val_loss: 12.5913 - val_mae: 2.7365\n",
            "Epoch 466/500\n",
            "89/89 [==============================] - 30s 327ms/step - loss: 10.6159 - mae: 2.6128 - val_loss: 12.6113 - val_mae: 2.7328\n",
            "Epoch 467/500\n",
            "89/89 [==============================] - 30s 332ms/step - loss: 10.6418 - mae: 2.6110 - val_loss: 11.3771 - val_mae: 2.6357\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - 29s 318ms/step - loss: 10.5601 - mae: 2.5992 - val_loss: 13.7205 - val_mae: 2.8321\n",
            "Epoch 469/500\n",
            "89/89 [==============================] - 24s 258ms/step - loss: 10.6896 - mae: 2.6137 - val_loss: 10.9616 - val_mae: 2.5770\n",
            "Epoch 470/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 10.7419 - mae: 2.6210 - val_loss: 13.4917 - val_mae: 2.8229\n",
            "Epoch 471/500\n",
            "89/89 [==============================] - 29s 312ms/step - loss: 10.8031 - mae: 2.6263 - val_loss: 10.7290 - val_mae: 2.5417\n",
            "Epoch 472/500\n",
            "89/89 [==============================] - 29s 322ms/step - loss: 9.8790 - mae: 2.5044 - val_loss: 13.8901 - val_mae: 2.8584\n",
            "Epoch 473/500\n",
            "89/89 [==============================] - 29s 316ms/step - loss: 10.5412 - mae: 2.5909 - val_loss: 11.7632 - val_mae: 2.6427\n",
            "Epoch 474/500\n",
            "89/89 [==============================] - 29s 314ms/step - loss: 10.0225 - mae: 2.5081 - val_loss: 12.3246 - val_mae: 2.7060\n",
            "Epoch 475/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 10.0535 - mae: 2.5416 - val_loss: 9.7004 - val_mae: 2.4477\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 10.3330 - mae: 2.5308 - val_loss: 11.9179 - val_mae: 2.6679\n",
            "Epoch 477/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 10.1100 - mae: 2.5435 - val_loss: 9.9353 - val_mae: 2.4610\n",
            "Epoch 478/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 9.9111 - mae: 2.5445 - val_loss: 12.3940 - val_mae: 2.6958\n",
            "Epoch 479/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 10.8663 - mae: 2.6249 - val_loss: 9.6957 - val_mae: 2.4528\n",
            "Epoch 480/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 10.6843 - mae: 2.5886 - val_loss: 10.5256 - val_mae: 2.5205\n",
            "Epoch 481/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 9.8053 - mae: 2.5133 - val_loss: 13.1999 - val_mae: 2.7932\n",
            "Epoch 482/500\n",
            "89/89 [==============================] - 29s 308ms/step - loss: 10.1039 - mae: 2.5207 - val_loss: 11.7891 - val_mae: 2.6481\n",
            "Epoch 483/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 10.1966 - mae: 2.5427 - val_loss: 11.4647 - val_mae: 2.6164\n",
            "Epoch 484/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 10.4645 - mae: 2.5866 - val_loss: 9.2455 - val_mae: 2.3882\n",
            "Epoch 485/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 10.5214 - mae: 2.6003 - val_loss: 11.6053 - val_mae: 2.6249\n",
            "Epoch 486/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 9.8844 - mae: 2.5148 - val_loss: 10.5677 - val_mae: 2.5243\n",
            "Epoch 487/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 10.1014 - mae: 2.5374 - val_loss: 10.8864 - val_mae: 2.5519\n",
            "Epoch 488/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 9.7335 - mae: 2.4790 - val_loss: 12.2757 - val_mae: 2.7049\n",
            "Epoch 489/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 9.8640 - mae: 2.5157 - val_loss: 10.2672 - val_mae: 2.4728\n",
            "Epoch 490/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 9.9681 - mae: 2.4947 - val_loss: 11.6277 - val_mae: 2.6338\n",
            "Epoch 491/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 10.1157 - mae: 2.5616 - val_loss: 11.3199 - val_mae: 2.6136\n",
            "Epoch 492/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 9.9513 - mae: 2.5293 - val_loss: 11.2157 - val_mae: 2.5984\n",
            "Epoch 493/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 10.2080 - mae: 2.5591 - val_loss: 12.4962 - val_mae: 2.7171\n",
            "Epoch 494/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 9.8646 - mae: 2.5006 - val_loss: 9.8579 - val_mae: 2.4603\n",
            "Epoch 495/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 10.3793 - mae: 2.5652 - val_loss: 10.6489 - val_mae: 2.5280\n",
            "Epoch 496/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 10.4477 - mae: 2.5735 - val_loss: 12.1106 - val_mae: 2.6659\n",
            "Epoch 497/500\n",
            "89/89 [==============================] - 22s 236ms/step - loss: 10.1519 - mae: 2.5310 - val_loss: 9.5754 - val_mae: 2.4174\n",
            "Epoch 498/500\n",
            "89/89 [==============================] - 23s 245ms/step - loss: 10.5264 - mae: 2.5815 - val_loss: 10.0363 - val_mae: 2.4606\n",
            "Epoch 499/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 9.7917 - mae: 2.4931 - val_loss: 9.2977 - val_mae: 2.3846\n",
            "Epoch 500/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 9.9090 - mae: 2.5015 - val_loss: 8.8587 - val_mae: 2.3378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and validation MeanAbsoluteError')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y3K89-CM-dfg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "73547063-a7a0-439b-a2df-8a5ed0492df3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_x"
      ],
      "metadata": {
        "id": "N_sP_ZmSY-Jv",
        "outputId": "5554835e-af8c-4692-e71e-e838a8d4d300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download the model\n"
      ],
      "metadata": {
        "id": "R19IJQSYoW7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/My Drive/cut_panoramic/Model', exist_ok=True)\n",
        "model.save('/content/drive/My Drive/cut_panoramic/Model/2e-4_16_0.2_Male18_500.h5')"
      ],
      "metadata": {
        "id": "Zed4TdFcG2iJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n5YxZ-5QjQ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download('/content/drive/My Drive/cut_panoramic/Model/1.1_รอบแรก_Flimpano_Male125_250.h5')"
      ],
      "metadata": {
        "id": "P5eMxm1NV-oY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wY_pDlxkRwxS"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}