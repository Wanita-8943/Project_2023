{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wanita-8943/Project_2023/blob/main/1_2e-2_16_0.2_Male18_500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#เรียกใช้ CSV"
      ],
      "metadata": {
        "id": "ow7eWoNw6U-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil"
      ],
      "metadata": {
        "id": "DlQ2oT7cIR6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_2Fe8u81d5r",
        "outputId": "f44f9bd1-dbd5-4305-a140-a2c0fe77129e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "5qxePnnn7TGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "D-hCRloc3t39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "YZWXwjXeGxZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#กำหนดค่าพารามิเตอร์\n"
      ],
      "metadata": {
        "id": "RooqSdBc7QHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "width = 150\n",
        "height = 150\n",
        "epochs = 500\n",
        "NUM_TRAIN = 1425\n",
        "NUM_TEST = 475\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3)"
      ],
      "metadata": {
        "id": "thDb7U9B3xOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clone efficientnet repo\n"
      ],
      "metadata": {
        "id": "pumGmy6f3eSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7iy2f8n16p0",
        "outputId": "01ce8bf9-42b0-48e6-e79e-c7197a30422e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (172/172), done.\u001b[K\n",
            "remote: Total 837 (delta 253), reused 248 (delta 187), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 31.44 MiB/s, done.\n",
            "Resolving deltas: 100% (493/493), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "Vbdblwbv89fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "vnLrobY_-GCx",
        "outputId": "7c87531c-2e6f-4fe5-9757-ca2390588a5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "tjZBRnfo3bN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "hNZAzbDp3lgA",
        "outputId": "f1154077-5d43-40ff-edb6-30219e43259b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(19, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "yWNKfQUt5rga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "NadBB12251jh",
        "outputId": "d5b2153e-439a-466d-d981-8620682cc3f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 19)                24339     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 20        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,073,923\n",
            "Trainable params: 4,031,907\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "GepWq3yy53t5",
        "outputId": "4c81b7e9-7bcd-4aca-9ab3-b677faeb0f5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD76D6c_79py",
        "outputId": "e82762e3-f1d2-4577-c362-909966b72474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 0\n",
            "Non-trainable params: 4,049,564\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#สร้างโฟลเดอร์ Train Valodation และ Test"
      ],
      "metadata": {
        "id": "J36J9EAE7qSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/MyDrive/cut_panoramic/Data/New_Data_Male125.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "skoKhKJDngAZ",
        "outputId": "dc7a1b6e-6678-4788-8022-6fc0cb7a46a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Fig_Age  Fig_Person_Sex  Age(year) Class  Class_Re       Filename  \\\n",
              "0           1               1          7  Y07M         0       VV03.jpg   \n",
              "1           2               1          7  Y07M         0  Flip_VV03.jpg   \n",
              "2           3               2          7  Y07M         0       VV04.jpg   \n",
              "3           4               2          7  Y07M         0  Flip_VV04.jpg   \n",
              "4           5               3          7  Y07M         0       VV05.jpg   \n",
              "...       ...             ...        ...   ...       ...            ...   \n",
              "2370      121              77         25  Y25M        18  Flip_J463.jpg   \n",
              "2371      122              78         25  Y25M        18       J464.jpg   \n",
              "2372      123              78         25  Y25M        18  Flip_J464.jpg   \n",
              "2373      124              79         25  Y25M        18       J465.jpg   \n",
              "2374      125              79         25  Y25M        18  Flip_J465.jpg   \n",
              "\n",
              "                                          Path_filename     Sex Floder  \n",
              "0     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "1     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "2     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "3     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "4     /content/drive/My Drive/TVT_Male125/train/Y07M...  เพศชาย   Both  \n",
              "...                                                 ...     ...    ...  \n",
              "2370  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2371  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2372  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2373  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "2374  /content/drive/My Drive/TVT_Male125/test/Y25M/...  เพศชาย   Both  \n",
              "\n",
              "[2375 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c7f5f60-fddf-4acd-8e21-2b5cfe722885\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fig_Age</th>\n",
              "      <th>Fig_Person_Sex</th>\n",
              "      <th>Age(year)</th>\n",
              "      <th>Class</th>\n",
              "      <th>Class_Re</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Path_filename</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Floder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>VV03.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>Flip_VV03.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>VV04.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>Flip_VV04.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>Y07M</td>\n",
              "      <td>0</td>\n",
              "      <td>VV05.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/train/Y07M...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2370</th>\n",
              "      <td>121</td>\n",
              "      <td>77</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J463.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2371</th>\n",
              "      <td>122</td>\n",
              "      <td>78</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>J464.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2372</th>\n",
              "      <td>123</td>\n",
              "      <td>78</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J464.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2373</th>\n",
              "      <td>124</td>\n",
              "      <td>79</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>J465.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2374</th>\n",
              "      <td>125</td>\n",
              "      <td>79</td>\n",
              "      <td>25</td>\n",
              "      <td>Y25M</td>\n",
              "      <td>18</td>\n",
              "      <td>Flip_J465.jpg</td>\n",
              "      <td>/content/drive/My Drive/TVT_Male125/test/Y25M/...</td>\n",
              "      <td>เพศชาย</td>\n",
              "      <td>Both</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2375 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c7f5f60-fddf-4acd-8e21-2b5cfe722885')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c7f5f60-fddf-4acd-8e21-2b5cfe722885 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c7f5f60-fddf-4acd-8e21-2b5cfe722885');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = df[df['Fig_Age'].between(1,75)]\n",
        "val = df[df['Fig_Age'].between(76,100)]"
      ],
      "metadata": {
        "id": "Z1zBw01Gl8Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/My Drive/TVT_Male125\"\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL0g-8iOnMC4",
        "outputId": "a7ec4ee5-a7b4-4c75-b6b1-f6442deef7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/TVT_Male125/train\n",
            "/content/drive/My Drive/TVT_Male125/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Train"
      ],
      "metadata": {
        "id": "bWEnlTSwazL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, #โมเดลส่วนใหญ่ต้องใช้ RGB ในช่วง 0–1\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "xGPrsn9no_pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'Path_filename',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'Path_filename',\n",
        "        y_col = 'Class_Re',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nVlmAszntK_",
        "outputId": "ab4cf8a7-71b3-433b-f8eb-5a2c5ef993c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1425 validated image filenames.\n",
            "Found 475 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "          optimizer=Adam(learning_rate=2e-2),\n",
        "          metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "N6qUmmF856ZE",
        "outputId": "06cdeaf8-5d99-4d5f-9022-f3de62d46137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-1fcca56a0755>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "89/89 [==============================] - 139s 1s/step - loss: 77.6536 - mae: 7.2478 - val_loss: 57.6414 - val_mae: 6.2004\n",
            "Epoch 2/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 46.5331 - mae: 5.6201 - val_loss: 38.1537 - val_mae: 5.1660\n",
            "Epoch 3/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 34.3890 - mae: 4.9769 - val_loss: 31.7593 - val_mae: 4.8316\n",
            "Epoch 4/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.9168 - mae: 4.7966 - val_loss: 30.1710 - val_mae: 4.7464\n",
            "Epoch 5/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.1257 - mae: 4.7512 - val_loss: 29.9201 - val_mae: 4.7381\n",
            "Epoch 6/500\n",
            "89/89 [==============================] - 20s 219ms/step - loss: 30.0686 - mae: 4.7523 - val_loss: 29.7670 - val_mae: 4.7203\n",
            "Epoch 7/500\n",
            "89/89 [==============================] - 27s 288ms/step - loss: 30.0788 - mae: 4.7467 - val_loss: 30.1763 - val_mae: 4.7525\n",
            "Epoch 8/500\n",
            "89/89 [==============================] - 27s 304ms/step - loss: 30.0321 - mae: 4.7423 - val_loss: 29.8071 - val_mae: 4.7219\n",
            "Epoch 9/500\n",
            "89/89 [==============================] - 22s 245ms/step - loss: 30.1538 - mae: 4.7535 - val_loss: 30.2132 - val_mae: 4.7580\n",
            "Epoch 10/500\n",
            "89/89 [==============================] - 20s 219ms/step - loss: 30.1775 - mae: 4.7585 - val_loss: 29.8993 - val_mae: 4.7232\n",
            "Epoch 11/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.9984 - mae: 4.7360 - val_loss: 29.8355 - val_mae: 4.7214\n",
            "Epoch 12/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 29.9615 - mae: 4.7314 - val_loss: 30.0177 - val_mae: 4.7477\n",
            "Epoch 13/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 29.9870 - mae: 4.7371 - val_loss: 29.7770 - val_mae: 4.7134\n",
            "Epoch 14/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 29.9777 - mae: 4.7364 - val_loss: 29.8800 - val_mae: 4.7307\n",
            "Epoch 15/500\n",
            "89/89 [==============================] - 26s 297ms/step - loss: 29.8871 - mae: 4.7332 - val_loss: 29.8505 - val_mae: 4.7268\n",
            "Epoch 16/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9515 - mae: 4.7353 - val_loss: 29.6735 - val_mae: 4.7103\n",
            "Epoch 17/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 29.9818 - mae: 4.7404 - val_loss: 30.1289 - val_mae: 4.7565\n",
            "Epoch 18/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 29.9476 - mae: 4.7335 - val_loss: 30.2467 - val_mae: 4.7774\n",
            "Epoch 19/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 29.9725 - mae: 4.7389 - val_loss: 30.1057 - val_mae: 4.7509\n",
            "Epoch 20/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 29.9362 - mae: 4.7333 - val_loss: 29.9247 - val_mae: 4.7258\n",
            "Epoch 21/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.1080 - mae: 4.7521 - val_loss: 29.7199 - val_mae: 4.7081\n",
            "Epoch 22/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.0263 - mae: 4.7446 - val_loss: 30.0339 - val_mae: 4.7419\n",
            "Epoch 23/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 30.0537 - mae: 4.7435 - val_loss: 30.0852 - val_mae: 4.7515\n",
            "Epoch 24/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0002 - mae: 4.7370 - val_loss: 29.8601 - val_mae: 4.7311\n",
            "Epoch 25/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.1193 - mae: 4.7497 - val_loss: 30.0414 - val_mae: 4.7388\n",
            "Epoch 26/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 29.9719 - mae: 4.7393 - val_loss: 29.8629 - val_mae: 4.7304\n",
            "Epoch 27/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 29.9131 - mae: 4.7344 - val_loss: 30.0199 - val_mae: 4.7442\n",
            "Epoch 28/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 29.9580 - mae: 4.7352 - val_loss: 29.8798 - val_mae: 4.7337\n",
            "Epoch 29/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 29.9295 - mae: 4.7323 - val_loss: 30.2097 - val_mae: 4.7623\n",
            "Epoch 30/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.1280 - mae: 4.7511 - val_loss: 30.0007 - val_mae: 4.7366\n",
            "Epoch 31/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.2088 - mae: 4.7657 - val_loss: 29.9290 - val_mae: 4.7183\n",
            "Epoch 32/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9453 - mae: 4.7413 - val_loss: 30.0779 - val_mae: 4.7511\n",
            "Epoch 33/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.1132 - mae: 4.7516 - val_loss: 29.8062 - val_mae: 4.7289\n",
            "Epoch 34/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 29.8885 - mae: 4.7286 - val_loss: 30.1180 - val_mae: 4.7559\n",
            "Epoch 35/500\n",
            "89/89 [==============================] - 26s 292ms/step - loss: 30.0148 - mae: 4.7425 - val_loss: 29.9727 - val_mae: 4.7294\n",
            "Epoch 36/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 30.0635 - mae: 4.7449 - val_loss: 29.8046 - val_mae: 4.7253\n",
            "Epoch 37/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.0324 - mae: 4.7432 - val_loss: 30.1774 - val_mae: 4.7592\n",
            "Epoch 38/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 30.0170 - mae: 4.7448 - val_loss: 29.9787 - val_mae: 4.7429\n",
            "Epoch 39/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 29.8533 - mae: 4.7262 - val_loss: 30.0458 - val_mae: 4.7439\n",
            "Epoch 40/500\n",
            "89/89 [==============================] - 29s 319ms/step - loss: 30.1504 - mae: 4.7564 - val_loss: 30.0189 - val_mae: 4.7419\n",
            "Epoch 41/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 30.0679 - mae: 4.7488 - val_loss: 30.0437 - val_mae: 4.7543\n",
            "Epoch 42/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.1114 - mae: 4.7479 - val_loss: 29.7301 - val_mae: 4.7210\n",
            "Epoch 43/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 30.0035 - mae: 4.7393 - val_loss: 30.0581 - val_mae: 4.7351\n",
            "Epoch 44/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.0235 - mae: 4.7423 - val_loss: 30.1725 - val_mae: 4.7438\n",
            "Epoch 45/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 30.1251 - mae: 4.7484 - val_loss: 29.9620 - val_mae: 4.7408\n",
            "Epoch 46/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.0937 - mae: 4.7495 - val_loss: 30.0210 - val_mae: 4.7464\n",
            "Epoch 47/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.0426 - mae: 4.7491 - val_loss: 29.7873 - val_mae: 4.7133\n",
            "Epoch 48/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 30.0848 - mae: 4.7494 - val_loss: 30.0033 - val_mae: 4.7375\n",
            "Epoch 49/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 30.0091 - mae: 4.7430 - val_loss: 30.1525 - val_mae: 4.7567\n",
            "Epoch 50/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 29.9483 - mae: 4.7361 - val_loss: 30.1417 - val_mae: 4.7642\n",
            "Epoch 51/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0797 - mae: 4.7480 - val_loss: 29.8974 - val_mae: 4.7325\n",
            "Epoch 52/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0918 - mae: 4.7480 - val_loss: 29.9392 - val_mae: 4.7291\n",
            "Epoch 53/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9429 - mae: 4.7363 - val_loss: 29.7824 - val_mae: 4.7194\n",
            "Epoch 54/500\n",
            "89/89 [==============================] - 23s 246ms/step - loss: 29.9052 - mae: 4.7327 - val_loss: 30.2533 - val_mae: 4.7741\n",
            "Epoch 55/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 30.0920 - mae: 4.7491 - val_loss: 30.1949 - val_mae: 4.7593\n",
            "Epoch 56/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0403 - mae: 4.7427 - val_loss: 30.3285 - val_mae: 4.7774\n",
            "Epoch 57/500\n",
            "89/89 [==============================] - 26s 278ms/step - loss: 30.1175 - mae: 4.7494 - val_loss: 30.1852 - val_mae: 4.7597\n",
            "Epoch 58/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 30.0086 - mae: 4.7433 - val_loss: 29.9221 - val_mae: 4.7333\n",
            "Epoch 59/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 30.0483 - mae: 4.7489 - val_loss: 30.0258 - val_mae: 4.7500\n",
            "Epoch 60/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0264 - mae: 4.7472 - val_loss: 30.0224 - val_mae: 4.7439\n",
            "Epoch 61/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 29.8729 - mae: 4.7268 - val_loss: 29.8979 - val_mae: 4.7245\n",
            "Epoch 62/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0191 - mae: 4.7447 - val_loss: 30.0996 - val_mae: 4.7617\n",
            "Epoch 63/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 30.0197 - mae: 4.7440 - val_loss: 29.7656 - val_mae: 4.7253\n",
            "Epoch 64/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.0269 - mae: 4.7466 - val_loss: 29.6929 - val_mae: 4.7106\n",
            "Epoch 65/500\n",
            "89/89 [==============================] - 23s 247ms/step - loss: 30.0752 - mae: 4.7456 - val_loss: 30.0614 - val_mae: 4.7399\n",
            "Epoch 66/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0229 - mae: 4.7428 - val_loss: 30.1187 - val_mae: 4.7526\n",
            "Epoch 67/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 30.1238 - mae: 4.7493 - val_loss: 30.1827 - val_mae: 4.7591\n",
            "Epoch 68/500\n",
            "89/89 [==============================] - 20s 217ms/step - loss: 30.1845 - mae: 4.7560 - val_loss: 30.3464 - val_mae: 4.7749\n",
            "Epoch 69/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0645 - mae: 4.7520 - val_loss: 29.9123 - val_mae: 4.7325\n",
            "Epoch 70/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.1061 - mae: 4.7466 - val_loss: 29.7343 - val_mae: 4.7179\n",
            "Epoch 71/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.0561 - mae: 4.7480 - val_loss: 29.9499 - val_mae: 4.7416\n",
            "Epoch 72/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.0449 - mae: 4.7426 - val_loss: 30.0784 - val_mae: 4.7501\n",
            "Epoch 73/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 29.9846 - mae: 4.7351 - val_loss: 29.7875 - val_mae: 4.7135\n",
            "Epoch 74/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.0699 - mae: 4.7446 - val_loss: 29.8320 - val_mae: 4.7245\n",
            "Epoch 75/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0666 - mae: 4.7450 - val_loss: 29.5869 - val_mae: 4.7017\n",
            "Epoch 76/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 29.8816 - mae: 4.7279 - val_loss: 29.7853 - val_mae: 4.7107\n",
            "Epoch 77/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.1670 - mae: 4.7570 - val_loss: 29.6262 - val_mae: 4.7013\n",
            "Epoch 78/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 29.9813 - mae: 4.7352 - val_loss: 29.9599 - val_mae: 4.7338\n",
            "Epoch 79/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 29.9206 - mae: 4.7336 - val_loss: 30.2760 - val_mae: 4.7713\n",
            "Epoch 80/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.0116 - mae: 4.7415 - val_loss: 30.1783 - val_mae: 4.7586\n",
            "Epoch 81/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 30.0480 - mae: 4.7464 - val_loss: 30.1199 - val_mae: 4.7502\n",
            "Epoch 82/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 30.1188 - mae: 4.7500 - val_loss: 30.1333 - val_mae: 4.7500\n",
            "Epoch 83/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.0650 - mae: 4.7456 - val_loss: 30.3690 - val_mae: 4.7809\n",
            "Epoch 84/500\n",
            "89/89 [==============================] - 26s 292ms/step - loss: 30.1057 - mae: 4.7508 - val_loss: 30.0904 - val_mae: 4.7507\n",
            "Epoch 85/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 30.1821 - mae: 4.7559 - val_loss: 29.8536 - val_mae: 4.7184\n",
            "Epoch 86/500\n",
            "89/89 [==============================] - 25s 281ms/step - loss: 29.9139 - mae: 4.7354 - val_loss: 30.0256 - val_mae: 4.7403\n",
            "Epoch 87/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 29.9127 - mae: 4.7311 - val_loss: 29.7035 - val_mae: 4.7098\n",
            "Epoch 88/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 30.1289 - mae: 4.7516 - val_loss: 30.3183 - val_mae: 4.7675\n",
            "Epoch 89/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0135 - mae: 4.7377 - val_loss: 29.6401 - val_mae: 4.7052\n",
            "Epoch 90/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 30.0418 - mae: 4.7413 - val_loss: 30.0565 - val_mae: 4.7410\n",
            "Epoch 91/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.9603 - mae: 4.7377 - val_loss: 30.1921 - val_mae: 4.7615\n",
            "Epoch 92/500\n",
            "89/89 [==============================] - 24s 262ms/step - loss: 29.9921 - mae: 4.7390 - val_loss: 30.0122 - val_mae: 4.7401\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 30.0862 - mae: 4.7477 - val_loss: 30.2664 - val_mae: 4.7669\n",
            "Epoch 94/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 29.9799 - mae: 4.7352 - val_loss: 29.6600 - val_mae: 4.7061\n",
            "Epoch 95/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 30.0453 - mae: 4.7427 - val_loss: 29.9541 - val_mae: 4.7398\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.1136 - mae: 4.7469 - val_loss: 30.1840 - val_mae: 4.7659\n",
            "Epoch 97/500\n",
            "89/89 [==============================] - 22s 238ms/step - loss: 30.2119 - mae: 4.7620 - val_loss: 30.2263 - val_mae: 4.7639\n",
            "Epoch 98/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0078 - mae: 4.7413 - val_loss: 29.6664 - val_mae: 4.7024\n",
            "Epoch 99/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0091 - mae: 4.7456 - val_loss: 30.2237 - val_mae: 4.7621\n",
            "Epoch 100/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.1149 - mae: 4.7485 - val_loss: 30.0007 - val_mae: 4.7334\n",
            "Epoch 101/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 30.1106 - mae: 4.7476 - val_loss: 29.9565 - val_mae: 4.7364\n",
            "Epoch 102/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 30.0428 - mae: 4.7491 - val_loss: 30.3154 - val_mae: 4.7776\n",
            "Epoch 103/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.1077 - mae: 4.7511 - val_loss: 30.5672 - val_mae: 4.8071\n",
            "Epoch 104/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0706 - mae: 4.7449 - val_loss: 30.1833 - val_mae: 4.7565\n",
            "Epoch 105/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 30.0384 - mae: 4.7450 - val_loss: 29.9698 - val_mae: 4.7333\n",
            "Epoch 106/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0413 - mae: 4.7440 - val_loss: 29.8387 - val_mae: 4.7223\n",
            "Epoch 107/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 30.0105 - mae: 4.7402 - val_loss: 29.6878 - val_mae: 4.7080\n",
            "Epoch 108/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0015 - mae: 4.7380 - val_loss: 30.1637 - val_mae: 4.7415\n",
            "Epoch 109/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0143 - mae: 4.7432 - val_loss: 30.1292 - val_mae: 4.7526\n",
            "Epoch 110/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0426 - mae: 4.7436 - val_loss: 30.1314 - val_mae: 4.7534\n",
            "Epoch 111/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 30.0460 - mae: 4.7440 - val_loss: 30.0726 - val_mae: 4.7496\n",
            "Epoch 112/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 30.0265 - mae: 4.7422 - val_loss: 29.5436 - val_mae: 4.7067\n",
            "Epoch 113/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.1206 - mae: 4.7554 - val_loss: 29.6450 - val_mae: 4.6971\n",
            "Epoch 114/500\n",
            "89/89 [==============================] - 26s 278ms/step - loss: 30.0544 - mae: 4.7456 - val_loss: 29.8337 - val_mae: 4.7207\n",
            "Epoch 115/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.8775 - mae: 4.7268 - val_loss: 29.7550 - val_mae: 4.7142\n",
            "Epoch 116/500\n",
            "89/89 [==============================] - 27s 303ms/step - loss: 30.1055 - mae: 4.7475 - val_loss: 30.0851 - val_mae: 4.7482\n",
            "Epoch 117/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 30.0144 - mae: 4.7423 - val_loss: 30.2375 - val_mae: 4.7663\n",
            "Epoch 118/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 29.8560 - mae: 4.7237 - val_loss: 29.9567 - val_mae: 4.7299\n",
            "Epoch 119/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 30.0507 - mae: 4.7424 - val_loss: 30.2069 - val_mae: 4.7692\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - 27s 287ms/step - loss: 30.1012 - mae: 4.7506 - val_loss: 29.7927 - val_mae: 4.7100\n",
            "Epoch 121/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0091 - mae: 4.7360 - val_loss: 29.7787 - val_mae: 4.7201\n",
            "Epoch 122/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.1931 - mae: 4.7589 - val_loss: 29.8698 - val_mae: 4.7302\n",
            "Epoch 123/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.1810 - mae: 4.7562 - val_loss: 29.8677 - val_mae: 4.7335\n",
            "Epoch 124/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.0637 - mae: 4.7490 - val_loss: 30.1016 - val_mae: 4.7437\n",
            "Epoch 125/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.1132 - mae: 4.7535 - val_loss: 29.8768 - val_mae: 4.7406\n",
            "Epoch 126/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 30.0366 - mae: 4.7418 - val_loss: 30.2057 - val_mae: 4.7652\n",
            "Epoch 127/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 29.9508 - mae: 4.7414 - val_loss: 30.0572 - val_mae: 4.7446\n",
            "Epoch 128/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 29.9852 - mae: 4.7366 - val_loss: 29.9991 - val_mae: 4.7417\n",
            "Epoch 129/500\n",
            "89/89 [==============================] - 22s 238ms/step - loss: 30.0145 - mae: 4.7448 - val_loss: 29.9900 - val_mae: 4.7461\n",
            "Epoch 130/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 29.9718 - mae: 4.7359 - val_loss: 29.7900 - val_mae: 4.7148\n",
            "Epoch 131/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 29.9039 - mae: 4.7341 - val_loss: 30.0826 - val_mae: 4.7395\n",
            "Epoch 132/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.1381 - mae: 4.7527 - val_loss: 29.8829 - val_mae: 4.7350\n",
            "Epoch 133/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0376 - mae: 4.7441 - val_loss: 29.9107 - val_mae: 4.7306\n",
            "Epoch 134/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9043 - mae: 4.7267 - val_loss: 29.9815 - val_mae: 4.7362\n",
            "Epoch 135/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0354 - mae: 4.7386 - val_loss: 29.9027 - val_mae: 4.7293\n",
            "Epoch 136/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.1955 - mae: 4.7628 - val_loss: 29.6628 - val_mae: 4.7061\n",
            "Epoch 137/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.0839 - mae: 4.7478 - val_loss: 29.9173 - val_mae: 4.7267\n",
            "Epoch 138/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 30.0101 - mae: 4.7442 - val_loss: 30.0636 - val_mae: 4.7406\n",
            "Epoch 139/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 30.0218 - mae: 4.7413 - val_loss: 29.9513 - val_mae: 4.7353\n",
            "Epoch 140/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 29.9660 - mae: 4.7357 - val_loss: 29.9927 - val_mae: 4.7330\n",
            "Epoch 141/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0814 - mae: 4.7490 - val_loss: 30.2429 - val_mae: 4.7615\n",
            "Epoch 142/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 30.0613 - mae: 4.7457 - val_loss: 29.6932 - val_mae: 4.7127\n",
            "Epoch 143/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0988 - mae: 4.7483 - val_loss: 30.1138 - val_mae: 4.7465\n",
            "Epoch 144/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0714 - mae: 4.7432 - val_loss: 30.4190 - val_mae: 4.7878\n",
            "Epoch 145/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 30.1289 - mae: 4.7548 - val_loss: 29.7388 - val_mae: 4.7148\n",
            "Epoch 146/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 29.9169 - mae: 4.7301 - val_loss: 29.8729 - val_mae: 4.7330\n",
            "Epoch 147/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0595 - mae: 4.7488 - val_loss: 29.8279 - val_mae: 4.7208\n",
            "Epoch 148/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.0251 - mae: 4.7416 - val_loss: 30.1875 - val_mae: 4.7484\n",
            "Epoch 149/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.1113 - mae: 4.7516 - val_loss: 29.6388 - val_mae: 4.7037\n",
            "Epoch 150/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 30.0509 - mae: 4.7471 - val_loss: 30.0333 - val_mae: 4.7375\n",
            "Epoch 151/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 30.0893 - mae: 4.7489 - val_loss: 29.9080 - val_mae: 4.7300\n",
            "Epoch 152/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 30.1253 - mae: 4.7507 - val_loss: 29.7980 - val_mae: 4.7243\n",
            "Epoch 153/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 30.1292 - mae: 4.7478 - val_loss: 29.9826 - val_mae: 4.7312\n",
            "Epoch 154/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.1174 - mae: 4.7509 - val_loss: 29.6195 - val_mae: 4.7006\n",
            "Epoch 155/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9567 - mae: 4.7385 - val_loss: 29.7623 - val_mae: 4.7169\n",
            "Epoch 156/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 30.1316 - mae: 4.7489 - val_loss: 29.7552 - val_mae: 4.7076\n",
            "Epoch 157/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.1581 - mae: 4.7614 - val_loss: 29.9081 - val_mae: 4.7348\n",
            "Epoch 158/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0420 - mae: 4.7458 - val_loss: 30.2695 - val_mae: 4.7723\n",
            "Epoch 159/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0177 - mae: 4.7375 - val_loss: 29.9431 - val_mae: 4.7273\n",
            "Epoch 160/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9291 - mae: 4.7308 - val_loss: 29.9174 - val_mae: 4.7288\n",
            "Epoch 161/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 29.9989 - mae: 4.7385 - val_loss: 29.9095 - val_mae: 4.7233\n",
            "Epoch 162/500\n",
            "89/89 [==============================] - 25s 284ms/step - loss: 29.9885 - mae: 4.7373 - val_loss: 30.1834 - val_mae: 4.7617\n",
            "Epoch 163/500\n",
            "89/89 [==============================] - 28s 293ms/step - loss: 30.0272 - mae: 4.7442 - val_loss: 29.9193 - val_mae: 4.7284\n",
            "Epoch 164/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 30.1371 - mae: 4.7599 - val_loss: 30.1317 - val_mae: 4.7482\n",
            "Epoch 165/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.0703 - mae: 4.7488 - val_loss: 30.1317 - val_mae: 4.7531\n",
            "Epoch 166/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0814 - mae: 4.7459 - val_loss: 30.0130 - val_mae: 4.7346\n",
            "Epoch 167/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0754 - mae: 4.7448 - val_loss: 29.9393 - val_mae: 4.7364\n",
            "Epoch 168/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.0543 - mae: 4.7461 - val_loss: 29.7777 - val_mae: 4.7198\n",
            "Epoch 169/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0796 - mae: 4.7454 - val_loss: 29.9794 - val_mae: 4.7417\n",
            "Epoch 170/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0948 - mae: 4.7458 - val_loss: 30.2873 - val_mae: 4.7734\n",
            "Epoch 171/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.0676 - mae: 4.7441 - val_loss: 29.6898 - val_mae: 4.6992\n",
            "Epoch 172/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 30.1586 - mae: 4.7533 - val_loss: 30.1125 - val_mae: 4.7547\n",
            "Epoch 173/500\n",
            "89/89 [==============================] - 29s 317ms/step - loss: 30.1291 - mae: 4.7604 - val_loss: 29.8930 - val_mae: 4.7327\n",
            "Epoch 174/500\n",
            "89/89 [==============================] - 21s 231ms/step - loss: 29.9497 - mae: 4.7327 - val_loss: 29.8095 - val_mae: 4.7204\n",
            "Epoch 175/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.1531 - mae: 4.7528 - val_loss: 30.3168 - val_mae: 4.7618\n",
            "Epoch 176/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0166 - mae: 4.7400 - val_loss: 29.9365 - val_mae: 4.7328\n",
            "Epoch 177/500\n",
            "89/89 [==============================] - 25s 272ms/step - loss: 30.1030 - mae: 4.7522 - val_loss: 29.8000 - val_mae: 4.7244\n",
            "Epoch 178/500\n",
            "89/89 [==============================] - 26s 293ms/step - loss: 30.1362 - mae: 4.7539 - val_loss: 29.8123 - val_mae: 4.7329\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0781 - mae: 4.7471 - val_loss: 30.0750 - val_mae: 4.7458\n",
            "Epoch 180/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0289 - mae: 4.7479 - val_loss: 30.0495 - val_mae: 4.7395\n",
            "Epoch 181/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.0818 - mae: 4.7474 - val_loss: 29.8725 - val_mae: 4.7154\n",
            "Epoch 182/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.0519 - mae: 4.7447 - val_loss: 29.7654 - val_mae: 4.7101\n",
            "Epoch 183/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 30.0507 - mae: 4.7434 - val_loss: 30.1586 - val_mae: 4.7608\n",
            "Epoch 184/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 29.9530 - mae: 4.7335 - val_loss: 30.0383 - val_mae: 4.7415\n",
            "Epoch 185/500\n",
            "89/89 [==============================] - 25s 283ms/step - loss: 30.0235 - mae: 4.7430 - val_loss: 29.8780 - val_mae: 4.7227\n",
            "Epoch 186/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 29.9859 - mae: 4.7379 - val_loss: 30.0870 - val_mae: 4.7514\n",
            "Epoch 187/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0060 - mae: 4.7424 - val_loss: 30.4280 - val_mae: 4.7908\n",
            "Epoch 188/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 29.9976 - mae: 4.7445 - val_loss: 29.9084 - val_mae: 4.7326\n",
            "Epoch 189/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 29.8921 - mae: 4.7304 - val_loss: 30.1852 - val_mae: 4.7684\n",
            "Epoch 190/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 30.0570 - mae: 4.7436 - val_loss: 30.2140 - val_mae: 4.7651\n",
            "Epoch 191/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9416 - mae: 4.7358 - val_loss: 29.6744 - val_mae: 4.7100\n",
            "Epoch 192/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 30.0182 - mae: 4.7394 - val_loss: 29.7285 - val_mae: 4.7250\n",
            "Epoch 193/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 30.0978 - mae: 4.7460 - val_loss: 29.6052 - val_mae: 4.7018\n",
            "Epoch 194/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.1472 - mae: 4.7547 - val_loss: 30.3284 - val_mae: 4.7752\n",
            "Epoch 195/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 30.0171 - mae: 4.7369 - val_loss: 30.0323 - val_mae: 4.7550\n",
            "Epoch 196/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 30.1023 - mae: 4.7470 - val_loss: 29.8515 - val_mae: 4.7230\n",
            "Epoch 197/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 29.9903 - mae: 4.7378 - val_loss: 29.7310 - val_mae: 4.7099\n",
            "Epoch 198/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0235 - mae: 4.7460 - val_loss: 29.9025 - val_mae: 4.7325\n",
            "Epoch 199/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.1450 - mae: 4.7560 - val_loss: 30.0259 - val_mae: 4.7469\n",
            "Epoch 200/500\n",
            "89/89 [==============================] - 26s 277ms/step - loss: 30.0091 - mae: 4.7422 - val_loss: 30.0850 - val_mae: 4.7507\n",
            "Epoch 201/500\n",
            "89/89 [==============================] - 20s 222ms/step - loss: 30.0169 - mae: 4.7418 - val_loss: 29.9320 - val_mae: 4.7302\n",
            "Epoch 202/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0288 - mae: 4.7428 - val_loss: 30.1634 - val_mae: 4.7542\n",
            "Epoch 203/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.0475 - mae: 4.7477 - val_loss: 29.9235 - val_mae: 4.7272\n",
            "Epoch 204/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 29.9414 - mae: 4.7335 - val_loss: 29.9437 - val_mae: 4.7349\n",
            "Epoch 205/500\n",
            "89/89 [==============================] - 25s 274ms/step - loss: 29.9122 - mae: 4.7255 - val_loss: 30.0028 - val_mae: 4.7339\n",
            "Epoch 206/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 30.1082 - mae: 4.7523 - val_loss: 29.9951 - val_mae: 4.7384\n",
            "Epoch 207/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 30.1359 - mae: 4.7559 - val_loss: 29.9924 - val_mae: 4.7387\n",
            "Epoch 208/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.1399 - mae: 4.7552 - val_loss: 29.6320 - val_mae: 4.7026\n",
            "Epoch 209/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 29.8887 - mae: 4.7300 - val_loss: 30.1707 - val_mae: 4.7586\n",
            "Epoch 210/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 29.9551 - mae: 4.7404 - val_loss: 30.0173 - val_mae: 4.7434\n",
            "Epoch 211/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 30.0463 - mae: 4.7476 - val_loss: 29.6410 - val_mae: 4.7119\n",
            "Epoch 212/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 29.8933 - mae: 4.7262 - val_loss: 30.2317 - val_mae: 4.7623\n",
            "Epoch 213/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0453 - mae: 4.7405 - val_loss: 29.8030 - val_mae: 4.7268\n",
            "Epoch 214/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0528 - mae: 4.7430 - val_loss: 30.0713 - val_mae: 4.7385\n",
            "Epoch 215/500\n",
            "89/89 [==============================] - 20s 224ms/step - loss: 30.0819 - mae: 4.7472 - val_loss: 29.6134 - val_mae: 4.7050\n",
            "Epoch 216/500\n",
            "89/89 [==============================] - 27s 304ms/step - loss: 30.0481 - mae: 4.7421 - val_loss: 30.3194 - val_mae: 4.7659\n",
            "Epoch 217/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 29.9704 - mae: 4.7374 - val_loss: 30.0142 - val_mae: 4.7489\n",
            "Epoch 218/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 29.9753 - mae: 4.7369 - val_loss: 30.0105 - val_mae: 4.7367\n",
            "Epoch 219/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 29.9915 - mae: 4.7398 - val_loss: 30.0193 - val_mae: 4.7421\n",
            "Epoch 220/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.1788 - mae: 4.7555 - val_loss: 29.8738 - val_mae: 4.7375\n",
            "Epoch 221/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 29.9053 - mae: 4.7292 - val_loss: 29.8293 - val_mae: 4.7266\n",
            "Epoch 222/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0641 - mae: 4.7437 - val_loss: 29.7297 - val_mae: 4.7163\n",
            "Epoch 223/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.1173 - mae: 4.7509 - val_loss: 29.9249 - val_mae: 4.7309\n",
            "Epoch 224/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 30.1120 - mae: 4.7484 - val_loss: 30.1182 - val_mae: 4.7444\n",
            "Epoch 225/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 30.0195 - mae: 4.7385 - val_loss: 29.9133 - val_mae: 4.7379\n",
            "Epoch 226/500\n",
            "89/89 [==============================] - 22s 242ms/step - loss: 30.1116 - mae: 4.7489 - val_loss: 30.1845 - val_mae: 4.7598\n",
            "Epoch 227/500\n",
            "89/89 [==============================] - 23s 252ms/step - loss: 29.9456 - mae: 4.7338 - val_loss: 29.6356 - val_mae: 4.6993\n",
            "Epoch 228/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.0682 - mae: 4.7488 - val_loss: 29.8684 - val_mae: 4.7267\n",
            "Epoch 229/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 30.0638 - mae: 4.7446 - val_loss: 30.1852 - val_mae: 4.7590\n",
            "Epoch 230/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0832 - mae: 4.7481 - val_loss: 30.3945 - val_mae: 4.7739\n",
            "Epoch 231/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 29.9940 - mae: 4.7407 - val_loss: 29.6723 - val_mae: 4.7099\n",
            "Epoch 232/500\n",
            "89/89 [==============================] - 25s 281ms/step - loss: 29.9336 - mae: 4.7352 - val_loss: 29.9195 - val_mae: 4.7314\n",
            "Epoch 233/500\n",
            "89/89 [==============================] - 20s 221ms/step - loss: 30.0892 - mae: 4.7471 - val_loss: 30.2033 - val_mae: 4.7518\n",
            "Epoch 234/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 29.8372 - mae: 4.7244 - val_loss: 30.3342 - val_mae: 4.7811\n",
            "Epoch 235/500\n",
            "89/89 [==============================] - 27s 284ms/step - loss: 29.9366 - mae: 4.7348 - val_loss: 29.7916 - val_mae: 4.7214\n",
            "Epoch 236/500\n",
            "89/89 [==============================] - 29s 320ms/step - loss: 29.9740 - mae: 4.7363 - val_loss: 29.9508 - val_mae: 4.7274\n",
            "Epoch 237/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0257 - mae: 4.7403 - val_loss: 29.9957 - val_mae: 4.7493\n",
            "Epoch 238/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.0862 - mae: 4.7454 - val_loss: 30.1470 - val_mae: 4.7672\n",
            "Epoch 239/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 30.1171 - mae: 4.7479 - val_loss: 30.3510 - val_mae: 4.7898\n",
            "Epoch 240/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0390 - mae: 4.7398 - val_loss: 29.8643 - val_mae: 4.7289\n",
            "Epoch 241/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.0116 - mae: 4.7442 - val_loss: 30.1042 - val_mae: 4.7472\n",
            "Epoch 242/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.0620 - mae: 4.7450 - val_loss: 29.7601 - val_mae: 4.7311\n",
            "Epoch 243/500\n",
            "89/89 [==============================] - 27s 303ms/step - loss: 30.0462 - mae: 4.7427 - val_loss: 30.3580 - val_mae: 4.7680\n",
            "Epoch 244/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 29.9243 - mae: 4.7347 - val_loss: 30.0200 - val_mae: 4.7398\n",
            "Epoch 245/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.0464 - mae: 4.7504 - val_loss: 29.9237 - val_mae: 4.7226\n",
            "Epoch 246/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 29.9109 - mae: 4.7327 - val_loss: 29.6865 - val_mae: 4.7074\n",
            "Epoch 247/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0661 - mae: 4.7486 - val_loss: 29.7779 - val_mae: 4.7260\n",
            "Epoch 248/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 30.0874 - mae: 4.7482 - val_loss: 30.2462 - val_mae: 4.7597\n",
            "Epoch 249/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0928 - mae: 4.7498 - val_loss: 29.7081 - val_mae: 4.7113\n",
            "Epoch 250/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9910 - mae: 4.7404 - val_loss: 29.9809 - val_mae: 4.7310\n",
            "Epoch 251/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0337 - mae: 4.7450 - val_loss: 30.1296 - val_mae: 4.7417\n",
            "Epoch 252/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0905 - mae: 4.7543 - val_loss: 29.5252 - val_mae: 4.6975\n",
            "Epoch 253/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0028 - mae: 4.7374 - val_loss: 29.7259 - val_mae: 4.7103\n",
            "Epoch 254/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 30.0130 - mae: 4.7391 - val_loss: 30.2310 - val_mae: 4.7620\n",
            "Epoch 255/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 30.0563 - mae: 4.7480 - val_loss: 30.1226 - val_mae: 4.7550\n",
            "Epoch 256/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 30.0120 - mae: 4.7377 - val_loss: 30.3615 - val_mae: 4.7723\n",
            "Epoch 257/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 29.9940 - mae: 4.7367 - val_loss: 30.1368 - val_mae: 4.7648\n",
            "Epoch 258/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0218 - mae: 4.7419 - val_loss: 30.0833 - val_mae: 4.7403\n",
            "Epoch 259/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 29.9797 - mae: 4.7400 - val_loss: 30.2421 - val_mae: 4.7654\n",
            "Epoch 260/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.8846 - mae: 4.7267 - val_loss: 29.5817 - val_mae: 4.7043\n",
            "Epoch 261/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.1104 - mae: 4.7517 - val_loss: 30.4548 - val_mae: 4.7908\n",
            "Epoch 262/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.0792 - mae: 4.7507 - val_loss: 30.0049 - val_mae: 4.7403\n",
            "Epoch 263/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.1446 - mae: 4.7560 - val_loss: 30.0278 - val_mae: 4.7439\n",
            "Epoch 264/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0592 - mae: 4.7420 - val_loss: 30.1175 - val_mae: 4.7544\n",
            "Epoch 265/500\n",
            "89/89 [==============================] - 29s 323ms/step - loss: 30.1462 - mae: 4.7548 - val_loss: 29.7536 - val_mae: 4.7159\n",
            "Epoch 266/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.9509 - mae: 4.7370 - val_loss: 30.0196 - val_mae: 4.7517\n",
            "Epoch 267/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 29.9481 - mae: 4.7326 - val_loss: 29.8481 - val_mae: 4.7320\n",
            "Epoch 268/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 29.9976 - mae: 4.7386 - val_loss: 29.9639 - val_mae: 4.7321\n",
            "Epoch 269/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0811 - mae: 4.7469 - val_loss: 29.9913 - val_mae: 4.7444\n",
            "Epoch 270/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 29.9908 - mae: 4.7367 - val_loss: 30.2819 - val_mae: 4.7713\n",
            "Epoch 271/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0240 - mae: 4.7429 - val_loss: 29.9499 - val_mae: 4.7279\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0484 - mae: 4.7502 - val_loss: 30.2774 - val_mae: 4.7691\n",
            "Epoch 273/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 29.9825 - mae: 4.7393 - val_loss: 30.1488 - val_mae: 4.7585\n",
            "Epoch 274/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 30.0717 - mae: 4.7517 - val_loss: 29.9789 - val_mae: 4.7302\n",
            "Epoch 275/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.1708 - mae: 4.7581 - val_loss: 30.0614 - val_mae: 4.7423\n",
            "Epoch 276/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.0671 - mae: 4.7449 - val_loss: 30.1349 - val_mae: 4.7552\n",
            "Epoch 277/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0955 - mae: 4.7490 - val_loss: 30.0447 - val_mae: 4.7365\n",
            "Epoch 278/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 30.2071 - mae: 4.7613 - val_loss: 29.7607 - val_mae: 4.7157\n",
            "Epoch 279/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 29.9294 - mae: 4.7342 - val_loss: 30.2950 - val_mae: 4.7730\n",
            "Epoch 280/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 30.1495 - mae: 4.7557 - val_loss: 29.8917 - val_mae: 4.7209\n",
            "Epoch 281/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 30.1948 - mae: 4.7616 - val_loss: 29.6442 - val_mae: 4.7141\n",
            "Epoch 282/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0418 - mae: 4.7425 - val_loss: 29.4606 - val_mae: 4.6833\n",
            "Epoch 283/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 30.0535 - mae: 4.7401 - val_loss: 29.8391 - val_mae: 4.7171\n",
            "Epoch 284/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0375 - mae: 4.7415 - val_loss: 30.0865 - val_mae: 4.7413\n",
            "Epoch 285/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0131 - mae: 4.7408 - val_loss: 29.9303 - val_mae: 4.7261\n",
            "Epoch 286/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0560 - mae: 4.7427 - val_loss: 30.1674 - val_mae: 4.7507\n",
            "Epoch 287/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 30.0458 - mae: 4.7496 - val_loss: 30.0818 - val_mae: 4.7565\n",
            "Epoch 288/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 30.1485 - mae: 4.7577 - val_loss: 30.1465 - val_mae: 4.7590\n",
            "Epoch 289/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0440 - mae: 4.7463 - val_loss: 29.9242 - val_mae: 4.7320\n",
            "Epoch 290/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.0203 - mae: 4.7421 - val_loss: 29.8508 - val_mae: 4.7222\n",
            "Epoch 291/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 30.0707 - mae: 4.7470 - val_loss: 30.0372 - val_mae: 4.7418\n",
            "Epoch 292/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.1822 - mae: 4.7569 - val_loss: 29.6710 - val_mae: 4.7027\n",
            "Epoch 293/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0164 - mae: 4.7442 - val_loss: 30.1941 - val_mae: 4.7580\n",
            "Epoch 294/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0675 - mae: 4.7454 - val_loss: 29.9627 - val_mae: 4.7338\n",
            "Epoch 295/500\n",
            "89/89 [==============================] - 28s 313ms/step - loss: 29.9944 - mae: 4.7369 - val_loss: 29.9631 - val_mae: 4.7335\n",
            "Epoch 296/500\n",
            "89/89 [==============================] - 21s 223ms/step - loss: 30.0375 - mae: 4.7506 - val_loss: 30.0094 - val_mae: 4.7426\n",
            "Epoch 297/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 30.1061 - mae: 4.7494 - val_loss: 30.1974 - val_mae: 4.7616\n",
            "Epoch 298/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0340 - mae: 4.7439 - val_loss: 30.0251 - val_mae: 4.7455\n",
            "Epoch 299/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 29.9741 - mae: 4.7395 - val_loss: 29.9507 - val_mae: 4.7349\n",
            "Epoch 300/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 30.0873 - mae: 4.7451 - val_loss: 30.0878 - val_mae: 4.7447\n",
            "Epoch 301/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 30.0704 - mae: 4.7460 - val_loss: 30.0683 - val_mae: 4.7433\n",
            "Epoch 302/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0581 - mae: 4.7436 - val_loss: 30.0390 - val_mae: 4.7451\n",
            "Epoch 303/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 30.0725 - mae: 4.7496 - val_loss: 30.0565 - val_mae: 4.7414\n",
            "Epoch 304/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0385 - mae: 4.7433 - val_loss: 30.1747 - val_mae: 4.7558\n",
            "Epoch 305/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9822 - mae: 4.7324 - val_loss: 30.1791 - val_mae: 4.7589\n",
            "Epoch 306/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.1001 - mae: 4.7499 - val_loss: 29.7293 - val_mae: 4.7090\n",
            "Epoch 307/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.1230 - mae: 4.7561 - val_loss: 30.0085 - val_mae: 4.7457\n",
            "Epoch 308/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0266 - mae: 4.7427 - val_loss: 30.2265 - val_mae: 4.7674\n",
            "Epoch 309/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 29.8228 - mae: 4.7202 - val_loss: 30.2539 - val_mae: 4.7687\n",
            "Epoch 310/500\n",
            "89/89 [==============================] - 21s 229ms/step - loss: 29.8787 - mae: 4.7274 - val_loss: 30.1347 - val_mae: 4.7546\n",
            "Epoch 311/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9760 - mae: 4.7348 - val_loss: 30.2284 - val_mae: 4.7641\n",
            "Epoch 312/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 29.9985 - mae: 4.7397 - val_loss: 30.0137 - val_mae: 4.7388\n",
            "Epoch 313/500\n",
            "89/89 [==============================] - 27s 285ms/step - loss: 30.0234 - mae: 4.7436 - val_loss: 30.1612 - val_mae: 4.7678\n",
            "Epoch 314/500\n",
            "89/89 [==============================] - 29s 322ms/step - loss: 29.9413 - mae: 4.7338 - val_loss: 30.0058 - val_mae: 4.7346\n",
            "Epoch 315/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0145 - mae: 4.7401 - val_loss: 29.8774 - val_mae: 4.7333\n",
            "Epoch 316/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 30.1053 - mae: 4.7470 - val_loss: 30.0991 - val_mae: 4.7460\n",
            "Epoch 317/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0471 - mae: 4.7416 - val_loss: 30.2316 - val_mae: 4.7636\n",
            "Epoch 318/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0322 - mae: 4.7407 - val_loss: 30.1679 - val_mae: 4.7544\n",
            "Epoch 319/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9280 - mae: 4.7347 - val_loss: 29.9348 - val_mae: 4.7323\n",
            "Epoch 320/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.9323 - mae: 4.7309 - val_loss: 30.0785 - val_mae: 4.7459\n",
            "Epoch 321/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0582 - mae: 4.7480 - val_loss: 29.9463 - val_mae: 4.7348\n",
            "Epoch 322/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 29.9928 - mae: 4.7382 - val_loss: 30.1804 - val_mae: 4.7578\n",
            "Epoch 323/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 29.9689 - mae: 4.7384 - val_loss: 29.5929 - val_mae: 4.7006\n",
            "Epoch 324/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 30.1268 - mae: 4.7525 - val_loss: 29.8425 - val_mae: 4.7333\n",
            "Epoch 325/500\n",
            "89/89 [==============================] - 23s 250ms/step - loss: 30.0890 - mae: 4.7473 - val_loss: 30.3670 - val_mae: 4.7770\n",
            "Epoch 326/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 29.8827 - mae: 4.7276 - val_loss: 30.2241 - val_mae: 4.7617\n",
            "Epoch 327/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.1556 - mae: 4.7542 - val_loss: 29.9934 - val_mae: 4.7284\n",
            "Epoch 328/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 29.9106 - mae: 4.7307 - val_loss: 29.9537 - val_mae: 4.7341\n",
            "Epoch 329/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 29.9011 - mae: 4.7331 - val_loss: 30.3335 - val_mae: 4.7848\n",
            "Epoch 330/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.0983 - mae: 4.7507 - val_loss: 30.1156 - val_mae: 4.7508\n",
            "Epoch 331/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.9370 - mae: 4.7352 - val_loss: 30.2127 - val_mae: 4.7687\n",
            "Epoch 332/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0608 - mae: 4.7425 - val_loss: 29.9923 - val_mae: 4.7295\n",
            "Epoch 333/500\n",
            "89/89 [==============================] - 29s 321ms/step - loss: 30.0584 - mae: 4.7422 - val_loss: 30.1630 - val_mae: 4.7520\n",
            "Epoch 334/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.0779 - mae: 4.7483 - val_loss: 30.0399 - val_mae: 4.7472\n",
            "Epoch 335/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 30.0570 - mae: 4.7437 - val_loss: 29.8922 - val_mae: 4.7285\n",
            "Epoch 336/500\n",
            "89/89 [==============================] - 22s 235ms/step - loss: 29.9879 - mae: 4.7383 - val_loss: 30.0861 - val_mae: 4.7575\n",
            "Epoch 337/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 30.0281 - mae: 4.7394 - val_loss: 30.1019 - val_mae: 4.7464\n",
            "Epoch 338/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 30.0203 - mae: 4.7408 - val_loss: 30.2705 - val_mae: 4.7711\n",
            "Epoch 339/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 29.8871 - mae: 4.7308 - val_loss: 30.1204 - val_mae: 4.7504\n",
            "Epoch 340/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.1317 - mae: 4.7588 - val_loss: 29.8043 - val_mae: 4.7182\n",
            "Epoch 341/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 30.0841 - mae: 4.7483 - val_loss: 30.1715 - val_mae: 4.7603\n",
            "Epoch 342/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 29.8103 - mae: 4.7216 - val_loss: 29.7384 - val_mae: 4.7096\n",
            "Epoch 343/500\n",
            "89/89 [==============================] - 29s 323ms/step - loss: 30.0324 - mae: 4.7383 - val_loss: 30.2357 - val_mae: 4.7648\n",
            "Epoch 344/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 30.0983 - mae: 4.7482 - val_loss: 29.9222 - val_mae: 4.7339\n",
            "Epoch 345/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 29.8931 - mae: 4.7297 - val_loss: 29.9034 - val_mae: 4.7266\n",
            "Epoch 346/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 29.9262 - mae: 4.7316 - val_loss: 29.9452 - val_mae: 4.7268\n",
            "Epoch 347/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 29.9903 - mae: 4.7336 - val_loss: 30.0868 - val_mae: 4.7517\n",
            "Epoch 348/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 30.0476 - mae: 4.7509 - val_loss: 29.6196 - val_mae: 4.7018\n",
            "Epoch 349/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 30.0853 - mae: 4.7513 - val_loss: 30.2163 - val_mae: 4.7670\n",
            "Epoch 350/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 30.1747 - mae: 4.7588 - val_loss: 29.6532 - val_mae: 4.7117\n",
            "Epoch 351/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 30.0782 - mae: 4.7450 - val_loss: 30.1453 - val_mae: 4.7615\n",
            "Epoch 352/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 30.0026 - mae: 4.7386 - val_loss: 30.0818 - val_mae: 4.7471\n",
            "Epoch 353/500\n",
            "89/89 [==============================] - 29s 319ms/step - loss: 29.9987 - mae: 4.7403 - val_loss: 30.3861 - val_mae: 4.7850\n",
            "Epoch 354/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 30.2426 - mae: 4.7652 - val_loss: 29.9098 - val_mae: 4.7274\n",
            "Epoch 355/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 29.9552 - mae: 4.7307 - val_loss: 30.2304 - val_mae: 4.7609\n",
            "Epoch 356/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 30.0431 - mae: 4.7411 - val_loss: 29.8327 - val_mae: 4.7214\n",
            "Epoch 357/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 30.2214 - mae: 4.7588 - val_loss: 30.0313 - val_mae: 4.7390\n",
            "Epoch 358/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 30.2015 - mae: 4.7618 - val_loss: 30.1835 - val_mae: 4.7563\n",
            "Epoch 359/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.1178 - mae: 4.7533 - val_loss: 29.7971 - val_mae: 4.7182\n",
            "Epoch 360/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 29.9898 - mae: 4.7426 - val_loss: 30.0882 - val_mae: 4.7516\n",
            "Epoch 361/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0587 - mae: 4.7421 - val_loss: 30.2697 - val_mae: 4.7612\n",
            "Epoch 362/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 29.8694 - mae: 4.7268 - val_loss: 30.3861 - val_mae: 4.7827\n",
            "Epoch 363/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.9640 - mae: 4.7332 - val_loss: 30.3527 - val_mae: 4.7812\n",
            "Epoch 364/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0555 - mae: 4.7466 - val_loss: 29.9544 - val_mae: 4.7357\n",
            "Epoch 365/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.1458 - mae: 4.7594 - val_loss: 30.2711 - val_mae: 4.7658\n",
            "Epoch 366/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.0144 - mae: 4.7421 - val_loss: 29.8100 - val_mae: 4.7272\n",
            "Epoch 367/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 29.8970 - mae: 4.7341 - val_loss: 30.1379 - val_mae: 4.7632\n",
            "Epoch 368/500\n",
            "89/89 [==============================] - 27s 304ms/step - loss: 30.0815 - mae: 4.7446 - val_loss: 29.8912 - val_mae: 4.7247\n",
            "Epoch 369/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 29.9488 - mae: 4.7354 - val_loss: 29.7035 - val_mae: 4.7158\n",
            "Epoch 370/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.1895 - mae: 4.7626 - val_loss: 30.1050 - val_mae: 4.7600\n",
            "Epoch 371/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.9320 - mae: 4.7319 - val_loss: 30.0505 - val_mae: 4.7345\n",
            "Epoch 372/500\n",
            "89/89 [==============================] - 25s 273ms/step - loss: 29.9143 - mae: 4.7299 - val_loss: 29.8751 - val_mae: 4.7271\n",
            "Epoch 373/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.2711 - mae: 4.7644 - val_loss: 30.4166 - val_mae: 4.7786\n",
            "Epoch 374/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 29.9527 - mae: 4.7381 - val_loss: 30.1443 - val_mae: 4.7590\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 29.9763 - mae: 4.7410 - val_loss: 30.0843 - val_mae: 4.7530\n",
            "Epoch 376/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 30.0071 - mae: 4.7400 - val_loss: 29.8994 - val_mae: 4.7300\n",
            "Epoch 377/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 30.0596 - mae: 4.7447 - val_loss: 29.7546 - val_mae: 4.7077\n",
            "Epoch 378/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 29.9220 - mae: 4.7331 - val_loss: 29.8995 - val_mae: 4.7346\n",
            "Epoch 379/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 29.9773 - mae: 4.7381 - val_loss: 30.0735 - val_mae: 4.7426\n",
            "Epoch 380/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0480 - mae: 4.7498 - val_loss: 29.6476 - val_mae: 4.7029\n",
            "Epoch 381/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 29.8974 - mae: 4.7311 - val_loss: 29.9911 - val_mae: 4.7423\n",
            "Epoch 382/500\n",
            "89/89 [==============================] - 23s 246ms/step - loss: 29.9848 - mae: 4.7362 - val_loss: 29.9838 - val_mae: 4.7330\n",
            "Epoch 383/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0819 - mae: 4.7509 - val_loss: 29.9180 - val_mae: 4.7282\n",
            "Epoch 384/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 29.9232 - mae: 4.7249 - val_loss: 30.1782 - val_mae: 4.7638\n",
            "Epoch 385/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 30.1661 - mae: 4.7577 - val_loss: 29.8999 - val_mae: 4.7273\n",
            "Epoch 386/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.1617 - mae: 4.7515 - val_loss: 30.2963 - val_mae: 4.7706\n",
            "Epoch 387/500\n",
            "89/89 [==============================] - 22s 235ms/step - loss: 30.1647 - mae: 4.7571 - val_loss: 30.2226 - val_mae: 4.7714\n",
            "Epoch 388/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0998 - mae: 4.7507 - val_loss: 29.8618 - val_mae: 4.7280\n",
            "Epoch 389/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.0748 - mae: 4.7494 - val_loss: 29.5588 - val_mae: 4.7022\n",
            "Epoch 390/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 30.0206 - mae: 4.7424 - val_loss: 30.1320 - val_mae: 4.7487\n",
            "Epoch 391/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.2046 - mae: 4.7654 - val_loss: 30.2622 - val_mae: 4.7721\n",
            "Epoch 392/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 29.9424 - mae: 4.7407 - val_loss: 29.8617 - val_mae: 4.7306\n",
            "Epoch 393/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.1028 - mae: 4.7493 - val_loss: 29.7734 - val_mae: 4.7158\n",
            "Epoch 394/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 30.1156 - mae: 4.7547 - val_loss: 30.2951 - val_mae: 4.7760\n",
            "Epoch 395/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 29.9989 - mae: 4.7391 - val_loss: 29.5535 - val_mae: 4.6971\n",
            "Epoch 396/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 30.0305 - mae: 4.7437 - val_loss: 29.7103 - val_mae: 4.7161\n",
            "Epoch 397/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 30.0124 - mae: 4.7372 - val_loss: 29.9216 - val_mae: 4.7366\n",
            "Epoch 398/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 30.1117 - mae: 4.7512 - val_loss: 29.8674 - val_mae: 4.7175\n",
            "Epoch 399/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 29.9953 - mae: 4.7368 - val_loss: 29.7108 - val_mae: 4.7090\n",
            "Epoch 400/500\n",
            "89/89 [==============================] - 30s 325ms/step - loss: 30.0599 - mae: 4.7477 - val_loss: 29.9592 - val_mae: 4.7437\n",
            "Epoch 401/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 30.0245 - mae: 4.7398 - val_loss: 29.8927 - val_mae: 4.7364\n",
            "Epoch 402/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 30.0550 - mae: 4.7516 - val_loss: 29.8495 - val_mae: 4.7256\n",
            "Epoch 403/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.0857 - mae: 4.7459 - val_loss: 30.0944 - val_mae: 4.7440\n",
            "Epoch 404/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.0029 - mae: 4.7400 - val_loss: 29.8041 - val_mae: 4.7257\n",
            "Epoch 405/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 30.0062 - mae: 4.7403 - val_loss: 30.1614 - val_mae: 4.7595\n",
            "Epoch 406/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 30.0423 - mae: 4.7427 - val_loss: 29.7950 - val_mae: 4.7265\n",
            "Epoch 407/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9819 - mae: 4.7344 - val_loss: 29.8649 - val_mae: 4.7230\n",
            "Epoch 408/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 29.9484 - mae: 4.7332 - val_loss: 29.6916 - val_mae: 4.7002\n",
            "Epoch 409/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.1143 - mae: 4.7534 - val_loss: 30.1857 - val_mae: 4.7561\n",
            "Epoch 410/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 29.9970 - mae: 4.7466 - val_loss: 29.7515 - val_mae: 4.7146\n",
            "Epoch 411/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0001 - mae: 4.7400 - val_loss: 29.7293 - val_mae: 4.7205\n",
            "Epoch 412/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 30.0118 - mae: 4.7432 - val_loss: 29.8042 - val_mae: 4.7193\n",
            "Epoch 413/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 29.9912 - mae: 4.7386 - val_loss: 29.4740 - val_mae: 4.6847\n",
            "Epoch 414/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 29.9904 - mae: 4.7405 - val_loss: 29.9867 - val_mae: 4.7373\n",
            "Epoch 415/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9789 - mae: 4.7402 - val_loss: 30.2796 - val_mae: 4.7598\n",
            "Epoch 416/500\n",
            "89/89 [==============================] - 21s 231ms/step - loss: 30.0561 - mae: 4.7463 - val_loss: 29.7790 - val_mae: 4.7199\n",
            "Epoch 417/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 29.9049 - mae: 4.7278 - val_loss: 29.6875 - val_mae: 4.7070\n",
            "Epoch 418/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.8988 - mae: 4.7305 - val_loss: 30.2545 - val_mae: 4.7606\n",
            "Epoch 419/500\n",
            "89/89 [==============================] - 29s 323ms/step - loss: 29.9330 - mae: 4.7330 - val_loss: 29.9520 - val_mae: 4.7294\n",
            "Epoch 420/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 30.1269 - mae: 4.7568 - val_loss: 29.6538 - val_mae: 4.7119\n",
            "Epoch 421/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 29.9152 - mae: 4.7322 - val_loss: 29.8690 - val_mae: 4.7271\n",
            "Epoch 422/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0165 - mae: 4.7384 - val_loss: 29.9324 - val_mae: 4.7325\n",
            "Epoch 423/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 30.0570 - mae: 4.7435 - val_loss: 29.8539 - val_mae: 4.7257\n",
            "Epoch 424/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0481 - mae: 4.7451 - val_loss: 29.8508 - val_mae: 4.7311\n",
            "Epoch 425/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9960 - mae: 4.7382 - val_loss: 29.9980 - val_mae: 4.7324\n",
            "Epoch 426/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.0899 - mae: 4.7517 - val_loss: 30.4573 - val_mae: 4.7884\n",
            "Epoch 427/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 30.1140 - mae: 4.7523 - val_loss: 29.8912 - val_mae: 4.7304\n",
            "Epoch 428/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.1018 - mae: 4.7512 - val_loss: 29.6425 - val_mae: 4.7113\n",
            "Epoch 429/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 29.8424 - mae: 4.7209 - val_loss: 30.1373 - val_mae: 4.7595\n",
            "Epoch 430/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 30.0678 - mae: 4.7484 - val_loss: 29.9758 - val_mae: 4.7372\n",
            "Epoch 431/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 29.9673 - mae: 4.7382 - val_loss: 29.9401 - val_mae: 4.7432\n",
            "Epoch 432/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 29.9929 - mae: 4.7422 - val_loss: 29.8717 - val_mae: 4.7330\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 30.0461 - mae: 4.7481 - val_loss: 30.1951 - val_mae: 4.7604\n",
            "Epoch 434/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 30.0827 - mae: 4.7460 - val_loss: 29.9460 - val_mae: 4.7319\n",
            "Epoch 435/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 30.0765 - mae: 4.7418 - val_loss: 30.0375 - val_mae: 4.7425\n",
            "Epoch 436/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 30.1126 - mae: 4.7503 - val_loss: 30.1297 - val_mae: 4.7575\n",
            "Epoch 437/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 30.0201 - mae: 4.7409 - val_loss: 30.0679 - val_mae: 4.7469\n",
            "Epoch 438/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 30.0631 - mae: 4.7498 - val_loss: 29.6659 - val_mae: 4.7071\n",
            "Epoch 439/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.1300 - mae: 4.7570 - val_loss: 30.2094 - val_mae: 4.7591\n",
            "Epoch 440/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9933 - mae: 4.7410 - val_loss: 30.2583 - val_mae: 4.7545\n",
            "Epoch 441/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 30.0304 - mae: 4.7403 - val_loss: 29.7999 - val_mae: 4.7188\n",
            "Epoch 442/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 30.0489 - mae: 4.7473 - val_loss: 29.7096 - val_mae: 4.7104\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 30.1541 - mae: 4.7549 - val_loss: 29.9918 - val_mae: 4.7376\n",
            "Epoch 444/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 30.0325 - mae: 4.7351 - val_loss: 30.2259 - val_mae: 4.7595\n",
            "Epoch 445/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 30.0198 - mae: 4.7467 - val_loss: 29.9431 - val_mae: 4.7252\n",
            "Epoch 446/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.0268 - mae: 4.7386 - val_loss: 30.1551 - val_mae: 4.7531\n",
            "Epoch 447/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.0109 - mae: 4.7420 - val_loss: 30.1588 - val_mae: 4.7467\n",
            "Epoch 448/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 30.1317 - mae: 4.7505 - val_loss: 29.9718 - val_mae: 4.7333\n",
            "Epoch 449/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 29.9748 - mae: 4.7427 - val_loss: 29.8683 - val_mae: 4.7265\n",
            "Epoch 450/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 29.9887 - mae: 4.7360 - val_loss: 29.8112 - val_mae: 4.7118\n",
            "Epoch 451/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.0245 - mae: 4.7457 - val_loss: 29.9960 - val_mae: 4.7388\n",
            "Epoch 452/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 30.1003 - mae: 4.7469 - val_loss: 29.6609 - val_mae: 4.7067\n",
            "Epoch 453/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 29.8644 - mae: 4.7286 - val_loss: 29.4120 - val_mae: 4.6837\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 30.0131 - mae: 4.7381 - val_loss: 30.0547 - val_mae: 4.7557\n",
            "Epoch 455/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 30.1099 - mae: 4.7525 - val_loss: 30.0377 - val_mae: 4.7423\n",
            "Epoch 456/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 29.9997 - mae: 4.7427 - val_loss: 30.2852 - val_mae: 4.7696\n",
            "Epoch 457/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 29.9350 - mae: 4.7373 - val_loss: 30.1217 - val_mae: 4.7500\n",
            "Epoch 458/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 29.9772 - mae: 4.7407 - val_loss: 30.2089 - val_mae: 4.7639\n",
            "Epoch 459/500\n",
            "89/89 [==============================] - 22s 234ms/step - loss: 30.0287 - mae: 4.7421 - val_loss: 29.9006 - val_mae: 4.7386\n",
            "Epoch 460/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.1666 - mae: 4.7599 - val_loss: 30.2801 - val_mae: 4.7687\n",
            "Epoch 461/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 29.9637 - mae: 4.7340 - val_loss: 29.9853 - val_mae: 4.7367\n",
            "Epoch 462/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 29.9509 - mae: 4.7345 - val_loss: 29.9488 - val_mae: 4.7362\n",
            "Epoch 463/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.0496 - mae: 4.7397 - val_loss: 30.2435 - val_mae: 4.7694\n",
            "Epoch 464/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 29.9032 - mae: 4.7336 - val_loss: 29.8608 - val_mae: 4.7320\n",
            "Epoch 465/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0123 - mae: 4.7369 - val_loss: 30.1723 - val_mae: 4.7664\n",
            "Epoch 466/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 29.9899 - mae: 4.7395 - val_loss: 29.9650 - val_mae: 4.7310\n",
            "Epoch 467/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0771 - mae: 4.7470 - val_loss: 30.0464 - val_mae: 4.7409\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - 23s 247ms/step - loss: 30.1363 - mae: 4.7524 - val_loss: 30.2884 - val_mae: 4.7754\n",
            "Epoch 469/500\n",
            "89/89 [==============================] - 21s 230ms/step - loss: 29.9898 - mae: 4.7398 - val_loss: 29.9424 - val_mae: 4.7286\n",
            "Epoch 470/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 30.0325 - mae: 4.7393 - val_loss: 30.3929 - val_mae: 4.7850\n",
            "Epoch 471/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 30.0411 - mae: 4.7387 - val_loss: 30.1048 - val_mae: 4.7442\n",
            "Epoch 472/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 30.1430 - mae: 4.7482 - val_loss: 29.7530 - val_mae: 4.7071\n",
            "Epoch 473/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.1207 - mae: 4.7529 - val_loss: 29.8787 - val_mae: 4.7255\n",
            "Epoch 474/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 30.0469 - mae: 4.7456 - val_loss: 29.8659 - val_mae: 4.7276\n",
            "Epoch 475/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0348 - mae: 4.7484 - val_loss: 29.8785 - val_mae: 4.7372\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - 30s 322ms/step - loss: 29.9635 - mae: 4.7340 - val_loss: 29.7990 - val_mae: 4.7125\n",
            "Epoch 477/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 30.0072 - mae: 4.7386 - val_loss: 30.0067 - val_mae: 4.7376\n",
            "Epoch 478/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 30.0776 - mae: 4.7441 - val_loss: 30.1815 - val_mae: 4.7460\n",
            "Epoch 479/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 29.9928 - mae: 4.7365 - val_loss: 30.1206 - val_mae: 4.7589\n",
            "Epoch 480/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 30.0416 - mae: 4.7415 - val_loss: 29.8520 - val_mae: 4.7269\n",
            "Epoch 481/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 30.1350 - mae: 4.7547 - val_loss: 30.1941 - val_mae: 4.7641\n",
            "Epoch 482/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.0990 - mae: 4.7513 - val_loss: 29.6123 - val_mae: 4.7009\n",
            "Epoch 483/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 29.8812 - mae: 4.7272 - val_loss: 30.0893 - val_mae: 4.7475\n",
            "Epoch 484/500\n",
            "89/89 [==============================] - 29s 317ms/step - loss: 30.0052 - mae: 4.7388 - val_loss: 29.9274 - val_mae: 4.7313\n",
            "Epoch 485/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0260 - mae: 4.7417 - val_loss: 30.2395 - val_mae: 4.7577\n",
            "Epoch 486/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.1956 - mae: 4.7617 - val_loss: 29.8073 - val_mae: 4.7258\n",
            "Epoch 487/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 29.8914 - mae: 4.7330 - val_loss: 29.6949 - val_mae: 4.7056\n",
            "Epoch 488/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 29.9976 - mae: 4.7377 - val_loss: 30.0449 - val_mae: 4.7387\n",
            "Epoch 489/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 29.9179 - mae: 4.7310 - val_loss: 30.0620 - val_mae: 4.7475\n",
            "Epoch 490/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 30.0390 - mae: 4.7481 - val_loss: 30.3430 - val_mae: 4.7721\n",
            "Epoch 491/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 30.0818 - mae: 4.7464 - val_loss: 30.0210 - val_mae: 4.7349\n",
            "Epoch 492/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 30.1423 - mae: 4.7564 - val_loss: 30.0563 - val_mae: 4.7473\n",
            "Epoch 493/500\n",
            "89/89 [==============================] - 29s 319ms/step - loss: 29.9948 - mae: 4.7353 - val_loss: 30.5549 - val_mae: 4.8005\n",
            "Epoch 494/500\n",
            "89/89 [==============================] - 21s 233ms/step - loss: 30.0165 - mae: 4.7326 - val_loss: 30.0455 - val_mae: 4.7518\n",
            "Epoch 495/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 30.1138 - mae: 4.7518 - val_loss: 30.0181 - val_mae: 4.7384\n",
            "Epoch 496/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 29.9268 - mae: 4.7304 - val_loss: 30.3319 - val_mae: 4.7716\n",
            "Epoch 497/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 29.9323 - mae: 4.7331 - val_loss: 30.0151 - val_mae: 4.7395\n",
            "Epoch 498/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 30.0784 - mae: 4.7465 - val_loss: 29.8097 - val_mae: 4.7161\n",
            "Epoch 499/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 30.0153 - mae: 4.7390 - val_loss: 30.2535 - val_mae: 4.7675\n",
            "Epoch 500/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 29.9848 - mae: 4.7369 - val_loss: 30.0638 - val_mae: 4.7385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and validation MeanAbsoluteError')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y3K89-CM-dfg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "fd8a80a9-6e69-4d16-f547-dcf977ba0efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_x"
      ],
      "metadata": {
        "id": "N_sP_ZmSY-Jv",
        "outputId": "6f3c8263-9c7f-47e5-e925-267eff0e74a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download the model\n"
      ],
      "metadata": {
        "id": "R19IJQSYoW7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/My Drive/cut_panoramic/Model', exist_ok=True)\n",
        "model.save('/content/drive/My Drive/cut_panoramic/Model/1_2e-2_16_0.2_Male18_500.h5')"
      ],
      "metadata": {
        "id": "Zed4TdFcG2iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n5YxZ-5QjQ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download('/content/drive/My Drive/cut_panoramic/Model/1.1_รอบแรก_Flimpano_Male125_250.h5')"
      ],
      "metadata": {
        "id": "P5eMxm1NV-oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wY_pDlxkRwxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}